<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://easoncao.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://easoncao.com/" rel="alternate" type="text/html" /><updated>2021-03-06T14:40:19+00:00</updated><id>https://easoncao.com/feed.xml</id><title type="html">Continuous Improvement</title><subtitle>Sharing technical stuffs, AWS, Cloud computing, container, workout, travling and more.</subtitle><author><name>Yang-Xin Cao (Eason Cao)</name></author><entry><title type="html">我是如何在還沒畢業就錄取並進入到 Amazon 工作</title><link href="https://easoncao.com/how-am-I-get-into-amazon-before-graduate/" rel="alternate" type="text/html" title="我是如何在還沒畢業就錄取並進入到 Amazon 工作" /><published>2021-02-20T00:00:00+00:00</published><updated>2021-02-20T00:00:00+00:00</updated><id>https://easoncao.com/how-am-I-get-into-amazon-before-graduate</id><content type="html" xml:base="https://easoncao.com/how-am-I-get-into-amazon-before-graduate/">&lt;p&gt;很多人好奇我是如何進到 Amazon 工作，並且在還沒畢業前就拿到 offer。今年 (2021) 是我加入 Amazon 這家公司的第三年 (把當 Intern 跟當兵的時間也算進去的話 XD)，所以一次把故事寫在這裡。&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-am-I-get-into-amazon-before-graduate/amazon-macaron.jpg&quot; alt=&quot;我是如何在還沒畢業就錄取並進入到 Amazon 工作&quot; /&gt;
  
    &lt;figcaption&gt;
      我是如何在還沒畢業就錄取並進入到 Amazon 工作

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;人生第一份履歷&quot;&gt;人生第一份履歷&lt;/h2&gt;

&lt;p&gt;2016 年底，大三，我投遞了人生第一份履歷。其實在前面我做了一些研究，當時的心態是希望能夠跳脫校園舒適的環境，並至少在畢業前有一些實務的經驗 (感受職場的殘酷)，並且實際了解產業的運作。(結果畢業了更想回去學校唸書)&lt;/p&gt;

&lt;p&gt;當時我應徵了下列的職缺：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Software Development Intern (The News Lens)&lt;/li&gt;
  &lt;li&gt;Cloud Support Engineer Intern (Amazon Web Services)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;為什麼是這兩個，第一家原因，是因為當時也在媒體紛亂的時代，使得我對於獨立媒體產業很有興趣；並且創辦人 Joey Chung 當時也有來到學校分享 (不過那個講座超少人參加)。想要成為改變媒體亂象的念頭深植我心，加上更了解企業運作跟文化後，我才知道原來 The News Lens 是一家很有個性的新創公司，當時打死都不接受商業媒體贊助，竟然還能活得這麼好，因此一直深感在這樣的公司裡面工作應該能學習到很多。&lt;/p&gt;

&lt;p&gt;點燃實際跨出找實習的契機，當時我在講座對 Joey Chung 問了一個問題 (具體內容我忘記了)，大意就是不太確定如何培養實務經驗跟具備企業所需要的能力，我獲得簡潔有力的回答：&lt;strong&gt;實際去 Internship&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;他也說到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Harvard 在你一開始入學時就將所有 tutions / fees 各項雜支計算給你，因此你必須很清楚你來學校的目的，並且努力善用你擁有的資源。&lt;/p&gt;

  &lt;p&gt;(現在想想真的是廢話，但當時真的是沒有那麼宏觀的格局，所以仍然十分感激這樣寶貴的建議)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加上學校對於畢業設立需要到業界實習的門檻 (真是優良的傳統)，這要求了每個學生在畢業前一定要去業界實習，並且透過老闆給的考核給予學分。雖然我當時找的時間點有點早 (同學們都還在煩惱中午吃什麼)，再加上大三又是課業最沈重的一年，雖然有點累，但想想仍其實利大於弊。因此待一切時機成熟，留意到有實習的職缺後，一個不猶豫有機會就投遞試試。&lt;/p&gt;

&lt;p&gt;我現在努力回想只記得去 The News Lens 面試軟體開發實習生時，問了幾個問題：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;過去用 PHP 做了哪些專案？&lt;/li&gt;
  &lt;li&gt;你解決過最難的技術問題是什麼，你如何解決？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我當時臨時只想到我在寫 PHP 專案時為了要設計選單式目錄，做了一些研究並想了怎麼規劃資料表，最後用遞迴的方式列舉出來覺得很酷。&lt;/p&gt;

&lt;p&gt;總之最後，獲得感謝信一封，我很慶幸 The News Lens 當時沒錄取我，讓我連猶豫的機會都沒有。&lt;/p&gt;

&lt;h2 id=&quot;差點錯過的機會&quot;&gt;差點錯過的機會&lt;/h2&gt;

&lt;p&gt;身為一名在資訊圈打混的人，多少都知道 Amazon Web Services (AWS) 就是一個聽起來就很炫砲的技術，會用 AWS 就像是站在技術之巔一樣。&lt;/p&gt;

&lt;p&gt;當時，AWS 第一次在台灣開啟校園招聘計劃，我也有注意到 AWS 在學校發布對應的招聘訊息。我看了招聘訊息就真的是超級熱血沸騰，即使實習職缺的說明會地點在台大，我都覺得想去試試。&lt;/p&gt;

&lt;p&gt;當時十分想去，但遺憾的是，時間點跟學校的課程衝突。而且，最重要的，&lt;strong&gt;招聘資訊上也沒有可以投履歷的窗口 (Email) ！！！&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;避免你覺得我在唬爛，我還有留當時招聘的文宣 (可見當時有多氣憤，可…可惡)：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-am-I-get-into-amazon-before-graduate/aws-campus-hiring-event-post-in-2016.png&quot; alt=&quot;AWS 第一次校園招聘與差點錯過的機會&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS 第一次校園招聘與差點錯過的機會

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;結果就以為這麼沒了。&lt;/p&gt;

&lt;p&gt;(是不是看完，沒有看到 Email？我還在想會不會藏第一題測驗問題，在圖片藏什麼解碼問題，還試著用純文字編輯器打開看看有沒有什麼有趣的地方。)&lt;/p&gt;

&lt;h2 id=&quot;陰錯陽差的面試&quot;&gt;陰錯陽差的面試&lt;/h2&gt;

&lt;p&gt;結果好巧不巧，當時招聘的團隊裡，有畢業的 Eefy 學長，幾天後在 Facebook 放了相同的招聘訊息。於是我二話不說，找了機會搭上線，看能不能投遞履歷。學長也很熱心的幫我把履歷轉給 HR (真的十分感激！)。&lt;/p&gt;

&lt;p&gt;約莫 1-2 週，我就收到一封信，裡面包涵了幾個線上測驗的問題。題目類型是情境題，給了一組 IAM 帳戶 (AWS 的帳戶) 要你登入進去 AWS Console 搭建環境。但說實話當時我連 AWS 的帳號都沒有，那次是我第一次使用 AWS 服務，更別提什麼 VPC。&lt;/p&gt;

&lt;p&gt;所以我也是在期中考那週，花幾個小時，研究並把環境搭建起來，並且使用我人生第一個 Load Balancer (ELB)。不過由於過去就有管理過機器，使用過像是 Linode、虛擬主機，設定 Linux，所以對我來說學習的門檻並不會很高。還記得當時秉持著接觸新知的心情在研究文件上的資料，反覆檢查自己的設定符不符合題目設計的要求，搞得比自己的期中考還緊張。&lt;/p&gt;

&lt;p&gt;再過了幾個星期，我就收到 HR 邀請安排面試的電話跟信件 (還是從北京發來的)！當時掛完電話收到 Email，就得知我即將迎來至少三關的面試。當時還很猶豫要怎麼穿面試服裝，最後還特地穿了個白襯衫加皮鞋。&lt;/p&gt;

&lt;p&gt;因為實在是太興奮 (OMG 我要在世界前幾大的公司面試啊！)，提早了快 30 分鐘到。&lt;strong&gt;那時候 Amazon 剛在台灣進駐，連辦公室都沒有，所以那時還是在共享辦公室安排面試。我還記得我到指定地點，還以為自己被詐騙 (因為混著不同公司的人，沒有人知道 Amazon 的位置在哪)&lt;/strong&gt;，直搗前台獲得引導才放下心中的忐忑。&lt;/p&gt;

&lt;p&gt;三關的面試，其中包含了兩關的與 Technical Interviewer 的技術面試，以及一關 Hiring Manager 的面試。&lt;/p&gt;

&lt;p&gt;技術面試問了很多 Linux 及網路相關的問題，主要目的都是測試對於這些知識的掌握程度。面試問題，說實話真的是很硬 (&lt;strong&gt;考官臉也很硬&lt;/strong&gt;)。當時 Linux 的考官 (也是我現在的老闆)，剛見到面的氣場就完全鎮壓，面試從來不告訴你對還是不對，就是一直瘋狂猛問。劈頭開場就快把 Linux 運作原理挖透了 (我的知識也快被掏空)。&lt;/p&gt;

&lt;p&gt;網路相關的問題，則問了很多基本網路概論，聊著聊著還分享了自己過去學習這些知識點的經歷、學習維運系統會做的哪些事情、自己當時系統因為用的套件有漏洞被 Hack 中間學到了哪些事情 (怎麼發現被 inject javascript、被放 DoS 的 PHP 程式還研究了一下樣本… etc)，當時大概就是說著說著眼裡都可以閃爍著光芒的那種程度。當時就只有一種想法，很慶幸能有機會與前輩交流，然後感受到在外商工作的人英文真的是很會，知道自己還有什麼樣的差距。&lt;/p&gt;

&lt;p&gt;面試到後面，主管進來就說前面的 Interviewer 評價給得還不錯，有打算想先給我個口頭 offer。所以你要說我僥倖錄取，也許吧，就這麼剛好能多少回答面試官的問題，上至基本開發到 Linux、網路，下聊到電腦視覺。技術問題真的是被電歪，小緊張是會的，但是當時心情是很興奮且雀躍的，因為又感覺自己進步，並且又了解更多東西。&lt;/p&gt;

&lt;p&gt;我還記得當時面試完我的腎上腺素飆高，打電話給我爸說我得到口頭 offer，結果還被我爸潑了冷水：「人家只是講講啦！啊你這樣會不會很忙，我是覺得你不用去實習啦。」真是幸虧我一直都很叛逆不怎麼聽話。&lt;/p&gt;

&lt;p&gt;真的收到正式 offer 也差不多隔了快一個月，當時打開信看到一堆英文加上很多 statements (完全沒有中文)，覺得非常新鮮 (結果現在英文文件已經看到爛且習慣了，但第一次的感覺還記憶猶新)，而且面試過感覺每個人都很強，還是難掩說不出的興奮。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我的天！我真的獲得進入國際大公司工作的機會！&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;到正式入職當天 (&lt;strong&gt;我第一天報到還沒出門就超嗨&lt;/strong&gt;)，Amazon 就馬上有了新的辦公室 (真的是決策很快)。我是第一批錄取的實習生，第一天到辦公室報到時，我以為會有一批實習生一起工作，並且未來的日子裡，將會跟電影情節一樣，有個很快樂美好的 Party 跟夥伴 (完全被 The Internship 誤導)。&lt;/p&gt;

&lt;p&gt;結果就是，全部都是幻想 ….&lt;/p&gt;

&lt;p&gt;後來才知道，當時邀請面試前，就先刷了一批人 (AWS 環境都搭不起來)，技術面試又再刷了一堆人。結果我進去之後，才發現原來當時我同事們竟然拿正職的考題來問我 (難怪一堆人被電歪)，只有我跟另一位台大的學長 William 倖存 (當時全台灣 Intern 就只有我們，這樣聽起來好像很厲害)。&lt;/p&gt;

&lt;p&gt;於是領了電腦、螢幕，有了專屬的座位，就開始了我在 Amazon 實習生的歷程。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;有時候就是一個機會，你無法預測現在做的決定，在未來是否仍是最好的選擇，但你永遠有權決定，是不是要把握這樣的機會。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;轉成正職&quot;&gt;轉成正職&lt;/h2&gt;

&lt;p&gt;我的前主管是非常厲害的一個人，並且用著很不一樣管理思維在看待實習生計畫。他認為實習生最大的任務，除了一起加入專案的開發幫助改善團隊的生產力外，其餘就是學習、甚至是在公司提供的資源下拿到 AWS Certificates。並且試著從團隊的目標中，在幫助實習生成長的過程裡，找到交集的可能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我覺得在我實習的日子裡，最感激 Amazon 有著很成熟的師徒制度 (Mentor and Mentee)、一堆優秀的同事，並且有著一輩子學習不完的訓練資源幫助你成長，所以你總能找到人為你提供建議，並向他們學習。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我當時最喜歡的電影是 The intern 及 The Internship。一個是年過 80 的退休人士在新興網路公司工作的歷程，而另外一個則是兩位中年大叔轉職去 Google 當實習生的電影。這兩部電影著實給我很大的啟發，我有時會無限回放，想著哪些原則是可以套用在自己身上，並不斷優化。我真的很感謝這兩部電影的製作團隊，這麼好的作品，真的都成為我學習人生課題一部分的重要素材。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;也因為這樣，當實習生的日子，我不怕丟臉，我的心態就是努力做、努力學習，像個海綿一樣。&lt;/strong&gt;雖然大三是課業繁重的一年，但當時我在週一到週五排課，想辦法空出一個全天，以及週六週日其中一天時間到辦公室工作。由於那段時間真的是沒有休息日，其實還有點黑暗，但我常常自主在辦公室裡讀著文件、學習材料，常常一待就快 12 個小時，跟著前輩討論技術問題。每次覺得又獲得更多知識，這個過程是有趣的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我在加入之後，其實也從不把自己當實習生看。&lt;/strong&gt;我一直認為我也需要有能力跟前輩們一樣解決困難的問題，所以也主動找機會觀察團隊目前在進行的工作是什麼，幫助其他前輩們回答客戶問題、一起解決現實中客戶他們所遭遇的技術挑戰。也記得我以實習生的身份獲得客戶給的正面評價那種快樂 (估計客戶不知道是一個實習生回覆的)，當然也遭遇過很棘手的狀況、被其他團隊的同事洗臉，但都促使我有能力從這些過程學習更多。&lt;/p&gt;

&lt;p&gt;在這短短一年內，我也推進自己通過&lt;a href=&quot;/how-i-pass-aws-all-five-certificate-within-one-year&quot;&gt;AWS 五張主要的核心認證&lt;/a&gt;。當時在台灣其實並沒有那麼多人獲得，在別的部門裡，都知道有個不太正常的實習生竟然在一年內全部都拿到 (&lt;del&gt;變相成為部門主管們 Push 績效的故事&lt;/del&gt;)。&lt;/p&gt;

&lt;p&gt;實習就這麼來到快一年的時間，我的學分也在大四也早就修滿。所以在整個大四，就直接跟系上申請校外實習學分，讓我能夠專心的在 Amazon 工作。基於政策，Amazon 是不會有實習那麼久的情況。即使如此，我的前主管很努力說服很多團隊，讓我仍繼續待在崗位上，努力往下一個階段邁進 (因為我還沒有畢業)，實在是很感激！中間當然也在「要不要考研究所」、「出國留學」、「轉換機會」之間掙扎，但在綜觀考慮下，還是決定進到轉正式職位的過程，於是，又開啟另一波準備面試的流程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Amazon 實習生轉成正職的階段，並不會因為已經過去錄取，就過過水讓你轉成正職。仍然需要經過一定的方式進行考核。&lt;/strong&gt;所以在我的面試中，同樣又安排了三關：技術面試、HR、Hiring manager。&lt;/p&gt;

&lt;p&gt;我的技術面試是西雅圖的同事幫我面的，又是另外一個境界的考題。考了很多 AWS 實務上、設計系統架構、執行部署的策略、細至解釋容器網路的行為等等 …..；HR 則了解人格特質、表明薪水談判的價碼跟期望待遇；Hiring Manager 則是我的前主管再一次的用更複雜的 behavior question 了解我的答案。&lt;/p&gt;

&lt;p&gt;總體而言，並不會因為自己已經是實習生就比較輕鬆。但相對而言，因為經歷一年的大風大浪跟磨練，比起剛步入職場的菜味，相比下，能用更平常的心態面對這些問題。&lt;/p&gt;

&lt;p&gt;一般剛畢業的學生，由於與業界所需求的能力仍有一段差距，所以能獲得的 offer 通常都是 Cloud Support Associate，也就是助理職位。然而，在我的面試官們綜觀討論結果後、包含過去在實習上的表現、有能力解決企業客戶遭遇的問題，前主管非常肯定我的績效、HR 很驚訝一個實習生竟然可以一年內全拿 5 個核心認證 (還與績效無關)、技術考官也覺得我的技術能力不輸一般在外面工作 3-5 年的工程師，所以最終決定以 Engineer (正式工程師) 給予我 offer，開啟我人生一個新篇章。&lt;/p&gt;

&lt;p&gt;聽到這樣的好消息，真的是放下心中的大石頭，就準備畢業當天上工，直到接兵單中間再去服役。&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-am-I-get-into-amazon-before-graduate/amazon-badge-eason.png&quot; alt=&quot;在 Seattle, US 參加 New Hire Training 換到的新 Badge&quot; /&gt;
  
    &lt;figcaption&gt;
      在 Seattle, US 參加 New Hire Training 換到的新 Badge。順帶一提，Amazon 的 Badge 有個有趣的&lt;a href=&quot;https://blog.aboutamazon.eu/discover-whats-behind-the-amazon-id-badges&quot;&gt;小巧思&lt;/a&gt;可以幫助你識別這個人到底是不是老屁股

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;成長心態&quot;&gt;成長心態&lt;/h2&gt;

&lt;p&gt;在投遞這份工作之前，我並不認為自己很有可能會上，當時只是想著：「希望這是一個成長的機會，即使失敗也是一個很寶貴的經驗。」直到跨出那一步嘗試後，才發現其實自己並沒有想像的糟。&lt;/p&gt;

&lt;p&gt;我回顧這些日子以來，我不敢說自己是最優秀的，但很努力是肯定的。在所有同學還不知道畢業後要幹什麼之前，我願意放棄放任時間流逝的機會，即使當時實習沒有學分，還是願意犧牲自己的假日，跨出那一步去學習我有興趣的知識。&lt;/p&gt;

&lt;p&gt;很多人無法理解為什麼我可以錄取這麼大的一間公司，我只能說，其實這得多虧前面長期做了很多累積，在很多人不理解的環境下，仍堅持並持續學習。&lt;/p&gt;

&lt;p&gt;我必須承認，真實世界 80% 的問題都不是學校會教的 (包含我自己面試遇到的問題也是)，學校只會告訴你課本上的答案，並且只是擔任領路人的角色，帶著你入門。我在學校學習的計算機概論、作業系統，考的也是一堆專業術語跟名詞 (e.g. 物件導向三大特性：encapsulation, polymorphism, inheritance / 作業系統怎麼處理 I/O 跟中斷 … )，而且我當時考題還是英文填空，拼錯就沒分 (管你是不是英文母語人士)。&lt;/p&gt;

&lt;p&gt;這種評分機制下，我作業系統跟計算機概論成績還只是 B~C (大概只是剛好及格的程度)，你不見得能夠知道會這些東西有什麼用，但我很清楚背名詞術語跟其用途，只是用來拿分的工具。如果你問我 X 語言怎麼寫出封裝、設定繼承衍生物件，我可以直接給你或是找到對應程式碼的範例。亦或是告訴你我在 Linux 看到哪些指標，具體可以知道現在系統 I/O 處於瓶頸影響到應用程式效能。&lt;/p&gt;

&lt;p&gt;很大一部分都是我在過去自己學習 Linux、操作實務上所累積的知識 (我從高中在 Linode 開啟了第一個虛擬機器並還在管理系統)。網路相關的知識也是過去經歷競賽，學習 CCNA 強化 Layer 1-3 的網路知識，並且在系統維運上也有對應的經驗 (雖然都是小規模)。我對於學習資訊技術本身就有濃厚的興趣，會在網路上搜尋開放式課程，更是把讀 O’Relly 系列的書視為聖經 (不得不說那個質量跟很多台灣出版商發佈的電腦書籍差太多了)。&lt;/p&gt;

&lt;p&gt;在我實習面試過程的問題，大部分都是依照過去經驗回答，在當初實習生的面試前，花一大部分時間讀鳥哥複習 Linux 的知識，但是學校不會把鳥哥寫的書列為參考書單，講講實務上可能會遭遇的問題 (至少我當時是這樣，現在我相信很多老師正努力改革！)。而一直沿用在 1983 年出版第一版、已經出版 20 年以上的 Operating Systen Concepts (恐龍書) 投影片講到退休，所以同學們上完這些課，感想大概就是你聽過但不知道這個東西能做什麼。&lt;/p&gt;

&lt;p&gt;這是台灣教育最有趣的一大特色，我有自信現有的教育制度，絕對能訓練出會考試的學生，但無法培養能夠解決未知問題的學生。所以我十分鼓勵你對有興趣的主題，透過實際遭遇的問題，找找線上資源充實你的求知慾。&lt;/p&gt;

&lt;p&gt;現在很多很棒的線上學習資源，圖書館也有非常多的好書。我相信學習有很多途徑，你也能挖掘到適合你的渠道。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在這短短幾年，我見識到了很多優秀同事工作的方式，並且從中學習好的部分，&lt;a href=&quot;/ten-thing-I-learned-in-amazon&quot;&gt;學習深入研究及思考問題&lt;/a&gt;，幫助自己不斷跨出舒適圈。我一直感受到心態其實也是影響事情結果很大的一項因素。在大部分的情況下，很多人總覺得自己不可能錄取，於是就選擇放棄行動且學習跨出那一步 (定型心態)。但學習以成長心態看待事情，往往可以發掘很多成長的機會，並且努力思考及行動，讓自己更貼近自己理想中的自己。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;當初畢業前我總是心心念念覺得自己應該成為一名軟體開發工程師，但在接觸這項工作內容後，我挖掘到了另一個面向發展的可能性。但當時很多人質疑我選擇這份&lt;a href=&quot;/what-is-cloud-support-engineer-doing-in-amazon&quot;&gt;職位&lt;/a&gt;，是否是一個正確的決定？&lt;/p&gt;

&lt;p&gt;我的前主管告訴我，他過去也從一名研究生、決定工作、後覺再到英國全球前幾名的學校完成 Geography 博士學位 (中間再決定多修一年 Computer Science)、再到創業管理團隊，最後選擇到 Amazon 接下主管職位。&lt;strong&gt;每一個選擇總是有捨及有得，因此，你能做的，就是將目前可能的選擇列出來，然後選擇一個你比較滿意的，你的心中，自然會有答案。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;希望我的故事能帶給閱讀到這邊的你一些啟發。我始終相信人生是一段長而持續的累積，如果你正為了在某個人生的分岔點感到徬徨，請記得，如果五年、十年後往回來看，這不過是人生旅途的一部分，並且，是最有趣的部分。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;往往我們總無法選擇最完美的結果，但我們能夠選擇當下最好的決定，並且努力思考及行動。&lt;/strong&gt;正是因為這樣的過程，才學習如何強大、感受命運帶來的驚喜及餽贈、認知自己的使命，並明白自己要走的方向。&lt;/p&gt;

&lt;h2 id=&quot;看更多系列文章&quot;&gt;看更多系列文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ten-thing-I-learned-in-amazon&quot;&gt;我在 Amazon 學到的 10 件事&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-am-I-get-into-amazon-before-graduate&quot;&gt;我是如何在還沒畢業就錄取並進入到 Amazon 工作&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/what-is-cloud-support-engineer-doing-in-amazon&quot;&gt;Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-i-pass-aws-all-five-certificate-within-one-year&quot;&gt;我是如何在一年內通過 AWS 五大核心認證&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><category term="Cloud Support" /><category term="Cloud Support Engineer" /><summary type="html">很多人好奇我是如何進到 Amazon 工作，並且在還沒畢業前就拿到 offer。今年 (2021) 是我加入 Amazon 這家公司的第三年 (把當 Intern 跟當兵的時間也算進去的話 XD)，所以一次把故事寫在這裡。</summary></entry><entry><title type="html">我是如何在一年內通過 AWS 五大核心認證</title><link href="https://easoncao.com/how-i-pass-aws-all-five-certificate-within-one-year/" rel="alternate" type="text/html" title="我是如何在一年內通過 AWS 五大核心認證" /><published>2021-02-20T00:00:00+00:00</published><updated>2021-02-20T00:00:00+00:00</updated><id>https://easoncao.com/how-i-pass-aws-all-five-certificate-within-one-year</id><content type="html" xml:base="https://easoncao.com/how-i-pass-aws-all-five-certificate-within-one-year/">&lt;p&gt;這是我在 2019 年與同事們分享的內容。當時我們團隊，只有少數人有去考 Solution Architect Associate。而我，是還在當&lt;a href=&quot;/how-am-I-get-into-amazon-before-graduate&quot;&gt;實習生&lt;/a&gt;的時候，在短短一年內，通過 AWS 五張主要的核心認證 (All five)：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/AWS-Developer-Associate-Preparation/&quot;&gt;Certified Developer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/AWS-Certified-Solution-Architect-Associate-Preparation/&quot;&gt;Solution Architect Associate&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/AWS-SysOps-Administrtor-Associate-Preparation/&quot;&gt;SysOps administrator Associate&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/AWS-Certified-DevOps-Engineer-Professional-Preparation/&quot;&gt;DevOps Engineer Professional&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Solution Architect Professional&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我在我同事的推薦下，當時還在內部辦了一個講座，分享自己的學習心得給大家，也讓很多同事陸陸續續獲得想要的認證。&lt;/p&gt;

&lt;p&gt;順帶一提，我是 2017 年三月加入 AWS 才第一次擁有 AWS 帳號並開始學習。所以我想下這個標題分享我當時講了什麼，應該是蠻圖文相符的。&lt;/p&gt;

&lt;h2 id=&quot;certificate-path&quot;&gt;Certificate Path&lt;/h2&gt;

&lt;p&gt;我的第一個認證 (Solution Architect Associate) 是在 2017 年五月拿到的，在這之後我就連續安排了學習計畫及考試：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/certificate-path.png&quot; alt=&quot;我在一年內的 AWS 認證歷程&quot; /&gt;
  
    &lt;figcaption&gt;
      我在一年內的 AWS 認證歷程

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;難易級別&quot;&gt;難易級別&lt;/h2&gt;

&lt;p&gt;當時 2019 年剛開始出現 Specialty (專家級) 類型的認證：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificates-in-2019.png&quot; alt=&quot;2019 年的 AWS 認證種類&quot; /&gt;
  
    &lt;figcaption&gt;
      2019 年的 AWS 認證種類

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;當然 AWS Certificate Team 也不是什麼事情都不做，這中間幾乎每 6 - 12 個月就誕生一個新種類。截至我寫這篇內容為止 (2021)，現階段又多了很多 Specialty (專家級) 的認證類型。&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificates-in-2021.png&quot; alt=&quot;2021 年的 AWS 認證種類&quot; /&gt;
  
    &lt;figcaption&gt;
      2021 年的 AWS 認證種類

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;我個人第一張認證是 Solution Architect Associate，但再轉考 Certified Developer Associate 時，唯獨大概要學習怎麼算一下 DynamoDB 的 Capacity，因此覺得特別簡單 (因為都只是考 API)。&lt;/p&gt;

&lt;p&gt;但如果依照難易度種類區別，我會把其難易度分為以下，這也是廣泛 AWS Certificate SME (認證專家) 認知的難易程度：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificates-difficulty-levels.png&quot; alt=&quot;AWS 認證難易度&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS 認證難易度 &lt;em&gt;(級別為個人意見僅供參考)&lt;/em&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;我的學習歷程&quot;&gt;我的學習歷程&lt;/h2&gt;

&lt;p&gt;我的準備策略除了依循上述的難易級別循序考上去之外。以下分幾點列舉出我在準備及學習 AWS 歷程上，對我十分有幫助的幾項重點：&lt;/p&gt;

&lt;h3 id=&quot;1-建立正確的心態&quot;&gt;1. 建立正確的心態&lt;/h3&gt;

&lt;p&gt;我考取認證的主要心態，並不是「為了考取認證」、「為了獲得認證」這種想法而去選擇準備，而是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;我希望我能透過認證的方式，有系統性的學習 AWS 服務&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;我並不認為通過認證就代表真的非常有能力在 AWS 上建構系統，並且在 AWS 上所向無敵，但相對而言，擁有基本且足夠的知識，能夠幫助你知道如何解決當前所遭遇的問題，我僅將其視為一項過程&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;我認為考取認證是充實不同面向知識的一種方式，並且希望從學習過程中在面對問題時，汲取「如何 / How」相關的經驗及知識&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;對我來說，&lt;strong&gt;考取認證不是目的，而是學習的一部分&lt;/strong&gt;。所以我並不認為一次的沒通過算什麼 (雖然當時要求自己可以的話就試著&lt;strong&gt;「一年全數通過」&lt;/strong&gt;)。相反，我反而能更加了解自己不太熟悉的內容是什麼。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(不過我每次都盡可能在考試日前，甚至犧牲假日努力準備跟複習，所以至少都有及格)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;而且這種考試根本與什麼定生死的學測不同，沒過還可以 Retake。&lt;/p&gt;

&lt;p&gt;所以其實將其視為一種學習的循環，相較而言，準備起來更為輕鬆，更像是一種時時刻刻檢視自己學習的過程 (不過當時為了不想浪費錢，要跳向 Professional 的考試真的給他拖有點久)。&lt;/p&gt;

&lt;h3 id=&quot;2-釐清考試目標及大綱&quot;&gt;2. 釐清考試目標及大綱&lt;/h3&gt;

&lt;p&gt;因此，我會推薦 Blueprint (考試指南) 一定要閱讀，這能在你安排準備該認證時，有清楚的藍圖知道這個認證著重要學習的重點是什麼。你通常可以在認證頁面找到對應的位置下載：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificate-check-blueprint.png&quot; alt=&quot;檢視 AWS 認證提供的指南&quot; /&gt;
  
    &lt;figcaption&gt;
      檢視 AWS 認證提供的指南

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;如果你仔細注意，你一定可以知道每個認證項目著重的重點是什麼，例如 DevOps Engineer Professional：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificate-blueprint-devops-engineer-professional.png&quot; alt=&quot;AWS DevOps Engineer Professional Blueprint&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS DevOps Engineer Professional 考試指南 (2021 年)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;從裡面你可以確實注意到 CI/CD 相關的項目佔了不小的成分。如此一來，我就會知道需要努力學習 AWS 上能夠滿足 DevOps 相關的解決方案及 Stack，並且透過 Whitepapaer、re:Invent 等影音，知道如何實踐 CI/CD 等策略。例如：CloudFormation、Beanstalk、CodeSuite (CodeCommit, CodeBuild, CodePipeline) 等等。&lt;/p&gt;

&lt;p&gt;但如果你看了 Solution Architect Professional，你會發現就幾乎是什麼都要沾一點邊：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/aws-certificate-blueprint-solution-architect-professional.png&quot; alt=&quot;AWS Solution Architect Professional&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS Solution Architect Professional 考試指南 (2021 年)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;所以我也認為 Solution Architect Professional 其考試真的有一些難度，當時的方法就是把產品頁面展開，然後看一看這些產品都是在做什麼。並且基於 Solution Architect Associate 的基礎再更進階有關高可用性等問題，大量透過實際案例做更深入的閱讀及學習理解。&lt;/p&gt;

&lt;h4 id=&quot;associate-類型的考試&quot;&gt;Associate 類型的考試&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;閱讀 Whitepapaer 並且針對你有興趣學習的服務打開&lt;a href=&quot;https://docs.aws.amazon.com/index.html&quot;&gt;AWS 文件&lt;/a&gt;閱讀你感興趣的章節。&lt;/li&gt;
  &lt;li&gt;由於 AWS 服務非常多，有些教育單位會提供模擬試題，盡可能了解回答及錯誤的 AWS 服務，並且再回頭看看文件或是做做 Lab，幫助你加深印象。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;professional-類型的考試&quot;&gt;Professional 類型的考試&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;沒有什麼訣竅能夠幫助你快速理解所有 AWS 服務。我推薦的方式是在 YouTube 上找找 re:Invent 系列的影片 (尤其是 Deep Dive 類型) 並且安排時間學習。re:Invent 系列的影片常常會包含許多實務應用範例及使用情境，甚至可以學習客戶的使用案例，這有助於幫助你了解一些複雜問題下該如何實踐最佳實務。&lt;/li&gt;
  &lt;li&gt;觀看 re:Invent 影片後推薦實際去玩一玩這些 AWS 服務，這有助於加深印象。&lt;/li&gt;
  &lt;li&gt;Professional 級別的考試因為題目又臭又長，每題都是情境題，因此更需要正確規劃作答時間。如果第一眼不是那麼確定，可以先選一個概略的答案，並且在答題系統上標記，等完成後再回來檢視。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-分散式學習&quot;&gt;3. 分散式學習&lt;/h3&gt;

&lt;p&gt;我的學習場域大部分都是在捷運上 (沒錯，就是捷運)：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/metro.png&quot; alt=&quot;Study on Metro&quot; /&gt;
  
    &lt;figcaption&gt;
      我的學習場域

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;在我的學習過程中，常常借助了大腦比較喜愛的學習模型 (這有機會再寫文章探討)。我會習慣將學習材料切割成多天分散學習，以幫助我在建構這些知識時，大腦有充分時間能夠組織及強健知識：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/distributed-study.png&quot; alt=&quot;分散式學習&quot; /&gt;
  
    &lt;figcaption&gt;
      分散式學習

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;有的人可能習慣在考前一天抱佛腳 (但往往考完就什麼都忘了)&lt;/li&gt;
  &lt;li&gt;有的人可能習慣在考前一週集中 (這通常也是考試導向，往往最終獲得的知識狀態也不見得很融會貫通)&lt;/li&gt;
  &lt;li&gt;或是分散幾天一小時 (有的人大腦需要一些時間喚醒才能進到之前學習的狀態)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;學習並沒有最佳做法，只有最適合你的方式。但已經有許多研究顯示，分散學習確實有助於建立大腦記憶區塊。&lt;/p&gt;

&lt;p&gt;在我的案例中，我大部分時間仍然是透過通勤時間 (20 - 30 分鐘) 聽著及看著學習材料幫助我了解不同項目，然後到辦公室繼續做其他事，假日有時間再額外看看或是做做 Lab。&lt;/p&gt;

&lt;p&gt;我一直認為通常說的「沒時間」其實只是表示「我覺得這件事情不重要」。當你認知這件事情有需要去做時，你總能安排時間。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;但當時在旁人眼裡，我就是一路盯著手機走出捷運站然後一路走到辦公室，外面的紛紛擾擾與我無關&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;在這篇內容中，我分享了我自身在過去如何於一年內獲得五張 AWS 核心認證的經驗，並且與你分享一些實際上可以參考的應用方式。如果你正在準備 AWS 認證，希望這樣的內容對你有幫助。&lt;/p&gt;

&lt;p&gt;如果你覺得這篇內容不錯，可以分享或是按個 Like / Clap。&lt;/p&gt;

&lt;h2 id=&quot;看更多系列文章&quot;&gt;看更多系列文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ten-thing-I-learned-in-amazon&quot;&gt;我在 Amazon 學到的 10 件事&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-am-I-get-into-amazon-before-graduate&quot;&gt;我是如何在還沒畢業就錄取並進入到 Amazon 工作&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/what-is-cloud-support-engineer-doing-in-amazon&quot;&gt;Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-i-pass-aws-all-five-certificate-within-one-year&quot;&gt;我是如何在一年內通過 AWS 五大核心認證&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><summary type="html">這是我在 2019 年與同事們分享的內容。當時我們團隊，只有少數人有去考 Solution Architect Associate。而我，是還在當實習生的時候，在短短一年內，通過 AWS 五張主要的核心認證 (All five)：</summary></entry><entry><title type="html">我在 Amazon 學到的 10 件事</title><link href="https://easoncao.com/ten-thing-I-learned-in-amazon/" rel="alternate" type="text/html" title="我在 Amazon 學到的 10 件事" /><published>2021-02-20T00:00:00+00:00</published><updated>2021-02-20T00:00:00+00:00</updated><id>https://easoncao.com/ten-thing-I-learned-in-amazon</id><content type="html" xml:base="https://easoncao.com/ten-thing-I-learned-in-amazon/">&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/ten-thing-I-learned-in-amazon/day-1.png&quot; alt=&quot;Every day is day 1&quot; /&gt;
  
    &lt;figcaption&gt;
      Every day is day 1

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;今年 (2021) 是我加入 Amazon 這家公司的第三年 (把當 Intern 跟當兵的時間也算進去的話 XD)。在加入這個組織這短短 3 年中，我明顯感受到自己的成長與變化，畢竟 Amazon 也是存活了 20 幾年的公司，在全世界雇用了上百萬名員工，並且仍不斷在成長。在組織運作執行上確實是有非常不得了的一套，許多原因使得我仍願意與企業組織一同成長，以下列舉幾項我至今覺得很寶貴並且感知受用的幾件體會：&lt;/p&gt;

&lt;h2 id=&quot;1-快速解決問題比起使用什麼技術來得重要-完美是更迭出來的&quot;&gt;1. 快速解決問題比起使用什麼技術來得重要 (完美是更迭出來的)&lt;/h2&gt;

&lt;p&gt;在過去，我完全就是一個有技術潔癖的人 (比如我是一個 Python 開發者我就一定要用 Python 的 tool stack 完成我所有的工作)，我很執著在解決問題前思考需要使用什麼樣的技術，目的可能是考慮了：擴展、比較嚴謹、之後可能比較不會出問題、想趁機會學習新東西、可以變成一項話題 (比如可以發表或是展現技術應用等) …… 等等。&lt;/p&gt;

&lt;p&gt;總之不管什麼原因，可能在處理一個問題時會審慎思考自己用的技術 (比如：需要爬一些網頁資料、整合一些資訊變成另一種形式的操作模型 (比如輸出成報表、戳自己的 webhook 發通知)、設計應用跑運算)，或是基於完美主義覺得一定要所有事情一氣呵成。&lt;/p&gt;

&lt;p&gt;拿爬網頁資料來說，我可能要：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;構建 Python 應用程式&lt;/li&gt;
  &lt;li&gt;查一下需要的套件跟操作方法，比如 requests&lt;/li&gt;
  &lt;li&gt;跑 pip install 安裝需要的套件&lt;/li&gt;
  &lt;li&gt;測試 Python 應用程式是否能正確運作&lt;/li&gt;
  &lt;li&gt;Debug 再回去修改應用到爽&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這還只是拿 Python 當範例，事實是可能來來回回都花了兩個小時，包含編譯打包做了一大堆工作，但是真正要解決的問題才有可能被解決。&lt;/p&gt;

&lt;p&gt;但是在隨著這樣的處理模型下，我發現人們更在乎的是「解決問題」，而不是你使用了什麼樣的技術。客戶的需求跟問題只會每天地不斷地在變化，對於小問題，人們更注重於能否在有效的時間內解決問題。&lt;/p&gt;

&lt;p&gt;所以現在，在工作上我可能傾向於使用 Linux 命令加上 Shell script 就可以完成我想做的大部分工作，可能只耗費我 30 分鐘做一個版本，只是步驟會被拆解成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拿 curl 把 html 資料爬回來&lt;/li&gt;
  &lt;li&gt;grep 一下！可能在用個正規表達式過濾一下我想要的訊息&lt;/li&gt;
  &lt;li&gt;把結果貼到 Sublime / 文字編輯器在處理下篩選我要的內容&lt;/li&gt;
  &lt;li&gt;需要重複這個操作，Shell Script 寫個幾行就搞定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;亦或者是我想要定期爬 facebook 上的資料，如果是用原生語言硬幹，我可能會：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;構建 Python 應用程式&lt;/li&gt;
  &lt;li&gt;查一下需要的套件跟操作方法，比如 requests&lt;/li&gt;
  &lt;li&gt;跑 pip install 安裝需要的套件&lt;/li&gt;
  &lt;li&gt;模擬使用者登入並且處理 facebook 上的驗證操作&lt;/li&gt;
  &lt;li&gt;測試 Python 應用程式是否能正確運作&lt;/li&gt;
  &lt;li&gt;登入之後在爬爬感興趣的 HTML 資料進行字串處理&lt;/li&gt;
  &lt;li&gt;測試 Python 應用程式是否能正確運作&lt;/li&gt;
  &lt;li&gt;將有興趣的資料進行下一步處理 (比如觸發 Webhook)&lt;/li&gt;
  &lt;li&gt;Debug 再回去修改應用到爽&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而我只要在我的執行環境中使用 Greasemonky，並且跑一台 VM 開著瀏覽器，完全可以做到上述的工作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安裝 Greasemonky&lt;/li&gt;
  &lt;li&gt;寫一寫 Javascript 把 DOM 跟感興趣的內容撈出來 (完全不用搞驗證)&lt;/li&gt;
  &lt;li&gt;Greasemonky 可以操作跨站存取 (CORS)，所以我可以把撈出來的資料吐到其他地方或是觸發 Webhook&lt;/li&gt;
  &lt;li&gt;在 Greasemonky 腳本裡面插一點檢查機制，發現資料撈不出來可以發個通知手動檢查下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;雖然問題拆了幾個部分，並且不是那麼漂亮 (沒有一氣呵成的感覺)，但是，整體開發跟配置相對簡化非常多工作。&lt;/p&gt;

&lt;p&gt;如果只是一些小規模或是立即性的需求，這種工作模型卻足以滿足大部分的工作。而且會發現，其實人們對於使用組合技 (特別是對於技術人員來說)，普遍接受程度並沒有自己想像中的低 (因為畢竟大家都只專注在解決問題)。&lt;/p&gt;

&lt;p&gt;一旦真的出現規模需要或是下一步更進階的使用需求後，再來設計原生語言的套件也不是一件特別困難的事情。此外，我發現這種模型間接幫助你驗證整體的業務邏輯，再改用原生語言進行實作，其實在開發上像是有了一個規格，也相對「清楚要做什麼」，並且容易許多。&lt;/p&gt;

&lt;h2 id=&quot;2-這個世界變化的比你想像中的快--and-its-always-day-1&quot;&gt;2. 這個世界變化的比你想像中的快 (… and, it’s always day 1)&lt;/h2&gt;

&lt;p&gt;Amazon 提供的雲端服務 (Amazon Web Services) 提供上百種不同的服務及解決方案，這其中都是上上下下數千個團隊跟不同部門合作下一起構建 (而且是不同跨時區下協同，幾乎整整 24 小時都有人在進行)。&lt;/p&gt;

&lt;p&gt;常常過幾個星期就又有團隊迸出新的功能或是服務要上線、一覺醒來又有新的變革，亦或是在你還沒準備好了解這項變動之前，已經有客戶想知道這些釋出的功能的具體細節 (我從沒有跟上客戶的腳步過)。&lt;/p&gt;

&lt;p&gt;在還沒加入這樣規模的組織前，我其實無法想像這個世界是多麽積極的在變化，尤其在台灣這樣的小島，每天媒體告訴你、周圍朋友在 Instagram / facebook 告知發生的事情，其實都大同小異，圈圈內流通的資訊更迭的速度其實並沒有那麼快，大家都很被動的感受自己接收的資訊。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;但在跨時區分工合作的運作下，你必須習慣睡覺醒來一切又有變化的事實，並且是超級快速，每一天都像是第一天 (Day 1)。這迫使你會需要練習快速學習的能力以適應這個世界的更迭，不斷主動更新自己的知識跟技能。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-學習必須有效並深入核心而不是只停留於表面-dive-deep&quot;&gt;3. 學習必須有效並深入核心而不是只停留於表面 (Dive Deep)&lt;/h2&gt;

&lt;p&gt;在 Amazon 提及的 14 條 &lt;a href=&quot;https://www.amazon.jobs/en/principles&quot;&gt;Leadeship Principles&lt;/a&gt;，有一個我感知最深的項目就是 Dive Deep (追根究底)。&lt;/p&gt;

&lt;p&gt;我在面試很多候選人後，我發現很多科技圈的人才都有一個普遍的慣性，就是對於新技術或是知識的理解都停留於 Surface Level (表面)，即使是自己熟悉的技術或能力也是。很多人往往對於接受一個知識或技術的理解都停留於表面就感到已經完全理解，但在我的工作中，往往是面對未知甚至是很多應用的 Bug，並且找到一個適當的解決方案。如果保持這樣的思考慣性，很容易導致在遇到問題時無法切入並解決核心問題。&lt;/p&gt;

&lt;p&gt;以大家很常說自己會的 Kubernetes 舉例來說，通常問以下問題 95% 的候選人都回答不出來：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;為什麼 Kubernetes 在同一個 Pod 中，不同的容器可以使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt; 彼此訪問？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你通常就是看書上說說知道 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pod&lt;/code&gt; 是什麼，並且會說是一個 Kubernetes 特有的元件。但往往就是這麼一個簡單的問題 (網路上也一堆解答)，讓我發現很多人對於接受一項知識的理解往往停留在：文件如此寫、會操作 kubectl 命令並且部署、看範例就是如此、Kubernetes 直接劃分一個可共用的網路環境 (然後就無限鬼打牆 … 例如：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;它就是一個抽象元件&lt;/code&gt; 等答案)。&lt;/p&gt;

&lt;p&gt;但如果你真的試著想要進一步了解其底層運作原理，你會發現其實 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pod&lt;/code&gt; 並不是什麼特別的東西，網路的共用特性與 Docker 相同技術定義的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Network sandbox&lt;/code&gt; / ECS 定義的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task&lt;/code&gt; 相似，這使得其可以使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt;，並且都基於 Linux Kernel 提供的功能執行類似虛擬化的操作。一旦你試著對知識刨根究柢，這在處理有關 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pod&lt;/code&gt; 網路相關的問題時，才會有更清楚的脈絡知道要往哪個層級剖析 (是容器？系統？還是外部網路？)。&lt;/p&gt;

&lt;p&gt;當然我也不是一開始就如此，而是在經歷過各種亙古難題，並且每天都得想辦法解決未知問題的環境訓練下而養成。&lt;strong&gt;也因為如此，我更重視自己在接受知識時，必須以更深層次思考的習慣，並與現有理解嘗試融會貫通，並不斷反思自己現有的知識跟理解是否正確及深入。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;這個 Dive Deep 也不是指要你鑽牛角尖，而是了解以更實際及應用的層次，逆推回去學習並引導自己往更核心的底層近一步理解。正是意識到這樣的能力培養在日常工作中的必要性，我仍不斷學習並強化自己的知識理解，進一步培養自己對於問題切入剖析核心的技巧，幫助我解決所遭遇未知的問題。&lt;/p&gt;

&lt;p&gt;這樣的原則我感受到不同領域上如此應用，通常能引導自己能提出更深層次的解答、或是嘗試在挖掘更多具體我所需要的資訊，並且有效的行動。&lt;/p&gt;

&lt;h2 id=&quot;4-不必太執著於工作但工作起來必須很執著&quot;&gt;4. 不必太執著於工作，但工作起來必須很執著&lt;/h2&gt;

&lt;p&gt;Amazon 有一句膾炙人口的標語：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Work hard, Have fun, Make history&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;看到這裡你覺得 Amazon 就是一家非常 &lt;strong&gt;Work hard&lt;/strong&gt; 的血汗工廠 (加上一堆新聞都說 Amazon 很血汗)。但事實是 Amazon 並不希望自己的員工 Overtime 加班，並且盡可能優化不同團隊之間工作效率。&lt;/p&gt;

&lt;p&gt;因此，在內部我們不斷的每天都在想著如何改善產品、改善服務、改善所有不太 OK 的事情，並且專注這些目標，努力在工作崗位上做出一些改變，而不是滿足於現狀，這是我們所謂的 &lt;strong&gt;Work hard&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;在這裡，老闆會耳提面命希望你不要一直加班，並且要求你必須注重自己的私人時間及生活，否則很快就燃燒殆盡 (Burnout)，領導階層也會時時刻刻了解是不是有任何工作負載過於集中的問題 (有點類似避免單點故障的概念 / Single point failure)。&lt;/p&gt;

&lt;p&gt;透過維運的角度盡可能幫忙你找到對應的 backup (換句話說，在身處周遭都是優秀的同事，其實你總能被取代，無需在自己的私人時間太執著於工作，並且需要相信你的隊友)。&lt;/p&gt;

&lt;p&gt;當然，該認真工作時絕對是很可怕的。在工作上，除了會有各式的績效及 KPI 你會需要想辦法滿足 (其實並不是特別困難)。&lt;strong&gt;但更重要的是，因為身邊的同事都十分優秀，你必須很認真的思考自己的職涯規劃並設定目標，想辦法讓自己成長、努力工作改善現有的問題及自我，並把自己跳出舒適圈往更高的階段推，而非滿於現狀，否則很快面臨瓶頸。&lt;/strong&gt;如果你是一個很樂於學習及成長的心態，在這個組織下工作是十分有趣的。(反之，你可能待不到幾個月就想走了 XD)&lt;/p&gt;

&lt;p&gt;在這裡，大家不是搏感情比誰更努力工作加班，而是比誰能更有效率的完成事情。&lt;/p&gt;

&lt;h2 id=&quot;5-在必要的地方花錢不手軟而不是無限度擴張福利&quot;&gt;5. 在必要的地方花錢不手軟，而不是無限度擴張福利&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;(我的解讀為：錢要花在刀口上)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 Amazon 工作，你真的會覺得很「摳」，你會發現組織對於「免費午餐」、「員工旅遊」這類的事情錙銖必較。在過年過節也不會有所謂的三節獎金、上班提供按摩之類的服務 ….。當初 Package 跟你談好多少就是多少，而且不多也不少 (在 Leadeship Principles 有一條 Frugality，完全就是體現這項原則)。&lt;/p&gt;

&lt;p&gt;但是相對的，若是對員工生產力、有助於客戶的事情，花錢絕對不手軟，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;需要擴充硬體設備進行開發&lt;/li&gt;
  &lt;li&gt;員工手機帳單可以報帳以提供在工作上穩定的網路服務&lt;/li&gt;
  &lt;li&gt;需要跨國出差，只要不是亂花錢，都願意全部包機票住宿及食宿給你飛過去 (我有幾次來回美洲幾星期，光一趟就報帳報掉快 7000 USD，折合約 10 - 20 幾萬台幣….)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;並且在教育資源及訓練上投入大量的資源，十分專注幫助員工成長。目的都是為了提供給予客戶更好的服務及產品、解決方案。&lt;/p&gt;

&lt;p&gt;有時候大家選擇工作可能優先都以公司提供什麼樣的福利及薪資為考量，這並沒有什麼問題，但最怕的是往往沒有意識到談來的薪水可能是一攤死水。然而，Amazon 招聘最為厲害的地方在於，其在吸引人才的招聘政策永遠不是用薪資及福利作為主要的手段 (當然薪水我相信在市場上也是很有競爭力的，但比起其他頂尖的外商我相信有更誇張的數字)，其更重視的是候選人願意用成長的角度加入這家公司，並且積極冒險及試驗，你會更感受到你跟公司的關係更像是合作夥伴，並且珍惜你手中握有的股票，這正是 Amazon 持續十幾年來一直在做的事情，致使你會願意參與這家公司的成長 (然後看著它市值一直一路往上，像是看著自己的孩子長大一樣)。&lt;/p&gt;

&lt;h2 id=&quot;6-做出初始版本跟寫好一頁的文件比起做好投影片重要&quot;&gt;6. 做出初始版本跟寫好一頁的文件比起做好投影片重要&lt;/h2&gt;
&lt;p&gt;在 Amazon，我們很少做投影片，你會發現公司上上下下沒有人在專注弄絢麗的投影片樣板 (除非是給外部客戶看的，不然通常都單調並且樸實無華)。大家反而更專注把自己的 Document (文件) 寫好，如果你要說服你的老闆、提交升職、提出一個計畫或改進，在這裡做簡報並把一切描繪&lt;del&gt;(吹噓)&lt;/del&gt;的多美好，是不太會有人理你的。&lt;/p&gt;

&lt;p&gt;文件一般來說，通常會是 1 頁 (內容不超出一頁)，至多也不會超過 6 頁。在文件的內容上，都明確寫下了具體的 data point (需要有事實的資料陳述) 還有你的計畫、或是績效，因此，寫好一份文件比設計投影片更來得困難。因為 …. &lt;strong&gt;你會發現做了一堆事結果可能在文件上只有一兩行字 ……&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;構建美好幻想不如實際做出一個初始版本，並且收集到相應的資料 (就算沒有初始資料也必須擁有相應的資料點幫助陳述你的計畫說服其他人)，並且讓他人閱讀你的文件後進行評論。在接受這樣的工作模式幾年後，我也覺得這種方式是一個很有效率的操作，&lt;strong&gt;由於你會需要累積足夠的 data points 置入，會更清楚知道自己的目標跟預期計劃是什麼，而不是埋著頭一直做，再讓老闆憑感覺同意你說的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在我的團隊，大家分享內容後，比較常聽到的是「我把今天提到的內容、相關的資訊也整理成 wiki 跟文件 (或錄影，可以使用 1.5x 倍觀看)，有問題可以再讓我知道」而不是「簡報我已經分享在 …，有需要可以自己去下載」&lt;/p&gt;

&lt;h2 id=&quot;7-能不開會就不開會要開也盡可能開得快&quot;&gt;7. 能不開會就不開會，要開也盡可能開得快&lt;/h2&gt;

&lt;p&gt;在 Amazon，員工每天都會匿名接受調查對於管理層的相應執行績效 (我們每天都會為管理層執行評分)，其中很常問的類似問題就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;The meeting I've is ... ?&lt;/code&gt; (類似可回答的答案: effective / non effective) 或像是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my workload is manageable?&lt;/code&gt;  這類的問題。&lt;/p&gt;

&lt;p&gt;你可以注意到 Amazon 一直在嘗試減少不必要會議的數量，並且盡可能幫助員工有效地執行工作，包含我所在的團隊，通常只有 team meeting 為慣例會議，並且主管也是快速用幾分鐘講講近期的更新或變動就直接進 Round table (讓團隊成員看看有沒有問題需要提出來的)，一週平均會議時間都低於 2 小時，在其他地區有的團隊也是實行 Stand-up meeting (站著開會，所以開太久會站很酸)，可見大家真的很不愛會議。&lt;/p&gt;

&lt;p&gt;即使 Manager 召集的會議，但若有其他重要事項在處理，如果沒什麼特別的事項，是有權拒絕參加的，並且由他人幫忙轉達 (而且通常大家都偏好能不開會就不開會，這點管理層或是上上下下的團隊大部分都很有共識)。通常會議的目的就是進行決議，或是傳遞必要訊息，工作上也會統計每週花在 meeting 上的時間，如果太多，你可能會需要審慎思考是不是真的有必要開這些會。&lt;/p&gt;

&lt;p&gt;其他例子像是：所有 interviewer 面到後面投票決定不錄取一位面試者，HR 事先也已經透過意見調查收集大家的共識，會議就會直接取消，而無需裝忙開會。&lt;/p&gt;

&lt;h2 id=&quot;8-搞懂-priority-優先順序-跟重要性比起做一堆-kpi-來得重要&quot;&gt;8. 搞懂 Priority (優先順序) 跟重要性比起做一堆 KPI 來得重要&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;(如果不是很重要，能不做就不做，而不是一昧瞎忙)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在上萬人的組織中工作，你會發現每天要做的事情真的是很多，且每天收到的 Email 就像雪花般的飄進來。由於每個人都很忙，每個團隊都有自己的問題需要解決，有時候你做了一堆 KPI 跟數字，卻到頭來發現並不能長遠解決團隊所遭遇的問題，以及契合目前公司營運的發展目標。這體現了正確工作的重要性，因此每次在新的季度，領導階層一定會告訴大家 KPI 跟目標是什麼。&lt;/p&gt;

&lt;p&gt;在每天被塞爆的工作中，&lt;strong&gt;你需要時時刻刻檢視自己正在做的事情是不是真的有必要，並且是完成重要的工作&lt;/strong&gt;。在這裡達到 KPI 是應該的，並沒有什麼特別。&lt;/p&gt;

&lt;p&gt;大家反而會不斷提出的問題是：「這真的是我們需要做的嗎？真的是重要的嗎？長遠來看對團隊是有幫助並能改善問題的嗎？」&lt;/p&gt;

&lt;p&gt;因此，正確的配置任務的優先順序，並且有效平衡你的時間運用。正確滿足 KPI 需求之外，同時需要契合你的成長目標更顯重要；而不是一昧地瞎忙，或是出於人情幫忙做了一堆事情 (搞得自己表面很忙)，卻不符合團隊的營運目標。&lt;/p&gt;

&lt;h2 id=&quot;9-在很重要的事情十分快速在不重要的事情無限拖延&quot;&gt;9. 在很重要的事情十分快速，在不重要的事情無限拖延&lt;/h2&gt;

&lt;p&gt;Amazon 真的是道道地地的美國企業。你完全可以在這裡感受到外國人做事的&lt;strong&gt;效率&lt;/strong&gt; (在有些事情上真的是超 …. 級 …. 慢 ….)。像是我曾經為了&lt;a href=&quot;https://easoncao.com/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/&quot;&gt;在 AWS 官方部落格發佈一篇文章&lt;/a&gt;，整整花了半年 (其實也就 2 個季度、6 個月，我已經見識過擺至少 1-2 年的待辦事項，仍懷著感恩的心)。&lt;/p&gt;

&lt;p&gt;像是在台灣你訂個包裹可能等個 3 天就快受不了了，但自從我在 Amazon 工作後，有些卡在其他團隊的事情，如同前面提到的，每個團隊都很忙。如果不是那麼緊急的事情，我在幾天至一個星期內得到回覆，真的是說不出的感激，並且覺得很快。在這樣的環境下，完全顛覆我過去的時間觀念，並且練就一身好耐性。&lt;/p&gt;

&lt;p&gt;在這裡，大家會對重要的事情十分快速，並且立刻做出相應的行動 (例如發現有 Bug 需要 Roll back 以解決一個非常嚴重的影響)。但如果優先順序並不是那麼急迫，或是影響並不是那麼大，那你得預期可能會是一場持久戰，甚至很可能並不會被解決，並且需要抱持著不斷騷擾人的心理準備。&lt;strong&gt;如果想加快，你必須用 data Point (數據) 說服其他團隊為你做事，而不是只因為你想或是你職位比較大。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;事實上，我認識很多國外的同事們並不愛加班、到晚餐時間一堆商店都關門，但這樣做出來他們的工作結果並不差。反過來真的會思考亞洲人拼命的文化到底是在拼什麼 (大陸的 996 工作制度真的有比較好？)，太執著於細節或是一些小事情，似乎並沒有帶來更大的效益，也許這也是值得你我學習的工作模式。&lt;/p&gt;

&lt;h2 id=&quot;10-永遠思考對客戶是否有幫助及符合長遠的目標而不是只因短期利益選擇做事&quot;&gt;10. 永遠思考對客戶是否有幫助及符合長遠的目標，而不是只因短期利益選擇做事&lt;/h2&gt;

&lt;p&gt;在我的工作中，時常會在不同的解決方案中建議客戶選擇合適的項目。若是一般的商業利益考量，大部分會為了短期績效及利益，可能建議客戶使用不見得適合他們且昂貴的方案。當然，這確實有助於增加年度營收。然而，這並不符合 Amazon 追求對於客戶服務的目標。在這裡，我們會想著如何幫客戶省錢 (例如: &lt;a href=&quot;https://aws.amazon.com/tw/blogs/aws/eks-price-reduction/&quot;&gt;Amazon EKS 折 50% 的定價&lt;/a&gt;、&lt;a href=&quot;https://aws.amazon.com/tw/blogs/compute/aws-fargate-price-reduction-up-to-50&quot;&gt;AWS Fargate 降價&lt;/a&gt;)，並且思考如何提供更好的服務及產品反饋給客戶，與客戶建立信賴關係。同時，為其帶來長遠的收益 (&lt;a href=&quot;https://aws.amazon.com/tw/10year/&quot;&gt;AWS 幾乎用了整整 10 年創新&lt;/a&gt;，並由虧轉盈)。即使可能短期效果並不是立即的反應，但這一直是 Amazon 做事的風格。&lt;/p&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;最後讓我用 VP - James Hamilton (同時也是分散式系統的專家) 在 2016 年發表的文章進行總結，與你分享我在這幾年切身感受的體會：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As a member of the AWS engineering team, my first impressions are probably best summarized as fast. Decisions are made quickly. New ideas end up in code and available to customers at a speed that just makes the pace of enterprise IT look like continental drift. In a previous role, I remember (half) jokingly saying “we ship twice a decade whether customers need it or not.” Now new features are going out so frequently they are often hard to track.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;作為AWS工程技術團隊的一員，新環境給我留下的第一印象就是一個“快”字-決策制定流程非常迅速，新思路能夠很快以代碼形式推出，並立即被交付至客戶手中 。這一切都讓傳統企業IT的響應速度看起來像是大陸板塊漂移。我記得自己曾經半開玩笑地回憶過往角色稱呼：“我們在十年中只發布了兩次客戶可能需要，也可能不需要 的更新。”現在新功能正以驚人的頻率推出，我們甚至很難追踪其推進節奏。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another interesting aspect of AWS is how product or engineering debates are handled. These arguments come up frequently and are as actively debated at AWS as at any company. These decisions might even be argued with more fervor and conviction at AWS but its data that closes the debates and decisions are made remarkably quickly. At AWS instead of having a “strategy” and convincing customers that is what they really need, we deliver features we find useful ourselves and we invest quickly in services that customers adopt broadly. Good services become great services fast.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;AWS的另一種有趣的特質在於對產品和工程技術相關的處理方式。各種爭議會不斷出現，而且AWS內部的辯論之聲要遠遠超過任何其他企業。這些決策的流程讓我可以選擇， AWS除了擁有卓越的數據處理能力外，也能夠快速中止逐步並存並採取出征性意見。在AWS中，我們不再自以為是地制定“戰略”並強行說服客戶認同其適用性，而又推出了適合自身業務環境 的方案，並通過面向服務的快速投入幫助更多客戶快速享受至由其帶來的便利。如此一來，優秀的服務能夠快速取代為卓越的服務。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(source: &lt;a href=&quot;https://perspectives.mvdirona.com/2016/03/a-decade-of-innovation/&quot;&gt;A Decade of Innovation&lt;/a&gt; / &lt;a href=&quot;https://aws.amazon.com/cn/blogs/china/a-decade-of-innovation/&quot;&gt;中文: AWS的十年创新之路&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;看更多系列文章&quot;&gt;看更多系列文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ten-thing-I-learned-in-amazon&quot;&gt;我在 Amazon 學到的 10 件事&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-am-I-get-into-amazon-before-graduate&quot;&gt;我是如何在還沒畢業就錄取並進入到 Amazon 工作&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/what-is-cloud-support-engineer-doing-in-amazon&quot;&gt;Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-i-pass-aws-all-five-certificate-within-one-year&quot;&gt;我是如何在一年內通過 AWS 五大核心認證&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><summary type="html">Every day is day 1</summary></entry><entry><title type="html">Amazon 的 Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)</title><link href="https://easoncao.com/what-is-cloud-support-engineer-doing-in-amazon/" rel="alternate" type="text/html" title="Amazon 的 Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)" /><published>2021-02-20T00:00:00+00:00</published><updated>2021-02-20T00:00:00+00:00</updated><id>https://easoncao.com/what-is-cloud-support-engineer-doing-in-amazon</id><content type="html" xml:base="https://easoncao.com/what-is-cloud-support-engineer-doing-in-amazon/">&lt;p&gt;AWS Cloud Support Engineer 主要面向的客戶受眾便是使用 AWS (Amazon Web Services) 服務的客戶，並為這些客戶提供訂閱制的支持計畫。(沒錯，也就是說，我們是付費的服務。)&lt;/p&gt;

&lt;p&gt;由於這項服務屬於訂閱制，費用可以單月從 &lt;strong&gt;Developer Support Plan (29 USD)&lt;/strong&gt; 至 &lt;strong&gt;Enterprise Support Plan (15,000 USD)&lt;/strong&gt; 以上不等。因此，在我的工作裡，我每天會接觸各式各樣的規模的客戶。小至獨立、新創開發團隊，大至市值規模全球前 10 大的公司都有。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(有的客戶可能這輩子只開一台 EC2 Instance，但同時，有的客戶可能單一個 AWS Account、單一個 AWS Region，就擁有數百至數千個大規格的 EC2 Instance)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/what-is-cloud-support-engineer-doing-in-amazon/amazon-bear-chen.png&quot; alt=&quot;我敬重的同事 Teddy Chen&quot; /&gt;
  
    &lt;figcaption&gt;
      我敬重的同事 Teddy Chen

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;面對的客戶&quot;&gt;面對的客戶&lt;/h2&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/what-is-cloud-support-engineer-doing-in-amazon/support-plan-price.png&quot; alt=&quot;AWS Support Plan 定價&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS Support Plan 定價 - &lt;a href=&quot;https://aws.amazon.com/tw/premiumsupport/plans/&quot;&gt;來源: 比較 AWS Support 計劃&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;15,000 USD 光換算台幣就已經 40 幾萬，什麼服務都還沒有用就先燒一堆錢，聽起來好像傻子才去買 Enterprise Support。但很多時候，我們的客戶會十分願意花錢買我們的 Enterprise Support，你可以繼續往下看，就知道原因是什麼。&lt;/p&gt;

&lt;p&gt;首先，AWS Support 針對不同級別的客戶提供了不同程度的響應時間：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/what-is-cloud-support-engineer-doing-in-amazon/support-plan-type.png&quot; alt=&quot;AWS Support Plan SLA&quot; /&gt;
  
    &lt;figcaption&gt;
      AWS Support Plan SLA - &lt;a href=&quot;https://aws.amazon.com/tw/premiumsupport/plans/&quot;&gt;來源: 比較 AWS Support 計劃&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;以 Premium Support 服務來說 (也就是 Cloud Support Engineer 的工作)，一旦購買 Support Plan，最大的優勢就是你可以開 Case 進來詢問我們團隊任何有關 AWS 相關的技術問題，並會有對應的專家提供解答。但這時候購買什麼 Support Plan 的差異就出現了，如果你是 Enterprise Support 的客戶，其回答的優先權絕對是很高的，即是只是詢問一般性指導問題 (e.g. A 服務怎麼使用、使用 B 有沒有什麼需要注意的細節)，服務 SLA 最久的保證時間是 24 小時內，你一定可以獲得回覆。此外，針對嚴重的問題，會有更快的響應速度。&lt;/p&gt;

&lt;p&gt;若屬於最嚴重的案例 (業務關鍵系統當機，通常是遭遇 service outage)，在客戶開啟案例的同時，就會有資深的技術支持工程師在 15 分鐘內響應協助客戶解決。並且持續追蹤、提供相應的技術建議，以幫助快速定位問題，並盡快幫助客戶恢復系統業務的工作。&lt;/p&gt;

&lt;p&gt;另外，若是簽訂 Enterprise Support，Enterprise Support Plan 的客戶會擁有專有的 Technical Account Manager，不斷地幫助客戶像 AWS 內部團隊反饋問題 (像是客戶的專屬窗口)、Solution Architect Team 也會協助客戶檢視系統架構設計的可行性。&lt;/p&gt;

&lt;p&gt;同時還會有 Concierge 團隊幫助你怎麼優化每個月的帳單、提供怎麼訂閱 RI (Reserved Instance) 的優化建議，對於 500 人以上的大規模企業，借助 AWS 的資源，總體可以幫助客戶節省非常多的費用。&lt;/p&gt;

&lt;p&gt;並且，這某種程度上幫助降低客戶端維運團隊的複雜性，專注在發展重要功能，並交付 AWS 的專業團隊及經驗快速幫助定位、協助解決很多超難又複雜的問題，並從中與我們合作互相學習成長。這正是許多客戶選擇購買 Enterprise Plan，以幫助他們加速業務成長的主要原因。&lt;/p&gt;

&lt;h2 id=&quot;cloud-support-engineer-的日常&quot;&gt;Cloud Support Engineer 的日常&lt;/h2&gt;

&lt;p&gt;Meet our Global Mandarin Premium Support Team at AWS:&lt;/p&gt;

&lt;p&gt;(以下影片有點久遠，不過可以大略了解這份職位的特性)&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/EgSHXUmiuDk&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;(What’s it like to work at Amazon Web Services?)&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/GC3bWcFFZTo&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;在我撰寫這篇內容的同時，AWS 已經擁有超過 100 種不同的雲端服務及相關的解決方案產品。由於不同產品有其複雜性，因此，Cloud Support Engineer 的工作主要是協助客戶在使用這些服務時所遭遇的問題給予相關的技術建議及指導。&lt;/p&gt;

&lt;p&gt;同時，在工作中也會需要協同不同團隊 (Development Team、Solution Architect Team、Account Team / Account Manager / Technical Account Manager) 亦或者是其他團隊，幫助解決客戶所遭遇的問題。&lt;strong&gt;由於 Cloud Support Engineer 最主要負責的技術是 AWS 相關的產品，每個工程師都會是相關服務的領域專家，因此，有的時候也可能是在協助 Amazon 內部其他團隊在使用 AWS 相關服務的問題。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在你第一天入職時，Amazon 就會有非常完善的技術訓練計畫，並且擁有累積多年的實務經驗，你還會有專屬的 Mentor。因此在前幾個月你會非常專注學習相關的知識，學習過去不同 Case 的處理方式，並在幾個月後完全的有能力處理不同規模客戶的案例。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;這些案例問題涵蓋的範圍眾多，除了一般可能詢問怎麼設定、API 怎麼使用、客戶剛入門的級別 (我們稱之為很甜的案例)，&lt;strong&gt;大部分情況，我們處理的問題通常都不是 “Happy Case”，客戶會問的問題也都不是寫在文件裡面的。大部分時間我們都在挖掘真實世界的問題並且給予實質建議&lt;/strong&gt;，比如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“服務中斷”&lt;/li&gt;
  &lt;li&gt;“網站掛點, 救命!”&lt;/li&gt;
  &lt;li&gt;“資料庫連不上”&lt;/li&gt;
  &lt;li&gt;“好像有東西不太對勁並且無法正確運行”&lt;/li&gt;
  &lt;li&gt;“為什麼我的應用程式跑了一陣子就會自己 Crash?”&lt;/li&gt;
  &lt;li&gt;“為什麼我的生產環境 / 應用無法解析 DNS?”&lt;/li&gt;
  &lt;li&gt;“如何升級的時候不要有服務中斷?”&lt;/li&gt;
  &lt;li&gt;“為什麼我的生產環境 / Cluster遷移到 AWS 就不能工作?”&lt;/li&gt;
  &lt;li&gt;“救命, 我的應用程式 / 服務在遭遇大流量的時候會崩潰”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在我的工作上，得需要在短時間內學習新技術的技能，並且持續不斷的學習 (由於 AWS 團隊很多，產品迭代速度很快，往往我們都還不會之前客戶就已經在問了)。同時，很多情況客戶會混合不同服務一起使用，亦或者是大規模的集群有東西壞掉。因此，你必須要有能力剖析在複雜架構下，真正問題的核心點是什麼，並且有效的進行問題的定位。&lt;/p&gt;

&lt;p&gt;同時，很多情況下，&lt;strong&gt;基於資料安全性及隱私權政策，往往我們都不會有權限存取客戶的資料、了解他實際運行的邏輯是什麼。這也意味著：我們沒有權限 SSH/登入 到客戶的環境裡面看設定、不知道客戶的 Code 是跑什麼、不確定客戶到底連接了哪些服務。所以你必須在這種情況下，還得知道如何有效的幫助客戶解決問題。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;但我們的團隊在上述的情況下，還可以明確地告訴你要收集哪些東西，然後開啟以下對話 …：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;哦！我從你收集回來的 kernel dump 確定是你的 Python 應用程式在 UTC 時間 XX 有問題，可能是有 memory leak，你要不要檢查一下&lt;/li&gt;
  &lt;li&gt;哦！你抓回來的封包明顯看得出來你的應用程式主動發 FIN 關閉連接耶，估計是超出應用程式可允許的等待時間。所以這不是 MySQL Server 關的，你要不要確認一下你語言 / Library 預設用的 timeout 時間?&lt;/li&gt;
  &lt;li&gt;哦！你的 Kubernetes Node 一直是 NotReady 是吧？從你收集回來的東西很明顯就是 kubelet 壞了啦！你說怎麼壞的？從我的分析可以注意到 Disk performance 不太 OK，你是不是跑了 I/O Bound 的應用啊？ …. 你說你不知道開發又寫了什麼埋坑讓你不能下班？好吧，所以你可以試試 …. etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(我們通常不會跟客戶這樣說話，但大意大概就是如此)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;很多人可能會以為這份工作就是一般的客服人員 / 低階接線生，就我在這個團隊幾年下來，我只能說這是大大的誤解，並且抱持偏見的角度在批判這份專業性的工作。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;因此，要用其他方式描述我們的工作內容，我一直認為類似於&lt;strong&gt;「急診室的醫生」&lt;/strong&gt;是滿貼切的說法。在客戶遭遇系統嚴重影響 (比如系統中斷、故障、service outage) 的情況。&lt;strong&gt;醫生也是秉持其專業需要在短時間內判斷下一步的動作是什麼，以盡快的緩解病患所遭遇的問題，並且適當的安排正確的處理優先順序&lt;/strong&gt; (所以為什麼在忙碌的急診室要排隊看發燒)。在我們的工作中，也會根據客戶所遭遇的問題嚴重等級正確區分 Priority，&lt;strong&gt;一直以來都視客務關鍵性業務系統當機為優先，以幫助降低客戶遭遇系統中斷而導致營收受損的影響。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在我們的工作中，提供了 24 小時 x 7 天 x 365 的工作模型，這意味著 Premium Support Team 在無時無刻都會有人在線上協助客戶所遭遇的問題。基於這種工作模型，我們的團隊屬於跨國的工作型態，在全世界各地都有相應的團隊執行交接。因此在台灣的下午及晚上，我們會將業務移轉至歐洲及美洲時區的同事執行跟進。&lt;/p&gt;

&lt;p&gt;我的工作中會使用中文及英文兩種不同語言協助客戶的問題。由於用中文在 AWS 服務客戶仍然是一個很稀缺的技能 &lt;strong&gt;(在看這篇的你如果有訂閱 AWS Support Plan，請記得我們支持用中文開 Case，只需要在標題加上 [Mandarin] 即可！)&lt;/strong&gt;，所以一大部分我的團隊會協助使用中文的客戶解決他們所遭遇的問題。&lt;/p&gt;

&lt;p&gt;另一部分仍是協助全球世界各地使用英文為主的客戶、工程師亦或是 Amazon 內部的團隊。&lt;/p&gt;

&lt;h3 id=&quot;特殊活動監控&quot;&gt;特殊活動監控&lt;/h3&gt;

&lt;p&gt;遇到 Enterprise 客戶有大型活動或是線上業務特別熱鬧的時候 (例如：Black Friday、雙 11、搶票活動 …. 等等)，這時候就是我們會特別祈禱別發生鯊魚咬斷電纜之類的事件。有時候對於非預期的大規模流量導致任何非預期情況發生時，資深的 Cloud Support Engineer 就會進場幫忙排除可能的問題是哪些、釐清可能觸及到的系統限制、幫助客戶緩解問題，降低停機時間的影響。&lt;/p&gt;

&lt;h3 id=&quot;training--sme&quot;&gt;Training &amp;amp; SME&lt;/h3&gt;

&lt;p&gt;Cloud Support Engineer 擁有完善的訓練計畫，並且有豐富的資源及經驗使得你可以接受到非常深入的技術訓練 &lt;strong&gt;(強度頗高)&lt;/strong&gt;。在我們的組織中，同時也有 &lt;strong&gt;Subject Matter Expert (SME)&lt;/strong&gt; 的角色存在，很多 Cloud Support Engineer 都會很努力的爭取該項角色，其代表的是某些特定 AWS 服務的領域專家，並且分佈在全球世界各地 (但一個服務如果越新，全球不到 5 位 SME 是有可能的)。&lt;/p&gt;

&lt;p&gt;不過你通常也不需要擔心沒有人可以為你提供相關的建議。這使我們很多情況我們會需要跟不同區域的外國同事交談，以討論深入的技術問題。&lt;/p&gt;

&lt;p&gt;要成為 SME 其有一定的門檻，但總體而言，在成為 SME 前必須要累積一定數量及技術水平的 Show Case (實際的客戶案例) 進行至少由兩位現有專家的審查。通過之後會另行安排技術面試，時間可能是半夜或是早上 (通常是配合其他時區的專家)。面試的考官除了有現有專家外，還會有開發那個服務的資深開發工程師加入，全程英文對話來測驗你對於服務的了解及深度，最終再由考官們投票看看是否一致通過。這過程真的是有很多曲折離奇的故事能寫，&lt;strong&gt;但我相信 Amazon 對於工程師的技術標準要求仍是有一定的水準，所以絕對不是一個純接線生這麼簡單。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cloud Support Engineer 如果成為某個領域的專家，通常也會想辦法貢獻自己的知識 (透過內部文件、製作訓練教材或是幫忙其他團隊審核公開發佈的內容)，並也有機會成為 Trainer，參與跨國訓練其他地區同事的機會。&lt;/p&gt;

&lt;h3 id=&quot;奧客與第三方軟體&quot;&gt;奧客與第三方軟體&lt;/h3&gt;

&lt;p&gt;這一定每個行業都會有的，在我的工作中，常會收到負面情緒的客人通常都是購買 Developer Support Plan 的客戶，常常會挨著說每個月要充值 29 USD 太貴、問一般性問題都只能回 Email、能不能直接登入/SSH 到我的機器幫我設？&lt;/p&gt;

&lt;p&gt;就我的觀察，這種客戶通常也是寧可花時間耗，而不是想解決問題。通常也不願意花力氣學習 AWS，並總希望有人幫忙幹到好。這種客戶典型會問的問題也像是：為什麼我的 EC2 關了還是會一直重開，你們是不是亂動我的帳戶！！！！&lt;/p&gt;

&lt;p&gt;(結果進場看個 3 秒就注意到客戶完全不知道自己在幹嘛開了 EC2 Auto Scaling)。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;小知識：AWS 非常重視 Secuirty &amp;amp; Privacy，特別關注客戶存放在雲端上資料的機密性及安全性。AWS Support 沒有權限更改客戶的資料/SSH 進入到客戶的機器，我們光要看客戶的一些你都覺得沒什麼的配置 (比如這個帳戶在某個區域開了幾台 EC2)，都有嚴謹的稽核制度。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通常處理一個案例，我們都得想辦法引導並還得拜託客戶提供及收集一些資訊，以幫助我們確認下一步的動作或是進行分析。對於手裡握著核彈的嬰兒來說，還是建議找找 Partner 提供的 Support 計畫吧。&lt;/p&gt;

&lt;p&gt;此外，還有些客戶會很喜歡詢問第三方軟體的一些問題 (比如 Terraform)。由於有些第三方軟體並不是我們能夠協助，也不是由 AWS 開發，這通常已經超出 AWS 所能協助的範圍。如果你跑來問 AWS Support，我們只能竭盡所能回答。然後你總是發現，即使我們可能看一眼知道怎麼修並且提供一些可能的建議，我們只能再無奈地請你關閉 Case，並請你去找 Partner (如果是 AWS 有合作的夥伴並且涵蓋在支持清單，我們會協助轉交至對應的團隊)。&lt;/p&gt;

&lt;p&gt;但畢竟我們並不是該產品的專家，如果你遇到 Bug，我們真的無法負責，這是一種心有餘但力不足的哀傷。&lt;/p&gt;

&lt;h2 id=&quot;技術挑戰&quot;&gt;技術挑戰&lt;/h2&gt;

&lt;p&gt;在有的情況，我們會需要跟 AWS 開發團隊合作，以改進產品上所遭遇的問題。在我的工作上，很多時候可能是客戶遭遇了服務功能不支持、或是產品有一些已知問題。如果 Cloud Support Engineer 想到能提供其他解決方法，我們會建議客戶採取相應的行動，例如我之前在網站上發佈的幾篇內容，都是我們客戶實際遭遇，並且提供一些可用案例的例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/&quot;&gt;CoreDNS(kube-dns) resolution truncation issue on Kubernetes (Amazon EKS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/&quot;&gt;[AWS][EKS] Zero downtime deployment(RollingUpdate) when using ALB Ingress Controller on Amazon EKS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述過程涵蓋的技術分析只是部分的案例，對於我們內部工程師在分析問題時，這通常是我們會需要從中挖掘的深度，並從中嘗試提供建議給客戶。但如果是要建議開發團隊修復的已知問題，同樣也會提供類似上述的分析報告，並且協同開發團隊在下一個版本中修復。&lt;/p&gt;

&lt;p&gt;這種長期在挖掘我們家產品的 Bug，每次甩分析報告過去建議他們怎麼修 Bug 的模式，其實已經某種程度上形成良好的循環，讓我們贏得開發團隊的信任，跟開發達成良好的合作關係。(我甚至也認識很多很猛的同事是直接幫忙貢獻代碼的)&lt;/p&gt;

&lt;p&gt;在我的團隊中能參與並能從中學習成長，事實是非常有成就感的一件事情。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;因此，很多情況下，我們會需要學習用宏觀的角度分析並且切入問題核心，並且給予正確建議，而非只看單一問題亂槍打鳥，這是我認為這份工作最困難也最有趣的地方。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;跟開發-sde-差在哪&quot;&gt;跟開發 (SDE) 差在哪&lt;/h2&gt;

&lt;p&gt;我們跟開發團隊相同共享存取服務原始程式碼的權限。所以有的時候，比較資深或是厲害的工程師，會協助開發找出應用程式的 Bug，並且指出要修正的細節。即使在我的團隊，仍有很多同事過去的背景從事開發、維運等不同角色。但在篩選標準上，技術的水平仍與 Amazon 開發團隊擁有著一致的標準，只是這個工作角色對於編程 (Coding / Programming) 的能力並不是必須。&lt;/p&gt;

&lt;p&gt;但往往有時候擁有開發的經驗，在執行這份工作上，也能幫助你更加有效的定位問題的可能 (若屬於應用程式的問題的話)。&lt;strong&gt;與開發團隊最大的差異便是我們的工作不是在寫 code、做 feature，而是在解技術問題、每天都在 troubleshooting！(有的開發團隊工程師甚至不見得知道怎麼 troubleshooting，需要我們耐心引導協助客戶調查核心問題，這是也最有趣的部分)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;並且工作型態固定，偏向 work-life balanced。&lt;/p&gt;

&lt;p&gt;(但這並不代表你不能開發，很多 Cloud Support Engineer 還是會自己兼著做很多好用的工具，內部還是有很多專案能幫忙做的)。&lt;/p&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;即使擔任雲端技術支持工程師約 1-2 年，我仍然對每天幫助人們應對棘手的技術挑戰感到興奮。我還是為了有機會每天學習這些實際案例而高興，並且能夠想辦法幫助不同產業客戶很多很偉大的業務達到成功 (大規模數據運算, 購物, 手持應用, Streaming, Travling, 加密貨幣交易 … 等)，確保客戶的服務運作在 AWS 上面保有高可用性和可靠性，以持續能運作 24 小時 x 365 天。最有趣的特別是面對一些未知問題最終發覺是個 Bug (不管是 K8s, Docker …)。&lt;/p&gt;

&lt;p&gt;在 AWS / Amazon，在全球真的有很多優秀的人、超級酷的 Manager 還有超強的同事。在這裡工作就像是玩一場又一場遊戲，這個遊戲需要我們花很多力氣分享很多分析報告並且嘗試打爆每一個看起來不是那麼友善的問題，只為了提供更好的產品跟技術服務給到客戶。我很高興能夠參與超級多場類似這種遊戲，並且能跟很多優秀的工程師和開發團隊合作。&lt;/p&gt;

&lt;p&gt;如果你正在疑惑 Cloud Support Engineer 是什麼樣的工作，希望這篇的內容能夠有助於你了解我們的工作日常。&lt;/p&gt;

&lt;p&gt;若你對於這樣的工作環境及內容有所興趣並躍躍欲試，我們仍在持續招聘優秀的人才加入我們，你可以附上 CV 並透過我的 LinkedIn 與我聯繫。&lt;/p&gt;

&lt;h2 id=&quot;看更多系列文章&quot;&gt;看更多系列文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ten-thing-I-learned-in-amazon&quot;&gt;我在 Amazon 學到的 10 件事&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-am-I-get-into-amazon-before-graduate&quot;&gt;我是如何在還沒畢業就錄取並進入到 Amazon 工作&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/what-is-cloud-support-engineer-doing-in-amazon&quot;&gt;Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/how-i-pass-aws-all-five-certificate-within-one-year&quot;&gt;我是如何在一年內通過 AWS 五大核心認證&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><category term="Cloud Support" /><category term="Cloud Support Engineer" /><summary type="html">AWS Cloud Support Engineer 主要面向的客戶受眾便是使用 AWS (Amazon Web Services) 服務的客戶，並為這些客戶提供訂閱制的支持計畫。(沒錯，也就是說，我們是付費的服務。)</summary></entry><entry><title type="html">身為 DevOps 你會想知道的 AWS 技巧 - 使用 AWS Lambda 和 Amazon SNS 取得來自 AWS CodeCommit (Git) 的檔案變更通知</title><link href="https://easoncao.com/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/" rel="alternate" type="text/html" title="身為 DevOps 你會想知道的 AWS 技巧 - 使用 AWS Lambda 和 Amazon SNS 取得來自 AWS CodeCommit (Git) 的檔案變更通知" /><published>2021-02-06T00:00:00+00:00</published><updated>2021-02-06T00:00:00+00:00</updated><id>https://easoncao.com/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit</id><content type="html" xml:base="https://easoncao.com/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/">&lt;p&gt;這篇內容主要是轉載我 2019 年 2 月公開發佈在 AWS 官方 DevOps 部落格的內容，在這篇內容中，主要展示了如何以 AWS Lambda 及 Amazon SNS 接收來自 AWS CodeCommit (Git Repository) 的檔案變更通知。由於屆時將滿 2 年 (時間過得真快)，覺得有必要翻譯成中文文件，以幫助中文的讀者也能夠了解這項內容。&lt;/p&gt;

&lt;p&gt;由於原內容為英文，如果有興趣，原文請見：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/devops/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/&quot;&gt;Using AWS Lambda and Amazon SNS to Get File Change Notifications from AWS CodeCommit&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;通知一直是 DevOps 工作流程中很重要的一環，幾乎很多牽扯維運相關的工作都少不了主動通知的行為。當然，你可以在任何 CI/CD 的階段中透過你已知的方法於任何邏輯中設置。但在這篇 Blog Post 中，我將會展示如何整合 AWS Lambda 和 Amazon SNS 以擴展 AWS CodeCommit 的功能性。特別的是，這篇解決方內容案描述了當 AWS CodeCommit 一旦有任何更新，如何從 Amazon SNS 中接收檔案變更和 Commit 訊息。以下主要簡介相應使用到的 AWS 服務：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/sns/&quot;&gt;Amazon SNS&lt;/a&gt;: Amazon Simple Notification Service (Amazon SNS) 是一項全受管簡訊服務，並且能夠幫助你發佈訊息至訂閱者。其非常容易使用並且支援任何規模大小的使用場景。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;AWS Lambda&lt;/a&gt;: 是一種無伺服器的運算服務，可讓您執行程式但不必佈建或管理伺服器、建立工作負載感知叢集擴展邏輯、維護事件整合或管理執行階段。使用 Lambda，您可以透過虛擬方式執行任何類型的應用程式或後端服務，全部無需管理。在這篇內容中，我使用了 Lambda Function 以推送訊息至 Amazon SNS 以發佈檔案更新。
Amazon CloudWatch&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/cloudwatch&quot;&gt;Amazon CloudWatch&lt;/a&gt; 提供資料和可行的洞見以監控應用程式、回應整個系統的效能變化、優化資源使用情況，以及透過整合的檢視來查看運作狀態。&lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html&quot;&gt;你可以也設定簡易的規則來偵測 AWS 資源的變更&lt;/a&gt;。在 CloudWatch 捕捉到來自你 AWS 資源的事件更新後，便能觸發特定的目標執行相應的操作 (例如：觸發一個 Lambda Function)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;為幫助你快入了解並且部署這項解決方案，我同時建立了一個 AWS CloudFormation template 以供這篇內容使用。&lt;a href=&quot;https://aws.amazon.com/cloudformation/&quot;&gt;AWS CloudFormation&lt;/a&gt; 是一個管理工具並且能夠使用通用的語法幫助你描述並且部署 AWS 相關的基礎建設和資源。&lt;/p&gt;

&lt;h2 id=&quot;概覽-overview&quot;&gt;概覽 (Overview)&lt;/h2&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/architecture.png&quot; alt=&quot;架構概覽&quot; /&gt;
  
    &lt;figcaption&gt;
      架構概覽

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;AWS CodeCommit 支持了多項實用的 &lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type&quot;&gt;CloudWatch Event&lt;/a&gt;，透過 CloudWatch Event，這能夠幫助你監控 AWS 資源的使用事件變更並且進行通知。透過設定一些簡單的觸發規則，你便能夠偵測有關 branch 或是 Repository 的變更。&lt;/p&gt;

&lt;p&gt;在這個範例中，我為一個 AWS CodeCommit Repository 建立了一個 CloudWatch event rule (事件規則)，如此一來，任何相應的變更事件便會觸發一個 Lambda Function。當對於 CodeCommit 的變更執行時，CloudWatch 便偵測該事件並且執行自定義 Lambda Function 的觸發。&lt;/p&gt;

&lt;p&gt;當這個 Lambda Function 被觸發，下列的行為將被依序執行：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用 CodeCommit API 中的 &lt;a href=&quot;https://docs.aws.amazon.com/codecommit/latest/APIReference/API_GetCommit.html&quot;&gt;GetCommit&lt;/a&gt; 操作取得最後一次的 Commit 紀錄。因為我想要比較原先上一次的 Commit ID 和最新的一筆，以進行變更的檢查。&lt;/li&gt;
  &lt;li&gt;對每一個 Commit 紀錄，使用 &lt;a href=&quot;https://docs.aws.amazon.com/codecommit/latest/APIReference/API_GetDifferences.html&quot;&gt;GetDifferences&lt;/a&gt; API 操作取得任何紀錄追蹤檔案的新增、變更和刪除資訊。&lt;/li&gt;
  &lt;li&gt;從比較的結果中，合併相關的變更資訊，並且，將這項資訊依照定義好的 Email 訊息格式推送到 Lambda Function 中環境變數中定義的 Amazon SNS (SNS topic) 資源。&lt;/li&gt;
  &lt;li&gt;允許 Reviewers (可能是 Code Reviewers, operation team … 等) 訂閱該 SNS Topic。如此一來，任何有關 CodeCommit 相應的更新都能推播到相關的訂閱者。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這個範例使用了 Python 和 Boto3 實作這項功能。完整的程式碼已被公開在 GitHub 上，你可以在 AWS 官方的 GitHub 上找到 - &lt;a href=&quot;https://github.com/aws-samples/aws-codecommit-file-change-publisher&quot;&gt;aws-codecommit-file-change-publisher&lt;/a&gt; 該範例。&lt;/p&gt;

&lt;h2 id=&quot;開始動手做&quot;&gt;開始動手做&lt;/h2&gt;

&lt;p&gt;為了幫助你快速搭建這項解決方案，該專案包含了一個 AWS CloudFormation template (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher.yml&lt;/code&gt;)。這個 template 使用了 &lt;a href=&quot;https://github.com/awslabs/serverless-application-model&quot;&gt;AWS Serverless Application Model (AWS SAM)&lt;/a&gt; 用以定義 CodeCommit 通知 Serverless 應用程式必須的組建，並且使用簡潔的語法表示。&lt;/p&gt;

&lt;p&gt;一旦部署後，這個 template 會被直接轉譯成 AWS CloudFormation stack 資源並且建立一個 SNS Topic, CloudWatch event rule 和一個 Lambda Function。這個 Lambda Function 程式碼已經展示了一個簡易的通知功能用例。你可以使用這項範例程式自行定義你的邏輯，甚至是使用其他 &lt;a href=&quot;https://aws.amazon.com/sdk-for-python/&quot;&gt;AWS SDK for Python (Boto3)&lt;/a&gt; 提供的 API 和方法擴展功能。&lt;/p&gt;

&lt;h3 id=&quot;預先準備工作&quot;&gt;預先準備工作&lt;/h3&gt;

&lt;p&gt;在開始部署該範例之前，你必須先&lt;a href=&quot;https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-repository.html&quot;&gt;建立/擁有一個 CodeCommit repository&lt;/a&gt;，以便後續使用 AWS CloudFormation template 執行相應的操作。在這篇文章中，我在 Ohio 區域 (us-east-2) 建立了一個乾淨的 CodeCommit Repository (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample-repo&lt;/code&gt;) 以便展示一個 CodeCommit Repository 在特定 branch 上有檔案修改更新。&lt;/p&gt;

&lt;p&gt;如果你已經擁有一個 CodeCommit Repository，你可以繼續往下閱讀下列步驟以部署 template 和 Lambda Function。&lt;/p&gt;

&lt;h3 id=&quot;部署-aws-cloudformation-template-及-lambda-function&quot;&gt;部署 AWS CloudFormation template 及 Lambda function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;下載 &lt;a href=&quot;https://github.com/aws-samples/aws-codecommit-file-change-publisher&quot;&gt;aws-codecommit-file-change-publisher&lt;/a&gt; 上的原始碼&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;登入至 AWS Console 及 &lt;a href=&quot;https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/getting-started.html#select-region&quot;&gt;選擇你 CodeCommit Repository&lt;/a&gt; 所在的區域。並且，手動建立一個 S3 Bucket 並且上傳 AWS Lambda deployment package (封裝好的 zip 文件 - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher.zip&lt;/code&gt;)。如果你不確定如何建立 S3 Bucket，請參考 Amazon S3 Console 的操作手冊 - &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html&quot;&gt;How Do I Create an S3 Bucket?&lt;/a&gt; 以引導你完成&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;上傳 Lambda deployment package 至你剛剛建立好的 S3 Bucket&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在這個範例中，我在相同區域 (Ohio, us-east-2) 建立了一個 S3 Bucket 名為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher&lt;/code&gt; 並且透過 Amazon S3 上傳 Lambda deployment package：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/upload-lambda-package-to-s3-bucket.png&quot; alt=&quot;上傳 Lambda deployment package 至 S3&quot; /&gt;
  
    &lt;figcaption&gt;
      上傳 Lambda deployment package 至 S3

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在 AWS Management Console, 選擇 CloudFormation 導引到該服務。你也可以直接使用這個連結訪問 - &lt;a href=&quot;https://console.aws.amazon.com/cloudformation&quot;&gt;https://console.aws.amazon.com/cloudformation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;選擇 &lt;strong&gt;Create Stack&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在 &lt;strong&gt;Select template&lt;/strong&gt; 頁面，選擇 &lt;strong&gt;Upload a template to Amazon S3&lt;/strong&gt;，指定剛剛下載的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher.yml&lt;/code&gt; template 檔案上傳進行下一步&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/upload-cfn-template.png&quot; alt=&quot;選擇 CloudFormation template&quot; /&gt;
  
    &lt;figcaption&gt;
      選擇 CloudFormation template

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;在指定參數項目中，填寫以下資訊：
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Stack Name&lt;/strong&gt;: codecommit-sns-publisher (你可以指定自己的名稱)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CodeS3BucketLocation&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher&lt;/code&gt; (指定你剛剛在建立 S3 Bucket 步驟中上傳 Lambda deployment package 的 S3 Bucket 名稱)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CodeS3KeyLocation&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecommit-sns-publisher.zip&lt;/code&gt; (這是上傳 Lambda deployment package 的名稱, 物件應為 zip 檔案)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CodeCommitRepo&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample-repo&lt;/code&gt; (你 CodeCommit repository 的名稱)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;MainBranchName&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master&lt;/code&gt; (指定你想觸發事件的 branch 名稱)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;NotificationEmailAddress&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user@example.com&lt;/code&gt; (指定要訂閱 SNS topic 的 Email 地址, 這個設置可以讓 CloudFormation template 建立一個 SNS topic 以推送通至訂閱者)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/cfn-typing-parameters.png&quot; alt=&quot;指定參數 Parameters&quot; /&gt;
  
    &lt;figcaption&gt;
      指定參數 Parameters

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;選擇 &lt;strong&gt;Next&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在 &lt;strong&gt;Review&lt;/strong&gt; 頁面中，於 &lt;strong&gt;Capabilities&lt;/strong&gt; 項目底下，勾選以下選項：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;I acknowledge that AWS CloudFormation might create IAM resources.&lt;/li&gt;
      &lt;li&gt;I acknowledge that AWS CloudFormation might create IAM resources with custom names.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/cfn-create-change-set-acknowledgement.png&quot; alt=&quot;選擇 Capabilities&quot; /&gt;
  
    &lt;figcaption&gt;
      選擇 Capabilities

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;在 &lt;strong&gt;Transforms&lt;/strong&gt; 項目，點擊 &lt;strong&gt;Create Change Set&lt;/strong&gt;。AWS CloudFormation 便會開始執行 template 轉譯的工作並且建立一個 Change Set&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/cfn-create-change-set-transforms.png&quot; alt=&quot;建立 Change Set&quot; /&gt;
  
    &lt;figcaption&gt;
      建立 Change Set

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;在完成轉譯後，選擇 &lt;strong&gt;Execute&lt;/strong&gt; 以建立 AWS CloudFormation stack&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/preview-change-set.png&quot; alt=&quot;執行建立 CloudFormation Stack&quot; /&gt;
  
    &lt;figcaption&gt;
      執行建立 CloudFormation Stack

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;在 Stack 建立完成後，若資源有正確建立，你應該可以預期會在信箱收到 SNS 訂閱確認信，請點擊確認，看到以下內容即成功訂閱：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/sns-subscribe.png&quot; alt=&quot;SNS 訂閱確認成功訊息&quot; /&gt;
  
    &lt;figcaption&gt;
      SNS 訂閱確認成功訊息

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;在你訂閱了 SNS Topic 之後，你便可以在 AWS CloudFormation Console 檢查對應建立出來的資源。如果你想要監控 Lambda Function 的執行狀態，點擊 &lt;strong&gt;Resources&lt;/strong&gt; 可以開啟 Lambda Function(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SNSPublisherFunction&lt;/code&gt;)：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/create-cfn-stackCWE.png&quot; alt=&quot;CloudFormation 部署的資源&quot; /&gt;
  
    &lt;figcaption&gt;
      CloudFormation 部署的資源

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;現在你可以在本機嘗試推送一個-commit-至遠端的-aws-codecommit-repository&quot;&gt;現在，你可以在本機嘗試推送一個 Commit 至遠端的 AWS CodeCommit Repository&lt;/h2&gt;

&lt;p&gt;Step 1. 下載你的 (git clone) CodeCommit repository 至你的本機電腦。更多有關連接到 AWS CodeCommit 和驗證的資訊，請參考 AWS CodeCommit 使用手冊中的 &lt;a href=&quot;https://docs.aws.amazon.com/en_us/codecommit/latest/userguide/how-to-connect.html&quot;&gt;Connect to an AWS CodeCommit Repository&lt;/a&gt; 內容幫助你設定。在這個範例中，展示了如何下載位於 Ohio 區域 (us-east-2) 名為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample-repo&lt;/code&gt; 的 repository：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone ssh://git-codecommit.us-east-2.amazonaws.com/v1/repos/sample-repo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step 2. 進入至該專案目錄並且見一粒一個純問自檔案&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;sample-repo/
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'This is a sample file'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; newfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step 3. 新增一個 Commit 並且紀錄這次的修改&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git add newfile
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Create initial file'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[輸出]&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[master (root-commit) 810d192] Create initial file
1 file changed, 1 insertion(+)
create mode 100644 newfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step 4. 推送到遠端的 CodeCommit Repository&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git push &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; origin master:master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[輸出]&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Counting objects: 100% (3/3), done.
Writing objects: 100% (3/3), 235 bytes | 235.00 KiB/s, done.
…
* [new branch]      master -&amp;gt; master
Branch 'master' set up to track remote branch 'master' from 'origin'.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在本機的 Commit 更改推送到遠端 CodeCommit Repository 後，將會觸發 CloudWatch event 並且偵測到這次的更新。你通常可以在訂閱的 Email 帳戶中預期看到以下的通知訊息：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Commit ID: &amp;lt;Commit ID&amp;gt;
author: [YourName] (YourEmail@example.com) - &amp;lt;Timestamp&amp;gt; +0000
message: Create initial file

File: newfile Addition - Blob ID: &amp;lt;Blob ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;在這篇內容中，我展示了如何使用 AWS CloudFormation template 快速部署一個範例的解決方案，能夠幫助你的維運或是開發團隊追蹤任何有關 CodeCommit Repository 的更新。本篇示例的 CloudFormation template 及 Lambda Function 同時也在 AWS 官方的 GitHub 上被公開 - &lt;a href=&quot;https://github.com/aws-samples/aws-codecommit-file-change-publisher&quot;&gt;aws-codecommit-file-change-publisher&lt;/a&gt;。你可以依據你的需求參考這個範例程式幫助你自定義 Email 的內容 (例如加上 HTML 樣式)，亦或者是新增其他有用的訊息至你的 Email 訊息中。&lt;/p&gt;

&lt;p&gt;若你對於這項實作感到興趣或是有其他建議，也歡迎在底下留言與我分享。當然，如果有任何關於開源專案的反饋，也歡迎開啟 GitHub issue 甚至是開啟 GitHub pull request 貢獻！&lt;/p&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="Lambda" /><category term="Lambda Function" /><category term="severless" /><category term="CodeCommit" /><category term="AWS CodeCommit" /><category term="SNS" /><category term="Amazon SNS" /><category term="Git" /><summary type="html">這篇內容主要是轉載我 2019 年 2 月公開發佈在 AWS 官方 DevOps 部落格的內容，在這篇內容中，主要展示了如何以 AWS Lambda 及 Amazon SNS 接收來自 AWS CodeCommit (Git Repository) 的檔案變更通知。由於屆時將滿 2 年 (時間過得真快)，覺得有必要翻譯成中文文件，以幫助中文的讀者也能夠了解這項內容。</summary></entry><entry><title type="html">Run application on EC2 and gather metric to Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)</title><link href="https://easoncao.com/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/" rel="alternate" type="text/html" title="Run application on EC2 and gather metric to Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)" /><published>2021-02-01T00:00:00+00:00</published><updated>2021-02-01T00:00:00+00:00</updated><id>https://easoncao.com/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp</id><content type="html" xml:base="https://easoncao.com/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/">&lt;h2 id=&quot;whats-amazon-managed-service-for-prometheus-amazon-prometheus--amp&quot;&gt;What’s Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale. With AMP, you can use the same open-source Prometheus data model and query language that you use today to monitor the performance of your containerized workloads, and also enjoy improved scalability, availability, and security without having to manage the underlying infrastructure. &lt;a href=&quot;https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There has a good article was dscribing the service feature on AWS Blog Post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/mt/getting-started-amazon-managed-service-for-prometheus/&quot;&gt;Getting Started with Amazon Managed Service for Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-if-i-would-like-to-gather-metric-without-having-kubernetesecs-cluster&quot;&gt;What if I would like to gather metric without having Kubernetes/ECS cluster?&lt;/h2&gt;

&lt;p&gt;The most of example was using EKS/ECS or Kubernetes Cluster as example. What if I would like to simply gather metrics for my application and got benefit without managing Prometheus so AWS would ensure the high availability? Generally, the idea is simple, however, it still takes me some times to do some research and get it a try. So it inspired me to write down the detail steps here.&lt;/p&gt;

&lt;p&gt;If you would like to , you should have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Application: Your application should follow the data model that supported by Prometheus (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;go_gc_duration_seconds_count 62&lt;/code&gt;) and expose your metrics with HTTP server, end with path &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;AWS SigV4 Proxy: By default, when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remote_write&lt;/code&gt; supported  The AWS SigV4 Proxy will sign incoming HTTP requests and forward them to the host specified in the Host header.&lt;/li&gt;
  &lt;li&gt;Standard Prometheus Server (or other agent to gather metrics): The Prometheus Server requires to be installed so it can from your application (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt;) and ship metric to the URL as specified in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remote_write&lt;/code&gt; section.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/ec2-with-amp-overview.png&quot; alt=&quot;Architecture overview&quot; /&gt;
  
    &lt;figcaption&gt;
      Architecture overview

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Monitoring Server (let’s say it is &lt;strong&gt;EC2 instance A&lt;/strong&gt;): This instance is running Grafana dashboard to see my metrics&lt;/li&gt;
  &lt;li&gt;Application Server (let’s say it is &lt;strong&gt;EC2 instance B&lt;/strong&gt;): The instance is running my application and standard Prometheus(gathering metrics)&lt;/li&gt;
  &lt;li&gt;A Workspaces in my Amazon Prometheus (AMP) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1&lt;/code&gt;: Used to receive metrics and provide consist readable endpoint so Grafana was able to query metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;configuration-steps&quot;&gt;Configuration steps&lt;/h2&gt;

&lt;p&gt;To demonstrate the entire working flow, I am going to use several docker images to quickly show how to do that. If you requires to run standalone application, you can use those Docker images, or build the binary by yourself by referencing the Dockerfile and documentations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/prom/prometheus/&quot;&gt;prom/prometheus&lt;/a&gt;: Standard Prometheus container image, binaries can be found on &lt;a href=&quot;https://prometheus.io/download/&quot;&gt;official documentation here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/grafana/grafana/&quot;&gt;grafana/grafana&lt;/a&gt;: Grafana dashboard&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sysdiglabs/custom-metrics-examples&quot;&gt;prometheus-golang&lt;/a&gt;: Sample application from sysdig for Prometheus metrics. (&lt;a href=&quot;https://sysdig.com/blog/prometheus-metrics/&quot;&gt;Source&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/aws-sigv4-proxy&quot;&gt;public.ecr.aws/aws-observability/aws-sigv4-proxy&lt;/a&gt;: AWS SigV4 Proxy to sign incoming HTTP requests and forward them&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-create-a-workspace-in-amazon-prometheus-amp&quot;&gt;1) Create a workspace in Amazon Prometheus (AMP)&lt;/h3&gt;

&lt;p&gt;If you got the preview access, the first step is to create a workspace in Amazon Prometheus:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/create-workspace-in-amp.png&quot; alt=&quot;Create a workspace in AMP&quot; /&gt;
  
    &lt;figcaption&gt;
      Create a workspace in AMP

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;So far it is simple and you can easily click one button to complete the creation. It usually takes few minutes as it was doing provisioning in the backend.&lt;/p&gt;

&lt;h3 id=&quot;2-set-up-iam-userrole-permission-for-your-grafana-dashboard&quot;&gt;2) Set up IAM user/role permission for your Grafana dashboard&lt;/h3&gt;

&lt;p&gt;Access to Amazon Managed Service for Prometheus actions and data requires credentials as AMP is using IAM for ensuing the data security. Therefore, it is required to have SigV4/IAM authentication when accessing the endpoint as it provides a consistent query endpoint for your AMP resource once you created a workspace. For example, the AMP resource in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1&lt;/code&gt; region can provide endpoint below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When adding the data source in my Grafana dashboard, it needs to follow the authentication model to access the data. To provide the access for my dashboard, in the example, I simply created a IAM user(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;amp-gra-user&lt;/code&gt;) with plain Access Key &amp;amp; Secret Key. To esnure it has permission to access AMP resource, I attached managed policy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AmazonPrometheusQueryAccess&lt;/code&gt; with following rules to provide readable access:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2012-10-17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Statement&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetLabels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetMetricMetadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetSeries&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:QueryMetrics&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/create-iam-user-for-grafana-dashboard.png&quot; alt=&quot;Create an IAM User for Grafana dashboard&quot; /&gt;
  
    &lt;figcaption&gt;
      Create an IAM User for Grafana dashboard

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;You still can use IAM role/attached EC2 IAM role as Grafana generally can use those permission if enabled loading AWS SDK config, as in my next step.&lt;/p&gt;

&lt;p&gt;As the managed policy might be changed once the service is generally available (GA), please refer to the Amazon Prometheus document - &lt;a href=&quot;https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-and-IAM.html&quot;&gt;IAM permissions and policies&lt;/a&gt; to get more detail.&lt;/p&gt;

&lt;h3 id=&quot;3-run-grafana-dashboard-on-my-monitoring-server-ec2-instance-a&quot;&gt;3) Run Grafana dashboard on my monitoring server (EC2 instance A)&lt;/h3&gt;

&lt;p&gt;This step I used Docker to run my Grafana dashboard. As Amazon Managed Service for Prometheus is integrated with AWS Identity and Access Management (IAM) to ensure that all calls to Prometheus APIs, such as query and ingest, are secured with IAM credentials.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;By default, the Prometheus data source in Grafana assumes that Prometheus requires no authentication. To enable Grafana to take advantage of AMP authentication and authorization capabilities, you will need to enable SigV4 authentication support in the Grafana data source. &lt;a href=&quot;https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-onboard-query-standalone-grafana.html#AMP-onboard-standalone-grafana-sigv4&quot;&gt;Reference&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To enable SigV4 on Grafana, I run my Grafana with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_SDK_LOAD_CONFIG&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GF_AUTH_SIGV4_AUTH_ENABLED&lt;/code&gt; environment variables set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;. The GF_AUTH_SIGV4_AUTH_ENABLED environment variable overrides the default configuration for Grafana to enable SigV4 support. &lt;a href=&quot;https://grafana.com/docs/grafana/latest/administration/configuration/#sigv4_auth_enabled&quot;&gt;sigv4_auth_enabled&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 3000:3000 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;grafana &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;GF_AUTH_SIGV4_AUTH_ENABLED=true&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AWS_SDK_LOAD_CONFIG=true&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    grafana/grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the Grafana is up and running, the next thing is that go to the AMP console then copy and paste the endpoint URL in your Grafana. You can find section &lt;strong&gt;Endpoint - remote write URL&lt;/strong&gt; &amp;amp; &lt;strong&gt;Endpoint - query URL&lt;/strong&gt; in the console:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/amp-endpoints.png&quot; alt=&quot;Endpoints in AMP console&quot; /&gt;
  
    &lt;figcaption&gt;
      Endpoints in AMP console

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The Grafana requires to use &lt;strong&gt;query URL&lt;/strong&gt; to read the metric, so go to the Grafana Dashboard and add a new Data Source. You can find the setting in the left navigation bar:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/grafana-configuration.png&quot; alt=&quot;Grafana Configurations&quot; /&gt;
  
    &lt;figcaption&gt;
      Grafana Configurations

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Select ‘Prometheus’ as new data source to add, fill the several information as below:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/grafana-add-data-source.png&quot; alt=&quot;Grafana add data source&quot; /&gt;
  
    &lt;figcaption&gt;
      Grafana add data source

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;HTTP &amp;gt; URL&lt;/strong&gt;: fill the endpoint URL you just copied (&lt;strong&gt;Endpoint - query URL&lt;/strong&gt;), please remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/query&lt;/code&gt; string that is appended to the URL, because the Prometheus data source will automatically append it. For example:
    &lt;ul&gt;
      &lt;li&gt;Endpoint (query URL): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-XXXXX-XXXX-XXX-XXXX-XXXXXX&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auth &amp;gt; SigV4 auth&lt;/strong&gt;: Enable the selection&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SigV4 Auth Details&lt;/strong&gt;: Select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Authentication Provider&lt;/code&gt; (Using Access &amp;amp; Secret key or else). In my testing I was filling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Access Key ID&lt;/code&gt; &amp;amp; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Secret Access Key&lt;/code&gt; with the credential of my IAM User (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;amp-gra-user&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then click &lt;strong&gt;Save &amp;amp; Test&lt;/strong&gt;, if everything works fine, you will see the message &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Data source is working&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/data-source-is-working.png&quot; alt=&quot;Data source is working&quot; /&gt;
  
    &lt;figcaption&gt;
      Data source is working

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;If not and shows other error messages, please refer to the following documentation to do troubleshooting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-onboard-query-standalone-grafana.html#AMP-onboard-standalone-grafana-troubleshoot&quot;&gt;Set up Grafana open source or Grafana Enterprise for use with AMP - Troubleshooting if Save &amp;amp; Test doesn’t work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this step we can ensure Grafana can query our metrics on AMP, but there has no data point yet, so the next step is to gather and write metrics to AMP.&lt;/p&gt;

&lt;h3 id=&quot;4-on-application-server-ec2-instance-b-with-private-ip-address-1723121133&quot;&gt;4) On application server (EC2 instance B, with private IP address: 172.31.21.133)&lt;/h3&gt;

&lt;p&gt;I attached an IAM Role to my EC2 instance B with following IAM policy (To write metric, at least to ensure you have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aps:RemoteWrite&lt;/code&gt; permission, as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AmazonPrometheusRemoteWriteAccess&lt;/code&gt;), this can allow AWS SigV4 proxy can use the permission and writes metrics to AMP:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2012-10-17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Statement&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:RemoteWrite&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:QueryMetrics&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetSeries&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetLabels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aps:GetMetricMetadata&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When your Prometheus server is ingesting metrics on your EC2 instance B, to secure the ingestion, it is requires to use SigV4 authentication when writing metrics to AMP. However, the Prometheus server generally only can use basic auth to do Authorization for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remote_write&lt;/code&gt;. Therefore, it is required to run a SigV4 proxy to provide access on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8005&lt;/code&gt; when forwarding remote write traffic.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8005:8005 public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; aps &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 &lt;span class=&quot;nt&quot;&gt;--host&lt;/span&gt; aps-workspaces.us-east-1.amazonaws.com &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; :8005
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the proxy will provide service on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8005&lt;/code&gt; and can be accessed via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8005&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.31.21.133:8005&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note: If you are not running resource in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1&lt;/code&gt; region, replace option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--region us-east-1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--host aps-workspaces.us-east-1.amazonaws.com&lt;/code&gt; as your own.&lt;/p&gt;

&lt;p&gt;Once the SigV4 proxy is up and running, I am going to run a sample Go applcation:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git clone https://github.com/sysdiglabs/custom-metrics-examples
$ docker build custom-metrics-examples/prometheus/golang -t prometheus-golang
$ docker run -d --rm --name prometheus-golang -p 80:8080 prometheus-golang
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Go application will expose the metric with path&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt;, so I can query them if using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl http://localhost/metrics | head

# HELP go_gc_duration_seconds A summary of the GC invocation durations.-     0
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&quot;0&quot;} 2.9336e-05
go_gc_duration_seconds{quantile=&quot;0.25&quot;} 3.1688e-05
go_gc_duration_seconds{quantile=&quot;0.5&quot;} 3.459e-05
go_gc_duration_seconds{quantile=&quot;0.75&quot;} 4.1515e-05
go_gc_duration_seconds{quantile=&quot;1&quot;} 8.7687e-05
go_gc_duration_seconds_sum 0.00251097
go_gc_duration_seconds_count 62
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;5-set-up-the-prometheus-server-to-collect-metrics&quot;&gt;5) Set up the Prometheus server to collect metrics&lt;/h3&gt;

&lt;p&gt;I created a configuration for Prometheus and scrape metric for Prometheus server itself (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:9090/metrics&lt;/code&gt;) and my Go application (Go application will expose metric through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172.31.21.133:80/metrics&lt;/code&gt;):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write&quot;&gt;remote_write&lt;/a&gt;: The section below can send metric to the remote endpoint. In this case, the url need to be specified through SigV4 proxy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[conf.yml]&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#global config&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;global&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;scrape_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;s&quot;&gt;15s&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;evaluation_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5s&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;scrape_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;     &lt;span class=&quot;s&quot;&gt;10s&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;external_labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;monitor'&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Scrape configs only contain one scrape target&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;scrape_configs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;job_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;prometheus'&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Override the global default and scrape targets from this job every 5 seconds.&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;scrape_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5s&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;static_configs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;localhost:9090'&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;172.31.21.133:80'&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;remote_write&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;http://172.31.21.133:8005/workspaces/ws-XXXXXX-XXXX-XXX-XXXX-XXXXXXX/api/v1/remote_write'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, start the Prometheus server to scrape metric:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker run \
     -p 9090:9090 \
     -v $PWD/conf.yml:/etc/prometheus/prometheus.yml \
     prom/prometheus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the Prometheus server is up and running, you would expect to see targets and know the health status:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/prometheus-targets.png&quot; alt=&quot;Prometheus targets&quot; /&gt;
  
    &lt;figcaption&gt;
      Prometheus targets

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;5-view-the-metric-&quot;&gt;5) View the metric !&lt;/h3&gt;

&lt;p&gt;Right now, if the Prometheus server can correctly write metric through AWS SigV4 Proxy, you would expect to view the metrics on Grafana dashboard:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/grafana-dashboard-view-amp-metric.png&quot; alt=&quot;View collected metrics on AMP in Grafana dashboard&quot; /&gt;
  
    &lt;figcaption&gt;
      View collected metrics on AMP in Grafana dashboard

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h2&gt;

&lt;h3 id=&quot;how-to-use-curl-to-query-my-amazon-prometheus-to-check-if-the-connectivity-or-iam-authentication-is-working-or-not&quot;&gt;How to use curl to query my Amazon Prometheus to check if the connectivity, or, IAM authentication is working or not?&lt;/h3&gt;

&lt;p&gt;To test the Sigv4, you can run the proxy and test the function with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; command. If you also would like to specify the IAM credential, choose either one of method to test the proxy and see if it can route the traffic for you:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Env vars&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-ti&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'AWS_ACCESS_KEY_ID=&amp;lt;YOUR ACCESS KEY ID&amp;gt;'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'AWS_SECRET_ACCESS_KEY=&amp;lt;YOUR SECRET ACCESS KEY&amp;gt;'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8005:8005 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; aps &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 &lt;span class=&quot;nt&quot;&gt;--host&lt;/span&gt; aps-workspaces.us-east-1.amazonaws.com &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; :8005

&lt;span class=&quot;c&quot;&gt;# Shared Credentials&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-ti&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; ~/.aws:/root/.aws &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8005:8005 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'AWS_PROFILE=&amp;lt;SOME PROFILE&amp;gt;'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; aps &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 &lt;span class=&quot;nt&quot;&gt;--host&lt;/span&gt; aps-workspaces.us-east-1.amazonaws.com &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; :8005
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws-sigv4-proxy&lt;/code&gt; is up and running, you can simply to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl&lt;/code&gt; to test the local endpoint:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An example with normal request but did not have query:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query &lt;span class=&quot;nt&quot;&gt;-vvv&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 127.0.0.1...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; TCP_NODELAY &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to localhost &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;127.0.0.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 8005 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query HTTP/1.1
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Host: localhost:8005
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; User-Agent: curl/7.53.1
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&amp;lt; HTTP/1.1 400 Bad Request
&amp;lt; Content-Type: application/json
&amp;lt; Date: Tue, 09 Feb 2021 15:02:56 GMT
&amp;lt; Server: amazon
&amp;lt; X-Amzn-Requestid: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
&amp;lt; Content-Length: 125
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host localhost left intact&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;status&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;error&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;errorType&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;bad_data&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;error&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;invalid parameter 'query': 1:1: parse error: no expression found in input&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;An example with normal request by feeding query according to &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/querying/api/&quot;&gt;HTTP API&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query?query=up -vvv

*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connected to localhost (127.0.0.1) port 8005 (#0)
&amp;gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query?query=up HTTP/1.1
&amp;gt; Host: localhost:8005
&amp;gt; User-Agent: curl/7.53.1
&amp;gt; Accept: */*
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Content-Type: application/json
&amp;lt; Date: Tue, 09 Feb 2021 15:10:49 GMT
&amp;lt; Server: amazon
&amp;lt; X-Amzn-Requestid: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
&amp;lt; Content-Length: 63
&amp;lt;
* Connection #0 to host localhost left intact
{&quot;status&quot;:&quot;success&quot;,&quot;data&quot;:{&quot;resultType&quot;:&quot;vector&quot;,&quot;result&quot;:[]}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;An example with failed response due to lack of IAM permission(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aps:QueryMetrics&lt;/code&gt;) for my IAM user/role:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query &lt;span class=&quot;nt&quot;&gt;-vvv&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 127.0.0.1...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; TCP_NODELAY &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to localhost &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;127.0.0.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 8005 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query HTTP/1.1
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Host: localhost:8005
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; User-Agent: curl/7.53.1
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&amp;lt; HTTP/1.1 403 Forbidden
&amp;lt; Content-Length: 200
&amp;lt; Content-Type: application/json
&amp;lt; Date: Tue, 09 Feb 2021 15:07:55 GMT
&amp;lt; Server: amazon
&amp;lt; X-Amzn-Errortype: AccessDeniedException
&amp;lt; X-Amzn-Requestid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
&amp;lt; X-Amzn-Requestid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host localhost left intact&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Message&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;User: arn:aws:iam::111111111111:user/myIAMUser is not authorized to perform: aps:QueryMetrics on resource: arn:aws:aps:us-east-1:111111111111:workspace/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, it shows a example to use Amazon Managed Service for Prometheus (AMP) to gather metrics when running standalone application on EC2 instance, rather than having Kubernetes to deploy. As it might be difficult to understand what things need to be noticed, so I shared configuration and steps. Giving the overview and share some tips you have to know if you are trying to push metrics to AMP.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fW2LOqHDUNM&quot;&gt;Demo Video - Amazon Managed Service for Prometheus (AMP) &amp;amp; Amazon Managed Service for Grafana (AMG)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/aws-sigv4-proxy&quot;&gt;AWS SigV4 Proxy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/configuration/configuration/&quot;&gt;Prometheus configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="amazon" /><category term="Prometheus" /><category term="AMP" /><category term="Amazon Managed Service for Prometheus" /><category term="EC2" /><category term="Elastic Compute Cloud" /><summary type="html">What’s Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)</summary></entry><entry><title type="html">CoreDNS(kube-dns) resolution truncation issue on Kubernetes (Amazon EKS)</title><link href="https://easoncao.com/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/" rel="alternate" type="text/html" title="CoreDNS(kube-dns) resolution truncation issue on Kubernetes (Amazon EKS)" /><published>2020-10-26T00:00:00+00:00</published><updated>2020-10-26T00:00:00+00:00</updated><id>https://easoncao.com/coredns-resolution-truncation-issue-on-kubernetes-kube-dns</id><content type="html" xml:base="https://easoncao.com/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/">&lt;p&gt;This article is describing the thing you need to aware for DNS resolution issue can occur on Kubernetes. Especially when your Pod is relying on CoreDNS(kube-dns) to resolve DNS record when connecting to Amazon ElastiCache or target with large response payload, the issue potentially can happen.&lt;/p&gt;

&lt;p&gt;Note: To help you understand the detail, I was using Amazon EKS with Kubernetes version 1.5 and CoreDNS (v1.6.6-eksbuild.1) as example.&lt;/p&gt;

&lt;h2 id=&quot;whats-the-problem&quot;&gt;What’s the problem&lt;/h2&gt;

&lt;p&gt;In Kubernetes, it provides an extra layer of the DNS resolution so containerized applications running on Kubernetes cluster basically heavily relying on using own DNS service to provide extra beneficial of service discovery by default, which is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-dns&lt;/code&gt; add-ons.&lt;/p&gt;

&lt;p&gt;And most common use cases are usually using CoreDNS as the default DNS resolution provider by deploying and running CoreDNS Pods within the cluster.&lt;/p&gt;

&lt;p&gt;Containerized applications running as Pod can use autogenerated service name that map to Kubernetes service’s IP(Cluster-IP), such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my-svc.default.cluster.local&lt;/code&gt;. It provides more flexibility to Pods to do internal service discovery as they can use the hostname to resolve the record without remembering the private IP address within the application when deploying to other environment. The kube-dns add-ons usually have ability to resolve external DNS record so Pods can also access service on Internet.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/kube-dns-overview.png&quot; alt=&quot;An overview of kube-dns running on Kubernetes&quot; /&gt;
  
    &lt;figcaption&gt;
      An overview of kube-dns running on Kubernetes - &lt;a href=&quot;https://cloud.google.com/kubernetes-engine/docs/concepts/service-discovery&quot;&gt;source / Service discovery and DNS (Google Cloud)&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;symptom&quot;&gt;Symptom&lt;/h3&gt;

&lt;p&gt;Basically, everything runs good for general use case and you propely won’t see any issue when running your production workload. But if your application need to resolve some DNS record with large DNS response payload(like sometimes your endpoint of ElastiCache response larger payload), you might notice the issue happen and your application never correctly resolve them.&lt;/p&gt;

&lt;p&gt;You can use some simple method by installing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig&lt;/code&gt; in your Pod to test and debug if you can see the difference from the result:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Get it a try to see if the external DNS provider&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Please make sure your Pod should have accessibility to reach out to Internet&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Otherwise you have to change 8.8.8.8 as your own private DNS server that Pod can reach out&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig example.com @8.8.8.8


&lt;span class=&quot;c&quot;&gt;# Get it a try to see if CoreDNS can correctly resolve the record&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig example.com @&amp;lt;YOUR_COREDNS_SERVICE_IP&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig example.com @&amp;lt;YOUR_COREDNS_PRIVATE_IP&amp;gt;

&lt;span class=&quot;c&quot;&gt;# Get it a try to see other record is having this issue or not&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig helloworld.com @&amp;lt;YOUR_COREDNS_PRIVATE_IP&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example if I have running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CoreDNS&lt;/code&gt; Pods and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-dns&lt;/code&gt; service in my Kubernetes cluster&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE     IP              NODE                                              NOMINATED NODE   READINESS GATES
kube-system   pod/coredns-9b6bd4456-97l97                   1/1     Running   0          5d3h    192.168.19.7    XXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   pod/coredns-9b6bd4456-btqpz                   1/1     Running   0          5d3h    192.168.1.115   XXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

NAMESPACE     NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;         AGE    SELECTOR
kube-system   service/kube-dns     ClusterIP   10.100.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP   5d3h   k8s-app&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-dns
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can use commands like below to check which point can cause failure:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dig example.com @10.100.0.10
dig example.com @192.168.19.7
dig example.com @192.168.1.115

dig helloworld.com @10.100.0.10
dig helloworld.com @192.168.19.7
dig helloworld.com @192.168.1.115
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;If you are getting result which is both are getting failed but other DNS record is working even using CoreDNS as name server. As a DNS resolver(CoreDNS), we can identify something went wrong when CoreDNS is trying to help you forward the DNS quries for some specific target, and this is the issue we would like to discuss&lt;/strong&gt;. However, If the Private IP of CoreDNS is working but Service IP (Cluster IP) is getting failure, or none of one are successful, you should pilot your investigation target on checking the Kubernetes networking encapsulation, such as: CNI plugin, kube-proxy, cloud provider’s setting or else that can break your Pod-Pod communication or host networking translation.&lt;/p&gt;

&lt;h3 id=&quot;how-to-reproduce&quot;&gt;How to reproduce&lt;/h3&gt;

&lt;p&gt;In my testing, I was running Kubernetes cluster with Amazon EKS (1.5) with default deployments (such as CoreDNS, AWS CNI Plugin and kube-proxy). The issue can be reproduced when following steps with commands below:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create svc externalname quote-redis-cluster &lt;span class=&quot;nt&quot;&gt;--external-name&lt;/span&gt; &amp;lt;MY_DOMAIN&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create deployment nginx &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nginx
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &amp;lt;nginx&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;dnsutils &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; nslookup quote-redis-cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My deployment with one nginx Pod and two CoreDNS Pods already running within my EKS cluster:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE    IP               NODE                                                NOMINATED NODE   READINESS GATES
default       pod/nginx-554b9c67f9-w9bb4    1/1     Running   0          40s    192.168.70.172   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
...
kube-system   pod/coredns-9b6bd4456-6q9b5   1/1     Running   0          3d3h   192.168.39.186   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   pod/coredns-9b6bd4456-8qgs8   1/1     Running   0          3d3h   192.168.69.216   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And here is the problem: when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nslookup&lt;/code&gt; to resolve the domain (set the DNS resolver as CoreDNS Pod &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.39.186&lt;/code&gt; instead of using Cluster IP of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-dns&lt;/code&gt;, this can exclude any issue can caused by Kubernetes networking based on iptables).&lt;/p&gt;

&lt;p&gt;The test result only return the canonical name (CNAME), without IP address (example.com):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; nginx-554b9c67f9-w9bb4 bash
root@nginx-554b9c67f9-w9bb4:/# nslookup quote-redis-cluster 192.168.39.186
Server:         192.168.39.186
Address:        192.168.39.186#53

quote-redis-cluster.default.svc.cluster.local   canonical name &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; example.com.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: The domain name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt; basically has normal A record with many IP address. (The real case is that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt; was an Amazon ElastiCache endpoint, with returning IP addresses of ElastiCache nodes)&lt;/p&gt;

&lt;p&gt;However, when testing other domain, basically can successfully return the IP address even using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nslookup&lt;/code&gt;, in the same container, with same DNS resolver (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.39.186&lt;/code&gt;):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create svc externalname quote-redis-cluster &lt;span class=&quot;nt&quot;&gt;--external-name&lt;/span&gt; success-domain.com

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; nginx-554b9c67f9-w9bb4 bash
root@nginx-554b9c67f9-w9bb4:/# nslookup my-endpoint 192.168.39.186
Server:         192.168.39.186
Address:        192.168.39.186#53

my-endpoint.default.svc.cluster.local     canonical name &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; success-domain.com.
Name:   success-domain.com.
Address: 11.11.11.11
Name:   success-domain.com.
Address: 22.22.22.22
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And this is the issue I would like to talk about. Let’s break down and understand why having difference in the result.&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-into-the-root-cause&quot;&gt;Deep dive into the root cause&lt;/h2&gt;

&lt;p&gt;Let’s start to break down what happen inside. To better help you understand what’s going on, it is required to know:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.70.172&lt;/code&gt; (Client - nslookup): The nginx Pod, in this Pod I was running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nslookup&lt;/code&gt; to test the DNS resolution ability.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.39.186&lt;/code&gt; (CoreDNS): The real private IP of CoreDNS Pod, play as DNS resolver for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-dns&lt;/code&gt; service.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.69.216&lt;/code&gt; (CoreDNS): The real private IP of CoreDNS Pod, play as DNS resolver for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-dns&lt;/code&gt; service.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;192.168.0.2&lt;/code&gt; (AmazonProvidedDNS): The default DNS resolver in VPC and can be used by EC2 instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On EKS, the DNS resolution flow can be looked like:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/eks-dns-resolution-flow.png&quot; alt=&quot;The DNS resolution flow on EKS&quot; /&gt;
  
    &lt;figcaption&gt;
      The DNS resolution flow on EKS

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;First, the Pod will issue a DNS query to CoreDNS. Once the CoreDNS receives the query, it will check if have any cache DNS record exists, otherwise, it would follow the setting mentioning in the configuration to forward the reqeust to upstream DNS resolver.&lt;/p&gt;

&lt;p&gt;The following is that CoreDNS will look up the DNS resolver according to the configuration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/resolv.conf&lt;/code&gt; of CoreDNS node(In my environment it was using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AmazonProvidedDNS&lt;/code&gt;):&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;Corefile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;.:53 {&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;forward . /etc/resolv.conf&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on the model, responses will follow the flow and send back to the client. The normal case(happy case) is that we always can query DNS &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; records and can correctly have addresses in every response:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/coredns-issue-normal.png&quot; alt=&quot;Normal DNS resolution packet&quot; /&gt;
  
    &lt;figcaption&gt;
      Normal DNS resolution packet, can see A record responsed(Using nslookup)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Right now, let’s move on taking a look what’s going on regarding the issue:&lt;/p&gt;

&lt;p&gt;1) On the client side, I was collecting packet and only can see the response with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CNAME&lt;/code&gt; record, as we expected on nslookup output:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/coredns-issue-failed-in-client-nslookup.png&quot; alt=&quot;The network packet collected from client&quot; /&gt;
  
    &lt;figcaption&gt;
      The network packet collected from client - only get the response with CNAME even asking for &lt;strong&gt;A record&lt;/strong&gt;.

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;2) On the CoreDNS node, I also collected the packet and can see:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The CoreDNS Pod was asking the record with AmazonProvidedDNS&lt;/li&gt;
  &lt;li&gt;In the response, it only have CNAME record&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;192.168.70.172 -&amp;gt; 192.168.39.186 (Client query A quote-redis-cluster.default.svc.cluster.local to CoreDNS)
192.168.39.186 -&amp;gt; 192.168.0.2    (CoreDNS query A quote-redis-cluster.default.svc.cluster.local to AmazonProvidedDNS)
192.168.0.2    -&amp;gt; 192.168.39.186 (AmazonProvidedDNS response the record, with CNAME)
192.168.39.186 -&amp;gt; 192.168.70.172 (CoreDNS Pod response the CNAME record)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/coredns-issue-failed-on-coredns-node.png&quot; alt=&quot;The network packet collected from client&quot; /&gt;
  
    &lt;figcaption&gt;
      The network packet collected from client - only get the response with CNAME even asking for &lt;strong&gt;A record&lt;/strong&gt;.

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;And if look the packet closer collected on CoreDNS node, here is they key point: &lt;strong&gt;The truncated flag (TC flag) was true, which means the DNS response is truncated.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/coredns-issue-failed-truncated-flag.png&quot; alt=&quot;The message is truncated in the DNS query response&quot; /&gt;
  
    &lt;figcaption&gt;
      The message is truncated in the DNS query response (CoreDNS Node)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;At this stage, the issue can be identified the DNS response was truncated.&lt;/p&gt;

&lt;h2 id=&quot;why-the-dns-response-message-was-truncated&quot;&gt;Why the DNS response (message) was truncated?&lt;/h2&gt;

&lt;p&gt;When you see the message truncation, you probably will say: “Holy … sh*t, DNS response is missing, upstream DNS resolver must eat the payload and did not working properly! “. However, the truth is, this is expected behavior according to the design of DNS when it initially comes to first 19s.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests. Queries generally consist of a single UDP request from the client followed by a single UDP reply from the server. — &lt;a href=&quot;https://en.wikipedia.org/wiki/Domain_Name_System&quot;&gt;wikipeida&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The basic payload can be allowed in DNS query generally won’t 512 bytes. It basically can perfectly work in first 19s, however, lately 20s, with the scale of Internet was growing, people start to aware the original design unable to fully satisfy the requirement and they would like to include more information in DNS query(like the usage of IPv4). According to &lt;a href=&quot;https://tools.ietf.org/html/rfc1123&quot;&gt;RFC#1123&lt;/a&gt;, it initiated the standard in case if a single DNS payload contain over 512 bytes limit for UDP:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is also clear that some new DNS record types defined in the future will contain information exceeding the 512 byte limit that applies to UDP, and hence will require TCP. Thus, resolvers and name servers should implement TCP services as a backup to UDP today, with the knowledge that they will require the TCP service in the future.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Therefore, in &lt;a href=&quot;https://tools.ietf.org/html/rfc5596&quot;&gt;RFC#5596&lt;/a&gt; basically mentioning the behavior of “DNS Transport over TCP”. Solutions can be: use EDNS or retransmit the DNS query over TCP if truncation flag has been set (TC Flag):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the absence of EDNS0 (Extension Mechanisms for DNS 0), the normal behaviour of any DNS server needing to send a UDP response that would exceed the 512-byte limit is for the server to truncate the response so that it fits within that limit and then set the TC flag in the response header.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;When the client receives such a response, it takes the TC flag as an indication that it should retry over TCP instead.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Therefore, when the length of the answer exceeds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512 bytes&lt;/code&gt;(The upstream server should set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TC flag&lt;/code&gt; to tell the downstream the message is truncated), when both client and server support EDNS, larger UDP packets are used including in additional UDP packet. Otherwise, the query is sent again using the Transmission Control Protocol (TCP). TCP is also used for tasks such as zone transfers. Some resolver implementations use TCP for all queries.&lt;/p&gt;

&lt;h2 id=&quot;how-to-remedy-the-issue&quot;&gt;How to remedy the issue?&lt;/h2&gt;

&lt;p&gt;The symptom generally happen when Pods was trying to resolve the DNS record in UDP through CoreDNS. If the domain name contains payload exceeds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512 bytes&lt;/code&gt;, it can hit the default limit of UDP DNS query. When the payload over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512 bytes&lt;/code&gt;, it is expected to get the response with truncation has been set (TC flag).&lt;/p&gt;

&lt;p&gt;However, when receiving payload with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TC flag&lt;/code&gt; set up and can think of the response is truncated, by default(on Amazon EKS), CoreDNS doesn’t apply the retransmit behavior by using TCP, instead, CoreDNS only response the truncated message and sent it back to the client. This workflow cause the client can aware the DNS query message was missing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Therefore, to remedy the issue, here are possible solutions can be adopted:&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;solution-1-using-edns0&quot;&gt;Solution 1: Using EDNS0&lt;/h3&gt;

&lt;p&gt;In common use case, the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512 bytes&lt;/code&gt; generally can satisfy most usage because we will not expect that our DNS shouldn’t contain too much IP addresses information. However, in some cases, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To balance my workload, I have the resolution will response many targets, because I am using DNS to do round-robin.&lt;/li&gt;
  &lt;li&gt;It is an endpoint of ElastiCache with many nodes.&lt;/li&gt;
  &lt;li&gt;Other use case can include long message in DNS response.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you can expect the DNS response will have larger payload, basically, client still able to send additional EDNS information to increase the buffer size in single UDP response. This still able to ask CoreDNS to forward the additional section using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EDNS0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The workflow can be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) Resolve the domain by querying &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CNAME&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;2) Once get the mapping domain name, can send another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; record query with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EDNS&lt;/code&gt; option&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(You can implement the logic in your own application. Because it has many different way to enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EDNS&lt;/code&gt; option, please refer to the documentation provided by your programming language or relevant library.)&lt;/p&gt;

&lt;p&gt;Here is an example regarding the output can see when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; CNAME my-svc.default.cluster.local
... &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Getting canonical name of the record, e.g. example.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dig example.com
&lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt; Got answer:
&lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt; -&amp;gt;&amp;gt;HEADER&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;opcode&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;: QUERY, status: NOERROR, id: 51739
;; flags: qr rd ra; QUERY: 1, ANSWER: 22, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 7fb5776972bf2aa4 (echoed)
;; QUESTION SECTION:
; example.com. IN A

;; ANSWER SECTION:
example.com. 15 IN A 10.4.85.47
example.com. 15 IN A 10.4.83.252
example.com. 15 IN A 10.4.82.121
...
example.com. 15 IN A 10.4.82.186
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the output, we can see that, by default, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig&lt;/code&gt; will add optional setting to increase allowed payload size in UDP query, in this case it was using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4096 bytes&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is a sample packet using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dig&lt;/code&gt; with EDNS:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/dns-with-edns-query.png&quot; alt=&quot;Request with EDNS0 (Query)&quot; /&gt;
  
    &lt;figcaption&gt;
      Request with EDNS0 (Query) - Can see the payload size has been set up as &lt;strong&gt;4096 bytes&lt;/strong&gt; in additional section

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/dns-with-edns-response.png&quot; alt=&quot;Request with EDNS0 (Response)&quot; /&gt;
  
    &lt;figcaption&gt;
      Request with EDNS0 (Response) - Can see the total DNS response size is up to &lt;strong&gt;3296 bytes&lt;/strong&gt;, which exceeds default 512 bytes limit

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;As the method generally need to implement in the application. If you would like to increase the default buffer size for every UDP quries sent by CoreDNS, you can use &lt;a href=&quot;https://coredns.io/plugins/bufsize/&quot;&gt;bufsize plugin&lt;/a&gt; to limits a requester’s UDP payload size.&lt;/p&gt;

&lt;p&gt;Here is an example to enable limiting the buffer size of outgoing query to the resolver (172.31.0.10):&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;. {&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;bufsize &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;512&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;forward . 172.31.0.10&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, it would ask CoreDNS to allow larger payload size in every DNS queries, it may cause DNS vulnerabilities potentially with performance degraded. Please refer to the CoreDNS documentation to get more detail.&lt;/p&gt;

&lt;h3 id=&quot;solution-2-having-tcp-retransmit-when-the-dns-query-was-truncated-recommended&quot;&gt;Solution 2: Having TCP retransmit when the DNS query was truncated (Recommended)&lt;/h3&gt;

&lt;p&gt;As mentioned, &lt;strong&gt;the normal behaviour of any DNS server needing to send a UDP response that would exceed the 512-byte limit is for the server to truncate the response so that it fits within that limit and then set the TC flag in the response header. When the client receives such a response, it takes the TC flag as an indication that it should retry over TCP instead.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So far, by analyzing the network flow with packets, we can certainly sure CoreDNS doesn’t help us to perform the retransmit over TCP if getting message truncation. So the question is, &lt;strong&gt;can we ask the CoreDNS should retry with TCP if getting message is truncated?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;And the answer is … &lt;strong&gt;YES!&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.6.2&lt;/code&gt;, CoreDNS should support syntax &lt;strong&gt;&lt;a href=&quot;https://coredns.io/plugins/forward/&quot;&gt;prefer_udp&lt;/a&gt;&lt;/strong&gt; to handle the truncated responses when forwarding DNS queries:&lt;/p&gt;

&lt;p&gt;The explanation of the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefer_udp&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Try first using UDP even when the request comes in over TCP. If response is truncated (TC flag set in response) then do another attempt over TCP. In case if both force_tcp and prefer_udp options specified the force_tcp takes precedence.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here is an example to add the option for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; plugin:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;forward . /etc/resolv.conf {&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;prefer_udp&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This option generally can be added in your configuration if you can aware a truncated response is received but CoreDNS doesn’t handle it. However, it is still recommended to get it a try and see any performance issue can cause. Consider scale out your CoreDNS Pods in your environment would be helpful if having large scale cluster.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, it dive deep into an DNS resolution issue regarding CoreDNS when running workload on Amazon EKS and break down the detail by inspecting network packet. The root cause relates to the message was truncated due to the response payload size exceeds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512 bytes&lt;/code&gt;, which result in the client (Pod) was unable to get the correct result with IP addresses. The response payload will return detail with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TC flag&lt;/code&gt; set up.&lt;/p&gt;

&lt;p&gt;This issue generally related to the design of DNS in UDP and it is expected to get the message truncation if having larger payload. In addition, as mentioned in &lt;a href=&quot;https://tools.ietf.org/html/rfc1123&quot;&gt;RFC#1123&lt;/a&gt;, the requester should have responsibility for handling the situation if aware that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TC flag&lt;/code&gt; has been set up. This article was mentioning the detail and the flow according to packet analyzing.&lt;/p&gt;

&lt;p&gt;To remedy the problem, this article also mentioned several methods can be applied on client side or CoreDNS, by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EDNS&lt;/code&gt; or apply TCP retransmit, both are able to be applied in CoreDNS.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/coredns/coredns/pull/3110&quot;&gt;CoreDNS Pull Request#3110&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="amazon" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="EDNS" /><category term="DNS" /><category term="CoreDNS" /><category term="kube-dns" /><category term="RFC" /><summary type="html">This article is describing the thing you need to aware for DNS resolution issue can occur on Kubernetes. Especially when your Pod is relying on CoreDNS(kube-dns) to resolve DNS record when connecting to Amazon ElastiCache or target with large response payload, the issue potentially can happen.</summary></entry><entry><title type="html">[AWS][EKS] Zero downtime deployment(RollingUpdate) when using ALB Ingress Controller on Amazon EKS</title><link href="https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/" rel="alternate" type="text/html" title="[AWS][EKS] Zero downtime deployment(RollingUpdate) when using ALB Ingress Controller on Amazon EKS" /><published>2020-10-20T00:00:00+00:00</published><updated>2020-10-20T00:00:00+00:00</updated><id>https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error</id><content type="html" xml:base="https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/">&lt;p&gt;This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment and prevent 502 errors.&lt;/p&gt;

&lt;h2 id=&quot;whats-alb-ingress-controller&quot;&gt;What’s ALB Ingress Controller&lt;/h2&gt;

&lt;p&gt;Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;type=LoadBalancer&lt;/code&gt;. Therefore, if you would like to expose your container service with Application Load Balancer (ALB) on EKS, it is recommended to integrate with ALB Ingress Controller.&lt;/p&gt;

&lt;p&gt;If you don’t know about what is the ALB Ingress Controller, here is an overview diagram to help you catch up:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/controller-design.png&quot; alt=&quot;How ALB ingress controller works&quot; /&gt;
  
    &lt;figcaption&gt;
      How ALB ingress controller works - &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/controller/how-it-works/&quot;&gt;source&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;(1) The controller watches for ingress events from the API server.&lt;/li&gt;
  &lt;li&gt;(2) An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal.&lt;/li&gt;
  &lt;li&gt;(3) Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.&lt;/li&gt;
  &lt;li&gt;(4) Listeners are created for every port detailed in your ingress resource annotations.&lt;/li&gt;
  &lt;li&gt;(5) Rules(ELB Listener Rules) are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: The new version of AWS ALB Ingress Controller is upcoming, while rename it to be “AWS Load Balancer Controller” with several new features coming out. For more detail, please refer the &lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&quot;&gt;GitHub project - kubernetes-sigs/aws-alb-ingress-controller&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;how-to-deploy-kubernetes-with-alb-ingress-controller-aws-load-balancer-controller&quot;&gt;How to deploy Kubernetes with ALB Ingress Controller (AWS Load Balancer Controller)?&lt;/h2&gt;

&lt;p&gt;Basically, the ALB Ingress Controller will be deployed as a Pod running on your worker node while continously monitor/watch your cluster state. Once there have any request for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ingress&lt;/code&gt;  object creation, ALB Ingress Controller will help you to manage and create Application Load Balancer resource. Here is a part of example for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v1.1.8&lt;/code&gt; deployment manifest:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;app.kubernetes.io/name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;app.kubernetes.io/name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;app.kubernetes.io/name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Setting the ingress-class flag below ensures that only ingress resources with the&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# annotation kubernetes.io/ingress.class: &quot;alb&quot; are respected by the controller. You may&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# choose any class you'd like for this controller to respect.&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--ingress-class=alb&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# REQUIRED&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Name of your cluster. Used when naming resources created&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# by the ALB Ingress Controller, providing distinction between&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# clusters.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# - --cluster-name=devCluster&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# AWS VPC ID this ingress controller will use to create AWS resources.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# If unspecified, it will be discovered from ec2metadata.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# - --aws-vpc-id=vpc-xxxxxx&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# AWS region this ingress controller will operate in.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# If unspecified, it will be discovered from ec2metadata.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# - --aws-region=us-west-1&lt;/span&gt;
          &lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker.io/amazon/aws-alb-ingress-controller:v1.1.8&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb-ingress-controller&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The deployment basically will run a copy of ALB Ingress Controller (pod/alb-ingress-controller-xxxxxxxx-xxxxx) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kube-system&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   pod/alb-ingress-controller-5fd8d5d894-8kf7z   1/1     Running   0          28s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/alb-ingress-controller   1/1     1            1           3m48s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Depends on your environment, suggested installation steps may involve the configuration of IRSA (IAM Role for Service Account) to grant permission for the ALB Ingress Controller Pods in order to interact with AWS resources, so it is recommended to take a look official documentation to help you quickly understand how to install ALB Ingress Controller:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html&quot;&gt;ALB Ingress Controller on Amazon EKS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, the service can be deployed as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ingress&lt;/code&gt; Object. For example, if you tried to deploy the simple 2048 application:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2048-ingress.yaml&lt;/code&gt; is mentioning the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;annotations&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spec&lt;/code&gt; in format that supported by ALB Ingress Controller can recognize:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-ingress&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-game&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;alb.ingress.kubernetes.io/scheme&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;internet-facing&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2048-ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/*&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;service-2048&quot;&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ingress object will construct ELB Listeners according rules and forward the connection to the corresponding backend(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serviceName&lt;/code&gt;), which match the group of service &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service-2048&lt;/code&gt;, any traffic match the rule &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/*&lt;/code&gt; will be routed to the group of selected Pods. In this case, Pods are exposed on the worker node based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;type=NodePort&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;Content of the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2048-service.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;service-2048&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-game&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NodePort&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;so--whats-the-problem&quot;&gt;So … what’s the problem?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Zero downtime deployment&lt;/strong&gt; is always a big challenge for DevOps/Operation team for any kind of business. When you adopt the ALB Ingress Controller as a solution to expose your service, it has couple things need to take care due to the behavior of Kubernetes, ALB and ALB Ingress Controller … you need to consider many perspectives. Especially, it can have some issue when you would like to roll out the new deployment for your Pods with ALB Ingress Controller.&lt;/p&gt;

&lt;p&gt;Let’s use the 2048 game as example to describe the scenario when you are trying to roll out a new version of your container application. In my environment, I have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A Kubernetes service &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service/service-2048&lt;/code&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodePort&lt;/code&gt; to expose the service&lt;/li&gt;
  &lt;li&gt;The deployment also have 5 copy of Pods for 2048 game, which is my backend application waiting for connections forwarding by Application Load Balancer (ALB)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
2048-game     pod/2048-deployment-58fb66554b-2f748          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-4hz5q          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-jdfps          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-rlpqm          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-s492n          1/1     Running   0          53s

NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;         AGE
2048-game     service/service-2048   NodePort    10.100.53.119   &amp;lt;none&amp;gt;        80:30337/TCP    52s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
2048-game     deployment.apps/2048-deployment          5/5     5            5           53s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And for sure, once the ALB Ingress Controller correctly set up and provision the ELB resource, the full domain of ELB also will be recorded to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ingress&lt;/code&gt; object:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kubectl get ingress -n 2048-game
NAME           HOSTS   ADDRESS                                                                      PORTS   AGE
2048-ingress   *       xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com   80      11m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can use the DNS name as endpoint to visit my container service:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com | &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt;
&amp;lt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;&lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &amp;lt;meta &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &amp;lt;title&amp;gt;2048&amp;lt;/title&amp;gt;

  &amp;lt;&lt;span class=&quot;nb&quot;&gt;link &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;style/main.css&quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;rel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stylesheet&quot;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/css&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &amp;lt;&lt;span class=&quot;nb&quot;&gt;link &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;shortcut icon&quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;favicon.ico&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-ingress-controller-with-2048-game-screenshot.png&quot; alt=&quot;2048 Game deployed with ALB Ingress Controller&quot; /&gt;
  
    &lt;figcaption&gt;
      2048 Game deployed with ALB Ingress Controller

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Like any kind of container application, as a administrator/SRE (Site Reliability Engineer)/part of operation team or DevOps engineer, the goal and our duty is: &lt;strong&gt;we always try to ensure the service can run properly without any issue and without any interruption&lt;/strong&gt;, especially facing the challenges like: when your developers are saying that &lt;strong&gt;“Oh! we need to upgrade/update the application”&lt;/strong&gt;, &lt;strong&gt;“we are going to roll out a bug fix”&lt;/strong&gt;, &lt;strong&gt;“the new feature is going to online”&lt;/strong&gt;, any service downtime can lead anyone of stakeholders(users, operation team or leadership) unhappy.&lt;/p&gt;

&lt;p&gt;I am going to use a simple loop trick to continously access my container service via the endpoint &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com&lt;/code&gt; to demonstrate a scenario: This is a popular web service and we always have customer access to it. (Like social media service, bitcoin trading platform or any else, we basically have zero tolerance for any service downtime as it can impact our revenue.), as below:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; ./request-my-service.sh&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;0.1&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010038
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012131
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005366
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010119
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012066
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005451
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010006
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012084
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005598
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010086
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012162
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005278
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010326
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012193
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005347
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Meanwhile, I am using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RollingUpdate&lt;/code&gt; strategy in my Kubernetes deployment strategy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxUnavailable=25%&lt;/code&gt;, which means, when Kubernetes need to update or patch(Like update the image or environment variables), the maximum number of unavailable Pods cannot exceed over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;25%&lt;/code&gt; as well as it ensures that at least 75% of the desired number of Pods are up (only replace 1-2 Pods if I have 5 copies at the same time):&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2048-deployment&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2048-game&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;rollingUpdate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxSurge&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;25%&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxUnavailable&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;25%&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RollingUpdate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;scenario-rolling-the-new-container-image-to-existing-container-application-with-potential-service-downtime&quot;&gt;Scenario: Rolling the new container image to existing container application with potential service downtime&lt;/h3&gt;

&lt;p&gt;When rolling the new version of my container application (for example, I update my deployment by replacing the container image with image &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt;), there potentially can have a period of time that would get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HTTP Status Code 502&lt;/code&gt; error in few hits:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/http-502-error-during-deployment-using-instance-mode.png&quot; alt=&quot;The HTTP 502 Error response from ELB (instance mode)&quot; /&gt;
  
    &lt;figcaption&gt;
      The HTTP 502 Error response from ELB during the rolling update deployment (instance mode)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;By default, ALB Ingress Controller is using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instance&lt;/code&gt; mode to register targets(Pods) to the ELB Target Group by using worker nodes’ instance ID with exposing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NodePort&lt;/code&gt;. In this case, the traffic will the Kubernetes networking design to do second tier of transportation according to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;externalTrafficPolicy&lt;/code&gt; defined in Kubernetes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Service&lt;/code&gt; object (No matter using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;externalTrafficPolicy=Cluster&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;externalTrafficPolicy=Local&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Due to the ALB Ingress Controller only care about to register Worker Node to the target group, so if the scenario doesn’t involve the worker node replacement, the case basically have miniumun even no downtime(expect that it is rare to have downtime if the Kubernetes can perfectly handle the traffic forwarding), however, this is not how real world operate, few seconds downtime still can happen potentially due to the workflow below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This is the general workflow when the client reach out to the service endpoint (ELB) and how was traffic goes&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client ----&amp;gt; ELB ----&amp;gt; Worker Node (iptables) / In this step it might be forwarded to other Worker Node ----&amp;gt; Pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, in these cases, you can see the downtime:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) The client established the connection with ELB, ELB is trying to but the Worker Node is not ready.&lt;/li&gt;
  &lt;li&gt;(2) Follow the iptables rules, the traffic forward to the Pod just terminated due to RollingUpdate (Or the Pod just got the reqeust but need to be terminated, it haven’t response back yet, caused the ELB doesn’t get the response from Pod.)&lt;/li&gt;
  &lt;li&gt;(3) ELB established connection with Worker Node-1, once the packet enter into the Worker Node-1, it follows the iptables then forward it to the Pod running on Worker Node-2 (jump out the current worker node), however, the Worker Node-2 just got terminated due to auto scaling strategy or any replacement due to upgrade, caused the connection lost.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s say if you try to remove the encapsulation layer of the Kubernetes networking design and make thing more easier based on the AWS supported CNI Plugin (Only rely on the ELB to forward the traffic to the Pod directly by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IP mode&lt;/code&gt; with annotation setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alb.ingress.kubernetes.io/target-type: ip&lt;/code&gt; in my Ingress object), you can see the downtime more obvious when Pod doing RollingUpdate. That’s because not only the problem we mentioned the issues in case (1)/(2)/(3), but also there has different topic on the behavior of ALB Ingress Controller need to be covered if the question comes to &lt;strong&gt;zero downtime deployment&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Here is an example by using IP mode (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alb.ingress.kubernetes.io/target-type: ip&lt;/code&gt;) as &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/#target-type&quot;&gt;resgistration type&lt;/a&gt; to route traffic directly to the Pod IP&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-ingress&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-game&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;alb.ingress.kubernetes.io/scheme&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;internet-facing&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;alb.ingress.kubernetes.io/target-type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2048-ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/*&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;service-2048&quot;&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-target-group-with-pod-in-ip-mode.png&quot; alt=&quot;An example when using IP mode in ALB Ingress Controller&quot; /&gt;
  
    &lt;figcaption&gt;
      An example when using IP mode in ALB Ingress Controller - Can see my Pods all are registering with Pod owns IP address

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Again follow the issue we mentioned (1) (2) (3), when doing the rolling update (I was replacing the image again in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IP mode&lt;/code&gt;), similar problem can be observed, potentially you can have 10-15 seconds even longer downtime can show up:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/http-502-error-during-deployment-using-ip-mode.png&quot; alt=&quot;The HTTP 502 Error response from ELB (IP mode)&quot; /&gt;
  
    &lt;figcaption&gt;
      The HTTP 502 Error response from ELB during the rolling update deployment (IP mode)

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;When Kubernetes is rolling the deployment, in the target group, you can see ALB Ingress Controller was issuing old targets draining process(Old Pods) in the meantime&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/old-target-doing-draining-in-target-group-with-ip-mode.png&quot; alt=&quot;Old targets were going to be draining state in target group&quot; /&gt;
  
    &lt;figcaption&gt;
      Old targets were going to be draining state in target group

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;However, you still can see HTTP 502/504 errors exceed 3-10 seconds for single requset&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005413
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.009980
502 Bad Gateway
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;502_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.076954
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005700
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010019
502 Bad Gateway
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;502_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.081601
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005527
502 Bad Gateway
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;502_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.070947
502 Bad Gateway
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;502_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.187812
504 Gateway Time-out
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;504_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.006324
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.011838
Welcome to nginx!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-issue-and-the-workflow-of-alb-ingress-controller&quot;&gt;The issue and the workflow of ALB Ingress Controller&lt;/h3&gt;

&lt;p&gt;Let’s use this scenario as it is a edge problem we need to consider for most use case, the issue is that the workflow between the Kubernetes, ALB Ingress Controller and ELB can lead HTTP status code 502/503(5xx) erros during deployment when having Pod termination.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In short, when a pod is being replaced, ALB Ingress Controller registers the new pod in the target group and removes the old Pods.&lt;/strong&gt; However, at the same time:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For the the new Pods, the target is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial&lt;/code&gt; state, until it pass the defined health check threshold (ALB health check)&lt;/li&gt;
  &lt;li&gt;For the old Pods is remaining as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;draining&lt;/code&gt; state, until it completes draining action for the in-flight connection, or reaching out the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Deregistration delay&lt;/code&gt; defined in the target group.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Which result in the service to be unavailable and return HTTP 502.&lt;/p&gt;

&lt;p&gt;To better understand that, I made following diagrams, so it might be helpful to you understanding the workflow:&lt;/p&gt;

&lt;p&gt;1) In diagram, I used following IP addresses to remark and help you recognize new/old Pods. Here is the initial deployment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Old Pods: Target-1(Private IP: 10.1.1.1), Target-2(Private IP: 10.2.2.2)&lt;/li&gt;
  &lt;li&gt;New Pods: Target-3(Private IP: 10.3.3.3), Target-4(Private IP: 10.4.4.4)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-1-initial.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 1. the initial deployment&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 1. the initial deployment

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;2) At this stage, I was doing container image update and start rolling out the new copies of Pods. The ALB Ingress Controller made RegisterTarget API call to ELB on behalf of the Kubernetes.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-2-register-target.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 2. start rolling out the new copies of Pods and ALB Ingress Controller is going to issue RegisterTarget API call&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 2. start rolling out the new copies of Pods and ALB Ingress Controller is going to issue RegisterTarget API call

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;3) Meanwhile, the DeregisterTarget API will be called by ALB Ingress Controller and new targets are in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial&lt;/code&gt; state.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-3-start-dereigster-old-target.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 3. ALB Ingress Controller start to dereigster old targets on ELB Target Group&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 3. ALB Ingress Controller start to dereigster old targets on ELB Target Group

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;4) At this stage, anything could happen to cause service outage. Because the DeregisterTarget API call might take some time to process, but, Kubernetes doesn’t have any design to monitor the current state of the ELB Target Group, it only care about rolling the new version of Pods and terminate old one.&lt;/p&gt;

&lt;p&gt;In this case, if the Pod got terminated by Kubernetes but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-1&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-2&lt;/code&gt; are still leaving in the ELB Target Group as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Active&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Healthy&lt;/code&gt; state (It need to wait few seconds to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Unhealthy&lt;/code&gt; once it reach out to the threshold of ELB HTTP health check), result in the ELB cannot forward the front-end request to the backend correctly.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-4-side-note-of-deregister.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 4. Note: issue cause by inconsistent state between Kubernetes and ELB&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 4. Note: issue cause by inconsistent state between Kubernetes and ELB

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;5) ELB received the DeregisterTarget request. So the ELB Target Group will start to perform connection draining(set old targets as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;draining&lt;/code&gt;), and mark the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-1&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-2&lt;/code&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;draining&lt;/code&gt; state, any new connection won’t be routed to these old targets.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-5-elb-doing-draining.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 5. ELB start to perform connection draining for old targets&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 5. ELB start to perform connection draining for old targets

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;6) However, here brings another issue: if the new targets (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-3&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Target-4&lt;/code&gt;) are still working on passing the health check of ELB(Currently those are in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Initial&lt;/code&gt; state), there has no backend can provide service at this moment, which can cause the ELB only can return HTTTP 5XX status code&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-6-response-5XX-error.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 6. ELB response HTTP 5XX error due to no healthy targets in can provide service&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 6. ELB response HTTP 5XX error due to no healthy targets in can provide service

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;7) Until the new Pods is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Running&lt;/code&gt; state as well as can react the health check reqeust from ELB through HTTP/HTTPS protocol, the ELB end up mark the targets as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Active/Healthy&lt;/code&gt; and the service become available&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-7-new-pod-in-active.png&quot; alt=&quot;Deployment workflow of ALB Ingress Controller - 7. The service need to wait a period to recover until new targets passed the ELB health check&quot; /&gt;
  
    &lt;figcaption&gt;
      Deployment workflow of ALB Ingress Controller - 7. The service need to wait a period to recover until new targets passed the ELB health check

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;how-to-resolve-the-issue-and-meet-zero-downtime&quot;&gt;How to resolve the issue and meet zero-downtime?&lt;/h2&gt;

&lt;p&gt;As mentioned in the previous workflow, it is required to use several workarounds to ensure the Pod state consistency between ALB, ALB Ingress Controller and Kubernetes. Here are few things you can aware:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Since version &lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/releases/tag/v1.1.6&quot;&gt;v1.1.6&lt;/a&gt;, ALB Ingress Controller introduced &lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/guide/ingress/pod-conditions.md&quot;&gt;Pod readiness gates&lt;/a&gt;: This feature can monitor the rolling deployment state and trigger the deployment pause due to any unexpected issue(such as: getting timeout error for AWS APIs), which guarantees you always have Pods in the Target Group even having issue on calling ELB APIs when doing rolling update.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example to add a readiness gate with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conditionType: target-health.alb.ingress.k8s.aws/&amp;lt;ingress name&amp;gt;_&amp;lt;service name&amp;gt;_&amp;lt;service port&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;(As it might be changed afterward,For more detail, please refer the documentation provided by ALB Ingress Controller (AWS Load Balancer Controller) project on GitHub):&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterIP&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;None&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-ingress&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alb&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;alb.ingress.kubernetes.io/target-type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;alb.ingress.kubernetes.io/scheme&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;internal&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-service&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/*&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;readinessGates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;conditionType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;target-health.alb.ingress.k8s.aws/nginx-ingress_nginx-service_80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For existing connections(As mentioned in workflow-4), the case is involving the gracefully shutdown/termination handling in Kubernetes. Therefore, it is requires to use the method provided by Kubernetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&quot;&gt;Pod Lifecycle&lt;/a&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preStop&lt;/code&gt; hook and make some pause(like using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sleep&lt;/code&gt; command) for Pod termination. This trick ensures ALB can have some time to completely remove old targets on Target Group (It is recommended to adjust longer based on your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Deregistration delay&lt;/code&gt;):&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;na&quot;&gt;lifecycle&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;preStop&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/bin/sh&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;40&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;70&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: If a container has a preStop hook configured, that runs before the container enters the Terminated state. Also, if the preStop hook needs longer to complete than the default grace period allows, you must modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terminationGracePeriodSeconds&lt;/code&gt; to suit this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;an-example-to-achieve-zero-downtime-when-doing-rolling-update-after-applying-methods-above&quot;&gt;An example to achieve zero downtime when doing rolling update after applying methods above&lt;/h3&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-deployment&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048-game&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;readinessGates&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;conditionType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;target-health.alb.ingress.k8s.aws/2048-ingress_service-2048_80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;70&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alexwhen/docker-2048&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Always&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2048&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;lifecycle&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;preStop&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/bin/sh&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;40&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is an example after following the practice I was getting a try. The deployment will apply the feature and can see the status of the readiness gates:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 2048-game &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES
2048-deployment-99b6fb474-c97ht   1/1     Running   0          78s   192.168.14.209   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           1/1
2048-deployment-99b6fb474-dcxfs   1/1     Running   0          78s   192.168.31.47    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           1/1
2048-deployment-99b6fb474-kvhhh   1/1     Running   0          54s   192.168.29.6     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           1/1
2048-deployment-99b6fb474-vhjbg   1/1     Running   0          54s   192.168.18.161   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           1/1
2048-deployment-99b6fb474-xfd5q   1/1     Running   0          78s   192.168.16.183   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &amp;lt;none&amp;gt;           1/1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once rolling the new version of the container image, the deployment goes smoothly and prevent the downtime issue as mentioned in previous paragraphs:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/zero-downtime-deployment-with-alb-demo.png&quot; alt=&quot;Zero downtime with ALB Ingress Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update&quot; /&gt;
  
    &lt;figcaption&gt;
      Zero downtime with ALB Ingress Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;In my scenario, the Kubernetes need to take at least 40 seconds termination period for single Pod, so the old targets are gradually moved out instead of remove all of them at once within few seconds, until entire target group only exists new targets.&lt;/p&gt;

&lt;p&gt;Also, the client can get normal responses from old Pods/existing connection during the deployment:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012028
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005383
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010174
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012233
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.007116
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010090
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012201
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005532
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010107
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012163
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005452
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.009950
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012082
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005349
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010142
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012143
2048
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005507

...
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012149
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005364
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010021
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.012092
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.005463
Welcome to nginx!
&lt;span class=&quot;nv&quot;&gt;HTTPCode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;200_TotalTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.010136
Welcome to nginx!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the practice in case having ALB Ingress Controller for doing graceful deployment with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RollingUpdate&lt;/code&gt;. However, it is another big topic need to be discussed regarding what type of the application when rolling the update. Because maybe some kind of applications need to establish long connection with the ELB or have requirement for considering persistence data need to be stored on the backend. All these things can bring out other issues we need to talk about.&lt;/p&gt;

&lt;p&gt;But in summarize, with the deployment strategy above, it is also recommended to design the client/backend application as stateless, implement retry and fault-tolerance. These mothod usually help to reduce the customer complain and provide better user experience in most common use case.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Due to the current design of Kubernetes, it is involving the state inconsistent issue when you are exposing the service with Application Load Balancer. Therefore, in this article, I mentioned the potential issue when doing rolling update in the scenario having container service integrating with ALB Ingress Controller (AWS Load Balancer Controller).&lt;/p&gt;

&lt;p&gt;Even the technology is always in revolution, I am still willing to help people better handle the deployment strategy. I used couple hours to draft this content and tried to cover several major issues, metioned things you might need to aware, break down the entire workflow and shared few practical suggestions that can be achieved in ALB Ingress Controller in order to meet the goal when doing zero downtime deployment.&lt;/p&gt;

&lt;p&gt;The article was written based on my working experience (Of course many communications back and forth with different customers using AWS), it might not be perfect but I hope it is helpful to you. For sure, if you find any typo or have any suggestions, please feel free and leave comment below.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html&quot;&gt;ALB Ingress Controller on Amazon EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/guide/ingress/pod-conditions.md&quot;&gt;Using pod conditions / pod readiness gates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/1124&quot;&gt;Issue#1124&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/814&quot;&gt;Issue#814&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/1064&quot;&gt;Issue#1064&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html">This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment and prevent 502 errors.</summary></entry><entry><title type="html">[Alexa] 靠一張嘴巴維運，我是如何成為出一張嘴的雲端工程師 - 使用 Alexa skill 管理 AWS EC2 防火牆 (Security Group) 規則</title><link href="https://easoncao.com/security-group-manager-alexa-skill/" rel="alternate" type="text/html" title="[Alexa] 靠一張嘴巴維運，我是如何成為出一張嘴的雲端工程師 - 使用 Alexa skill 管理 AWS EC2 防火牆 (Security Group) 規則" /><published>2020-08-30T00:00:00+00:00</published><updated>2020-08-30T00:00:00+00:00</updated><id>https://easoncao.com/security-group-manager-alexa-skill</id><content type="html" xml:base="https://easoncao.com/security-group-manager-alexa-skill/">&lt;p&gt;隨著 COVID-19 疫情大爆發，科技圈再次掀起了一股遠端辦公的趨勢，遠距工作的形式也逐漸改變許多人的工作型態。身為一名工程師，因應疫情，今年也頻繁的在家辦公也好幾個月了 (還好台灣還很安全)，仍有很多與傳統地域限制型態的工作模式有很多不同的地方。&lt;/p&gt;

&lt;p&gt;平常在辦公室，由於直接連接公司內部網路，通常都有特定的 IP 區段可以很簡單的掌握白名單，防火牆規則都非常好設置。但是自從開始遠端工作後，家裡的網路都是使用浮動 IP，有時候工程師的惰性驅使，又很懶得連上 VPN 在那邊拿 MFA Key  驗證身份，光是要設定自己 EC2 資源的白名單常常都要改來改去的。與此同時，在日常工作中也偶爾跟同事屁來屁去講一些垃圾話，注意到一些覺得可以進行自動化、增加生產力的一些想法及提案，秉持著發明家的精神，於是以下我就來說說我是如何靠一張嘴巴滿足我的懶惰的。&lt;/p&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;在沒有靠嘴巴維運之前，為了要正確的更改防火牆規則讓我能夠在家中工作時能夠連上跳板機器操作，我是這麼做的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;打開 AWS Management Console&lt;/li&gt;
  &lt;li&gt;輸入帳號密碼&lt;/li&gt;
  &lt;li&gt;彈出 MFA (Multi-Factor Authentication) 認證 -&amp;gt; 打開手機 App 確認現在的動態碼 -&amp;gt; 輸入動態碼&lt;/li&gt;
  &lt;li&gt;打開 EC2 Console&lt;/li&gt;
  &lt;li&gt;選擇 EC2 / Security Group&lt;/li&gt;
  &lt;li&gt;更新 EC2 規則 (獲取自己的外部 IP 地址更新上去)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述的動作常常都要花我 3-5 分鐘，有時候手機丟在臥室不在身邊又要走超遠！為了滿足我的懶惰，以下是我目前的工作流程，先來看一段示範影片&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/3AFGvt1Ck5c&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;上面的動作，只要正確的發出聲音指令後，Alexa Skill 便可以正確地更新相應的 Security Group 規則 (並且需要符合自己定義的 PIN Code，避免所有人都可以任意的發出命令更改)，並且只開放我家中正確對外的特定 IP 地址。如此一來，我就能直接地在家透過 SSH 連接上對應的 EC2 Instance 進行工作。&lt;/p&gt;

&lt;h2 id=&quot;什麼是-amazon-alexa-&quot;&gt;什麼是 Amazon Alexa ?&lt;/h2&gt;

&lt;p&gt;大概就像是 Apple 的 Siri、Google 推出的 Google Home …&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Amazon Alexa，簡稱 Alexa，是亞馬遜公司推出的一款智能助理，最初用於 Amazon Echo 智能音箱。它具有語音交互、音樂播放、待辦事項列表、鬧鐘、流播播客、播放有聲讀物以及提供天氣，交通，體育和其他實時信息（如新聞）的功能[3]。Alexa還可以將自身用作智慧家庭系統來控制多個智能設備。該產品由Amazon Lab126開發，是一名女性語音助手。用戶可以通過安裝插件（由第三方供應商開發的其他功能）來擴展Alexa功能。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alexa的大多數設備允許用戶通過一個特定的詞語（如「Alexa」或「Amazon」）來喚醒，剩下的（如IOS和Andriod上的Amazon移動應用與Amazon Dash Wand）則需用戶通過按按鈕來使之進入聆聽模式。一些其他廠商的手機也同樣支持用戶發出一些指令來喚醒屏幕，如「Alexa」或「Alexa wake」。到目前為止，Alexa的交流和應答僅可用英語、德語、法語、義大利語、西班牙語、葡萄牙語、日語和印地語。Alexa在加拿大可用於英語和法語（包括魁北克法語）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;來源: &lt;a href=&quot;https://zh.wikipedia.org/wiki/Amazon_Alexa&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;架構概覽-architecture-overview&quot;&gt;架構概覽 (Architecture Overview)&lt;/h2&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/security-group-manager-alexa-skill/security-group-manager-alexa-skill-architecture.png&quot; alt=&quot;Alexa Skill - Security Group Manager 架構概覽&quot; /&gt;
  
    &lt;figcaption&gt;
      Alexa Skill - Security Group Manager 架構概覽

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;上述的架構在我看完 Alexa Skills Kit SDK for Python 的範例後，整體的實作到部署我大概花了 3 - 5 小時實作出來。目前含註解整個代碼不超過 300 行，難度並不會太高，以下具體描述一些實作細節。&lt;/p&gt;

&lt;h2 id=&quot;實作細節&quot;&gt;實作細節&lt;/h2&gt;

&lt;h3 id=&quot;怎麼開發&quot;&gt;怎麼開發？&lt;/h3&gt;

&lt;p&gt;照著 &lt;a href=&quot;https://github.com/alexa/alexa-skills-kit-sdk-for-python&quot;&gt;Alexa Skills Kit SDK for Python&lt;/a&gt; 快速實作了一個可以動的版本。如果你是第一次玩 Alexa Skill，&lt;a href=&quot;https://github.com/alexa/skill-sample-python-colorpicker/blob/master/instructions/1-voice-user-interface.md&quot;&gt;Color Picker&lt;/a&gt; 是一個不錯的範例，你可以找到很多不同語言的相應實做。&lt;/p&gt;

&lt;h3 id=&quot;怎麼上傳到-aws-lambda&quot;&gt;怎麼上傳到 AWS Lambda&lt;/h3&gt;

&lt;p&gt;建立完 AWS Lambda Function 後，便可以打包你的程式碼上傳到 Lambda 執行。&lt;/p&gt;

&lt;p&gt;一般來說，AWS 文件可能會建議你使用 VirtualEnv 在本機建立 (如同在之前 &lt;a href=&quot;https://easoncao.com/create-a-line-bot/&quot;&gt;使用 AWS Lambda 建立 Line bot&lt;/a&gt; 中提到的)，但我其實偷了一些懶，直接用 Docker image 加上一行 Docker 命令打包 deployment package (python):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;:/var/task &lt;span class=&quot;s2&quot;&gt;&quot;lambci/lambda:build-python3.6&quot;&lt;/span&gt; /bin/sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;pip install -r requirements.txt -t package/; cp lambda_function.py package/; cd package; zip -r9 lambda.zip .&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;過程中會生成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package/&lt;/code&gt; 目錄，上傳該目錄底下壓縮完的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lambda.zip&lt;/code&gt; 即可。&lt;/p&gt;

&lt;h3 id=&quot;安全性設置---iam-policy&quot;&gt;安全性設置 - IAM Policy&lt;/h3&gt;

&lt;p&gt;我在 Lambda Function 使用的 Execution Role 中定義了下列規則：&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2012-10-17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Statement&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Sid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;UpdateSG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2:RevokeSecurityGroupIngress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2:AuthorizeSecurityGroupEgress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2:AuthorizeSecurityGroupIngress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2:RevokeSecurityGroupEgress&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:ec2:&amp;lt;REGION&amp;gt;:&amp;lt;ACCOUNT_ID&amp;gt;:security-group/sg-XXXXXXXX&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Sid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;DescribeSG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2:DescribeSecurityGroups&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如此一來可以避免我的應用程式未經授權操作其他的 Security Group 資源，並且只允許更改特定 Security Group ID 的相關 入站(Ingres) / 出站(Egress) 規則。&lt;/p&gt;

&lt;h3 id=&quot;alexa-如何接受指令&quot;&gt;Alexa 如何接受指令？&lt;/h3&gt;

&lt;p&gt;在 &lt;a href=&quot;https://developer.amazon.com/alexa/console/ask&quot;&gt;Amazon Alexa Developer Console&lt;/a&gt; 中建立完 Alexa Skill 後，可以進行相關的定義。 Alexa 會根據 Interaction Model 設定很多不同的 Intent，每個 Intent 能夠對應的相應操作行為，以下是一個當我想要輸入 PIN code 各種不同的命令範例：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/security-group-manager-alexa-skill/alexa-skill-intents-enter-pin-example.png&quot; alt=&quot;Alexa Skill - Security Group Manager 設置 Intent&quot; /&gt;
  
    &lt;figcaption&gt;
      Alexa Skill - Intent 設置

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;如此一來，在應用程式中便可以使用動態的名稱 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EnterPINCodeIntent&lt;/code&gt; 和變數 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PIN_CODE&lt;/code&gt; 來執行並且獲取一些邏輯。&lt;/p&gt;

&lt;h3 id=&quot;lambda-如何更新-security-group&quot;&gt;Lambda 如何更新 Security Group?&lt;/h3&gt;

&lt;p&gt;當 Alexa 根據觸發 Lambda 的時候，例如：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Alexa, update my security group!&lt;/code&gt;，根據我的設置，便觸發 “UpdateMySGIntent”，於是在相關的執行邏輯在包含 PIN Code 驗證的情況，可以是以下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;can_handle_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_intent_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UpdateMySGIntent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_my_sg_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Check if PIN code is provided in session attributes alues. If provided, then
    update security group with invoking source IP address.
    If not, then it asks user to provide the PIN code.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# type: (HandlerInput) -&amp;gt; Response
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;slots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request_envelope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slots&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIN_CODE_SLOT_KEY&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attributes_manager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_attributes&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attributes_manager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_attributes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PIN_CODE_SLOT_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre_defined_pin_code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PIN code matches, I am updating the security group.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response_builder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;update_response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_security_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Security group has been updated. {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reprompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;You can ask me your security group setting by saying, &quot;&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;what's my security group ?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You did not correctly specify the PIN code or the PIN code doesn't match&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reprompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I'm not sure what your PIN code is, &quot;&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;You can say, &quot;&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;my PIN code is blah blah blah....&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response_builder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reprompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handler_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response_builder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 Python 應用程式中，使用 DDNS 解析獲取目前外部 IP 後，更新 Security Group 的行為主要使用了 EC2 - &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AuthorizeSecurityGroupIngress.html&quot;&gt;AuthorizeSecurityGroupIngress API&lt;/a&gt; 執行了這項操作：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_security_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ec2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boto3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ec2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;security_group_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;security_group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ec2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SecurityGroup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;security_group_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_source_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cidr_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/32'&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;security_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;authorize_ingress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IpProtocol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tcp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CidrIp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cidr_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FromPort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToPort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The address {} has been added to the security group {} in region {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;security_group_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;security_group_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;alexa-skill-如何觸發-lambda&quot;&gt;Alexa Skill 如何觸發 Lambda?&lt;/h3&gt;

&lt;p&gt;在 &lt;a href=&quot;https://developer.amazon.com/alexa/console/ask&quot;&gt;Amazon Alexa Developer Console&lt;/a&gt; 中可以設置自定義的 Lambda function endpoint：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/security-group-manager-alexa-skill/alexa-skill-setup-lambda-endpoint.png&quot; alt=&quot;Alexa Skill - Security Group Manager 設置 Lambda endpoint&quot; /&gt;
  
    &lt;figcaption&gt;
      Alexa Skill - 設置 Lambda endpoint

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;lambda-如何知道目前的使用者外部來源-ip&quot;&gt;Lambda 如何知道目前的使用者外部來源 IP&lt;/h3&gt;

&lt;p&gt;根據我的觀察，由於 Alexa 觸發 Lambda Function 時，是由 Voice Server 去戳 Lambda，並且也沒有附帶相關的 IP 訊息。因此，為了達成我的目的，我主要使用了如同架構中描述的方式獲取 DDNS 中的紀錄取得真實外部 IP 地址後，進行更改。&lt;/p&gt;

&lt;p&gt;透過路由器韌體通常都支援更改 DDNS 的設定，通常好一點的路由器都支持一些很奇耙的功能，如果你會設定的話可以完成蠻多有趣的事情 (例如：&lt;a href=&quot;https://easoncao.com/enable-snmp-agent-on-dlink-dsl-7740c/&quot;&gt;在中華電信提供的 D-Link DSL-7740C 啟用 SNMP&lt;/a&gt;)。我的網路環境使用了 D-Link DSL-7740C，透過 D-Link 韌體提供的功能，能夠輕鬆的設定並且直接幫助更新我的 Dynamic DNS record，再由 Lambda Function 邏輯中主動解析獲取。&lt;/p&gt;

&lt;p&gt;同理，另一種方式是你可以在你的環境中運行一隻小程式 (agent) 或是透過 Cronjob 定義的 Shell Script，幫助你更新 DNS 紀錄，也是一樣的方法。&lt;/p&gt;

&lt;p&gt;若你知道 &lt;a href=&quot;https://aws.amazon.com/tw/api-gateway/&quot;&gt;API Gateway&lt;/a&gt; 是什麼的話，也許可以利用 API Gateway 搭建 endpoint 並且在上個動作的階段改用 Custom HTTPS endpoint 觸發，或許在交付客戶端的 Alexa device (比如 Echo dot) 觸發時，是由客戶端主動進行訪問，這種情況下，在附帶的請求訊息中也許能知道相關的 IP，但是我沒試過，如果你試了，也歡迎在底下分享你的發現。&lt;/p&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;本篇內容簡介了一項使用 Alexa Skill 進行系統維運的實作方法，展示了工程師的懶惰成性，並且分享一項參考架構，提及如何靠一張嘴巴更改防火牆規則，同時提及相關的實作細節。透過聲音命令簡化並且自動化繁瑣的更新步驟，減少了每次花費 3-5 分鐘的操作時間 (操作 10 次省下 30 分鐘、100次省下 300 分鐘 … 請自行誇飾及想像)，幫助提升日常工作中的相應生產力。&lt;/p&gt;

&lt;p&gt;若你對於這項實作感到興趣或是有其他建議，也歡迎在底下留言與我分享！&lt;/p&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="alexa" /><category term="alexa skill" /><category term="echo" /><category term="echo dot" /><summary type="html">隨著 COVID-19 疫情大爆發，科技圈再次掀起了一股遠端辦公的趨勢，遠距工作的形式也逐漸改變許多人的工作型態。身為一名工程師，因應疫情，今年也頻繁的在家辦公也好幾個月了 (還好台灣還很安全)，仍有很多與傳統地域限制型態的工作模式有很多不同的地方。</summary></entry><entry><title type="html">[Book] 原子習慣：細微改變帶來巨大成就的實證法則 - 我的心得及實作</title><link href="https://easoncao.com/atomic-habits-reading-feedback/" rel="alternate" type="text/html" title="[Book] 原子習慣：細微改變帶來巨大成就的實證法則 - 我的心得及實作" /><published>2020-08-02T00:00:00+00:00</published><updated>2020-08-02T00:00:00+00:00</updated><id>https://easoncao.com/atomic-habits-reading-feedback</id><content type="html" xml:base="https://easoncao.com/atomic-habits-reading-feedback/">&lt;p&gt;今年 5 月 (May, 2020) 我讀完 &lt;a href=&quot;https://easoncao.com&quot;&gt;安靜的力量&lt;/a&gt; 一書就在 Instagram (aka. IG) 上面問問有沒有人可以推薦書籍來一起交換心得，本書是在美國 UC 念研所的高材生 Chia-Tien 的真心推薦，一講完我就刷中文版進到我的 Kindle 書庫了 (Kindle 中文版竟然還比原文便宜！會中文真好 XD)。&lt;/p&gt;

&lt;p&gt;原本抱著對未來總有時間的期待能一次把它讀完，無奈生活、工作一被很多事情塞滿 (說白了應該是偷懶，耍廢的優先級總能比閱讀這種太營養的事情來的高 …)，我其實閱讀這本書的時候都是用很碎片、零碎到不行的時間 (睡前讀個 5 分鐘、幾頁之類的 ….)，今天終於有時間一次完結 (算一算也已經八月了 XD)，順便把 Kindle 上面做的筆記、重點一次給它摘要起來。因此，本篇內容主要簡述我對原子習慣 (Atomic Habits) 的讀書心得及自己的實作分享。&lt;/p&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;https://m.media-amazon.com/images/I/41dx7A1jFFL.jpg&quot; alt=&quot;&quot; /&gt;
  
    &lt;figcaption&gt;
      &lt;a href=&quot;https://amzn.to/2ZmQL1S&quot;&gt;原子習慣: 細微改變帶來巨大成就的實證法則 (Traditional Chinese Edition) - Buy kindle version on Amazon!&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;概覽&quot;&gt;概覽&lt;/h2&gt;

&lt;p&gt;本書著重於習慣的建立，並且歸納出好習慣建立的四大法則。書中舉了非常多各領域透過微小習慣帶來巨大改變的例子，在開頭舉了一個很有趣的案例，是英國自行車選手自從聘用新的國家隊教練 (Dave Brailsford) 透過非常多微小 (小到無法理解的) 的改變，像是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“重新設計單車座墊，使其坐起來更為舒適”&lt;/li&gt;
  &lt;li&gt;“輪胎上塗抹酒精，增加抓地力”&lt;/li&gt;
  &lt;li&gt;“穿上電熱式緊身褲，使肌肉在騎車時維持理想溫度”&lt;/li&gt;
  &lt;li&gt;“調整賽衣的布料，使選手在比賽時更加輕盈並且符合空氣力學”&lt;/li&gt;
  &lt;li&gt;“測試按摩油，看哪種能更快幫助肌肉恢復”&lt;/li&gt;
  &lt;li&gt;“請外科醫師教導選手洗手，以減少感冒機率”&lt;/li&gt;
  &lt;li&gt;“找出帶來最佳睡眠品質的枕頭與床墊”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;等等等等 …. 並且納入選手的訓練中以作為日常習慣的一部分。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;甚至是把後勤卡車的內壁刷成純白！以幫助查看到微小的塵埃以避免影響到精準調整的比賽用車！&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;這些細微的改善，累積起來，竟然大幅度提升英國自行車運動員的運動成績。原本一直吊車尾的英國車隊 (歐洲單車品牌製造商甚至拒絕販賣商品給英國車隊，以免對品牌造成負面影響)，才短短五年，英國自行車隊就在 2008 年北京奧運稱霸，拿下六成金牌，並且更在四年後創下九項奧運紀錄跟七項世界紀錄，聽起來都很不可思議。&lt;/p&gt;

&lt;p&gt;以下分享幾項我認為十分有幫助的概念進行摘要。&lt;/p&gt;

&lt;h2 id=&quot;習慣的複利效應&quot;&gt;習慣的複利效應&lt;/h2&gt;

&lt;p&gt;百分之一的改善並不特別值的注意 (&lt;strong&gt;有時候根本不被注意&lt;/strong&gt;)，但隨著時間過去，微小改善所能造成的變化十分驚人，例如：如果每天都能進步百分之一 (1%)，持續一年，最後會進步 37 倍 [1]。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;[1] Note: 我很好奇為什麼是 37 倍，網路上一堆分享似乎都只複製貼上書中的內容但是都沒說說為什麼，終於讓我找出答案是 1.01 ^ 365&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;相反地，若是每天退步百分之一 (1%)，持續一年，則其會弱化趨近於零 (0.99 ^ 365 ~= 0.03)。&lt;/p&gt;

&lt;p&gt;換句話說，習慣就是「自我改善」這件事情的複利，並且效果會在你重複執行的過程中加倍。&lt;strong&gt;若隨便挑一天執行習慣的行為來看，其效應似乎很小，然而，一旦經過幾個月、甚至幾年下來，就有可能造成極巨大的影響。&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/habit-compound.png&quot; alt=&quot;習慣帶來的複利效應&quot; /&gt;
  
    &lt;figcaption&gt;
      習慣帶來的複利效應

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;習慣的建立系統&quot;&gt;習慣的建立系統&lt;/h2&gt;

&lt;p&gt;要在日常生活中體會上述的概念並不容易，並且人的大腦天生設計出來就是通常特別喜歡偷懶、簡單、立即反饋性的事情。這正說明為什麼我們寧可躺在床上滑一小時社群媒體，而不願意走出門外出去跑步一個小時，因為後者相對耗費了更多精力，並且沒有立即性反饋的效應。&lt;/p&gt;

&lt;p&gt;滑臉書一個小時，吸收訊息的同時滿足了大腦想接受感興趣訊息刺激的渴望，同時觸發了大腦的愉悅機制，立即被滿足。例如：發佈文章後，很迫切的關注自己的貼文有多少個讚，得到很多個讚後，可以滿足某種層次上想被關注的渴望，於是習慣機制被建立，你不自覺的反覆以下的習慣公式：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;自然的拿起手機拍照及錄影 (例如跟朋友吃個飯仍彼此互拍對方及食物，即使在旁人眼裡無法理解，你還是會習慣這麼做) -&amp;gt; 打開社群媒體 App -&amp;gt; 修圖 / 美化內容 -&amp;gt; 發佈文章 / 發佈限時動態 -&amp;gt; 渴望得知獲得的讚數和評價&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上述的動作可能短短付出幾分鐘就能得到反饋，這正是為什麼人機介面專家不斷地在努力優化即時反饋功能 (利用通知數及紅色觸發提示)，跟一群資料科學家們，認真的利用機器學習努力的學會識別你感興趣的內容並且嘗試投放你可能會喜歡的資訊，以奪走你的注意力。&lt;/p&gt;

&lt;p&gt;反之，出去跑步一個小時，你可能得到的是，身體會非常疲憊，同時，若你剛接觸跑步，你也不可能跑一天就成為跑步大師，短短幾天就能在一小時內跑完 10 公里，甚至從中獲得讚美及成就感滿足你大腦渴望的需求。這個動作的反饋其帶來的成果總是來得不夠快，於是你仍決定回去滑社群軟體一小時而不是踏出門跑步。&lt;/p&gt;

&lt;p&gt;你不會因為今天只吃一餐垃圾食物，體重計上的指針就突然大幅度的移動，特別是大腦往往漠視對於當下的單一決定其長期帶來的影響。然而，當日復一日重複百分之一的錯誤，複製不當決策及係為過錯，並且將小藉口合理化，這些小小的選擇隨著複利的威力，變成有害的後果。因此，本書舉了非常多的實例並且具體提出幫助檢視習慣的方式，以了解自己的習慣是否處於正確的軌道，同時提出多項方法，以建立系統化的方式建立習慣。&lt;/p&gt;

&lt;p&gt;也許書中內容有一部分為了增加故事性，把很多枝微末節的東西湊起來，以不斷強調微小習慣建立的重要性。但本書核心仍著重於習慣的建立，並且歸納出下列好習慣建立的四大法則：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;讓提示顯而易見 (Make it obvious)&lt;/li&gt;
  &lt;li&gt;讓習慣有吸引力 (Make it attractive)&lt;/li&gt;
  &lt;li&gt;讓行動輕而易舉 (Make it easy)&lt;/li&gt;
  &lt;li&gt;讓獎賞令人滿足 (Make it satisfying)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;反之，若要學會破除壞習慣，則可以反轉上述法則&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;讓提示隱而不現 (Make it invisible)&lt;/li&gt;
  &lt;li&gt;讓習慣毫無吸引 (Make it unattractive)&lt;/li&gt;
  &lt;li&gt;讓行動困難無比 (Make it hard)&lt;/li&gt;
  &lt;li&gt;讓後果令人不滿 (Make it unsatisfying)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;讓提示顯而易見&quot;&gt;讓提示顯而易見&lt;/h3&gt;

&lt;p&gt;行為的改變始於察覺，必須先意識到自己的習慣，才能開始改變它。&lt;/p&gt;

&lt;p&gt;我們觀察自己日常的小習慣被觸發的線索 (例如：起床順手拿起手機、肚子餓時就不自覺打開外送 App … etc)，並幫這個習慣評比，同時請考慮&lt;strong&gt;最長遠的益處&lt;/strong&gt;，如果無法決定這個習慣是好還是壞，可以問自己：「這個習慣能幫助我成為我想要成為的那種人嗎？對於我渴望的身份認同，這個習慣是投下同意還是反對票？」。&lt;/p&gt;

&lt;h3 id=&quot;讓習慣有吸引力&quot;&gt;讓習慣有吸引力&lt;/h3&gt;

&lt;p&gt;習慣就是多巴胺驅動的回饋迴路。極度容易養成習慣的行為──吸毒、吃垃圾食物、打電動、瀏覽社群媒體──都與較高濃度的多巴胺有關，亦適用於最基本的習慣：飲食、喝水及社交。讓我們採取行動的，是對獎賞的預期。大腦分配給「想要」獎賞的神經迴路 (腦幹、依、腹側被蓋區、背側紋狀體、杏仁核以及部分前額葉皮質)，遠比分配給「喜歡」獎賞的要多。&lt;/p&gt;

&lt;p&gt;也許你想要知道最新名人的八卦，但又需要鍛鍊身材，你可以要求自己只能在健身房邊跑步邊看八卦新聞、看實境秀。善用誘惑綑綁，幫助你將獎賞與提示產生連結，以更好地吸引你養成該項習慣。&lt;/p&gt;

&lt;h3 id=&quot;讓行動輕而易舉&quot;&gt;讓行動輕而易舉&lt;/h3&gt;

&lt;p&gt;習慣的養成取決於頻率，而非時間，如同身體肌肉會對規律的重量訓練產生反應，大腦的特定區域也會在被使用時增長，在被拋棄時萎縮。每次重複一個行為，你就活化了跟那個習慣有關的神經迴路。&lt;/p&gt;

&lt;p&gt;大腦設定會盡可能節省能量，人類的天性仍遵循「最小努力原則」，在兩個類似的選項中抉擇時，人自然傾向選擇花費最少力氣的那個。養成一個習慣所需的能量越少，養成的可能性越高。這意味著，&lt;strong&gt;讓習慣簡單到就算沒有意願也會執行至關重要。&lt;/strong&gt;若能讓執行好習慣更加方便，你就更有可能貫徹。(健身房在上下班路上，你就比較容易去健身，因為順路停下來不會為原本的生活增添太多阻力。要是健身房不在通勤路上，就算只差幾公尺，也會變成像是「特地」去健身，養成固定去健身房的習慣就更加困難。)&lt;/p&gt;

&lt;h3 id=&quot;讓獎賞令人滿足&quot;&gt;讓獎賞令人滿足&lt;/h3&gt;

&lt;p&gt;生活在現代，你做的許多選擇都&lt;strong&gt;不會&lt;/strong&gt;馬上得到好處。你在工作上表現優異，你會在&lt;strong&gt;幾週後&lt;/strong&gt;收到薪水支票; 今天運動，也許&lt;strong&gt;明年&lt;/strong&gt;就不會過重; 現在開始儲蓄，也許&lt;strong&gt;幾十年&lt;/strong&gt;後就有足夠的錢享受退休。從遠古時期，人類的大腦必須響應立即性的回饋 (吃什麼、在哪裡睡覺、如何躲避獵食者，總是把焦點放在當下或是接近的未來) 這與大腦的設計背道而馳，正是為什麼延遲回饋的習慣特別難容易養成的原因。&lt;/p&gt;

&lt;p&gt;因此，習慣必須讓人感受到愉快，我們才有可能持續重複某一行為。&lt;/p&gt;

&lt;p&gt;「迴紋針策略」：1993年，加拿大一位股票經紀人，每天早晨，會把兩個罐子放在辦公桌上，一個是空的、另一個則放了 120 個迴紋針。每天一做好開工準備，他就會打一通業務拜訪電話，一旦講完，就立即的把一個迴紋針移動到空罐，重複整個過程。直到所有的迴紋針都被移動到另一個罐子裡。&lt;/p&gt;

&lt;p&gt;這種視覺上的測量提供了進步的清楚證據，並且強化行為，且為活動增添一點立即的滿足感。透過追蹤習慣，更利於習慣的養成。&lt;/p&gt;

&lt;h2 id=&quot;改變習慣最有效的方法是改變身份認同&quot;&gt;改變習慣最有效的方法，是改變身份認同&lt;/h2&gt;

&lt;p&gt;大部分人無法堅持一項習慣，通常是因為人們對於習慣的建立，是基於&lt;strong&gt;「目標」&lt;/strong&gt;導向，例如：我必須培養運動的習慣，每週必須跑 10 公里，以幫助我能夠順利在三個月後的路跑活動中順利跑完半程馬拉松。然而，一旦完成該項目標後，原本培養的習慣就難以維持，便很容易復歸 (大腦總會有個聲音：啊！今天休息一下好了，於是在幾天後變成，下個月再跑好了！)，於是習慣的系統瞬間瓦解。&lt;/p&gt;

&lt;p&gt;在本書中，特別強調「習慣」這件事，其實就是通往身份轉變的道路。每當你選擇做某個壞習慣，就投了一票給予你想成為的身份 (例如：我選擇每天下班去酒吧來好幾個 shot，這樣我允許我自己成為愛喝酒的人，並且這意味著&lt;strong&gt;我願意成為&lt;/strong&gt;那樣的人。)。因此，想要維持一個習慣，首先，&lt;strong&gt;先決定你自己想要成為什麼樣的人&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;你想要學習精進自己的習慣？那先改變你自己的身份認同吧！ — 「我是一個終身學習的人」&lt;/li&gt;
  &lt;li&gt;你想要保持健身的習慣？那先改變你自己的身份認同吧！ — 「我是一個擁有良好運動習慣的人」&lt;/li&gt;
  &lt;li&gt;你想要保持理財或是存錢的好習慣？那先改變你自己的身份認同吧！ — 「我是一個擁有理財紀律並且是擁有規劃財務習慣的人」&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你想成為一個健康的人，隨時問問自己：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;一個健康的人會怎麼做&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;這種想法變使得你的大腦在日常生活中進行決策時，不自覺地導入你想養成習慣的相應決策，並且成為你的行為指南：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;一個健康的人會選擇走路或搭計程車？&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;一個健康的人會點墨西哥捲餅或是沙拉？&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;擁有上述的自我認同後，便是不斷的透過生活中的各項小勝利來向自己證明 (今天又完成了這個習慣)，&lt;strong&gt;永遠將習慣的建立放置焦點在成為某一種人，而非得到某一種成果&lt;/strong&gt;，如此一來，習慣的建立便輕而易舉。&lt;/p&gt;

&lt;h2 id=&quot;我的實作成果&quot;&gt;我的實作成果&lt;/h2&gt;

&lt;p&gt;先說說我自己的心得，身為一名雲端工程師，工作性質不外乎就是長時間待在電腦前面弄得出神入化，可能一個小時內幫助很多很知名的品牌、線上服務提供非常實用的技術性建議，以處理很多系統故障的狀況、找出客戶開發團隊自己埋的坑，拯救全世界因為網路連結那些孤寂的靈魂和龐大的虛擬需求，但外人完全搞不懂你在衝啥。不管是軟體工程師、系統工程師、維運工程師、各種打雜工程師，總之，不論身在哪個職位、一般人對於工程師的印象，就是&lt;del&gt;又宅又臭&lt;/del&gt;，並且讓人感覺除了自己專業上面的事情，其餘事情就是一竅不通。但我就是不甘願這種刻板印象 (沒錯，我就是要&lt;strong&gt;打！破！它！&lt;/strong&gt;)，當然，這些習慣有一部分出於對自己所做事情的喜愛，因為不希望自己分享的內容又淪為空泛的談論，因此，本節以我自己為例，以自身的經驗分享我自己的實作成果，分為兩個部分：馬拉松跑者、大力士計畫&lt;/p&gt;

&lt;h3 id=&quot;馬拉松跑者之路&quot;&gt;馬拉松跑者之路&lt;/h3&gt;

&lt;p&gt;我自己的目標，起初是希望擁有一個健康的身體，並且想要學習跨出舒適圈，挑戰自己。於是我在 2018 年底，就開始啟動自己的長跑訓練計畫，目標當然是能靠自己完成跑完半程甚至是全程馬拉松的距離。當然，這都是我在接觸這本書之前就有的計畫，但是真的就是一股腦的亂訓練，也沒有正確的學習培養習慣。在大學時期有時候想到運動就會去跑跑，但是距離上可能 5 公里內就差不多了，而且訓練非常看心情，跑步這件事，常常可以有非常多理由就中斷。這件事情在 2019 年初設下想要跑半馬的目標後，真的就是憑一股傻勁在維持：&lt;/p&gt;

&lt;div class=&quot;feature__wrapper&quot;&gt;

  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-2018.jpg&quot; alt=&quot;2018 - 2019 Jan&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2018 - 2019 Jan&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-june-2019.jpg&quot; alt=&quot;2019 May - June&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2019 May - June&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-aug-2019.jpg&quot; alt=&quot;2019 July - Aug&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2019 July - Aug&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;從上面的紀錄可以看到我的訓練其實是斷斷續續的，當初可能就想週末至少跑一下長距離 10 公里。但是仍缺乏維持的系統。所以可能跑一週，一次一股腦衝個兩天，但下週又總能找到理由休一週。直到接近賽季的時候才認真訓練起來 (我的初半馬在十月 - October, 2019)：&lt;/p&gt;

&lt;div class=&quot;feature__wrapper&quot;&gt;

  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-sep-2019.jpg&quot; alt=&quot;2019 Sep&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2019 Sep&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-dec-2019.jpg&quot; alt=&quot;2019 Dec&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2019 Dec&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;然後我也如期完賽了，初半馬跑出 02:20:46 的成績，10 月跑完直接休快一個月，結果繼續到 2019 年底斷斷續續的在維持幾天：&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;a href=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/pocari-sweat-run-2019-self-and-dashboard.jpg&quot; title=&quot;Pocari Sweat Run 2019 - 21km&quot;&gt;
          &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/pocari-sweat-run-2019-self-and-dashboard.jpg&quot; alt=&quot;Pocari Sweat Run 2019&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/pocari-sweat-run-2019-record.jpg&quot; title=&quot;Pocari Sweat Run 2019 - 21km 完賽證明&quot;&gt;
          &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/pocari-sweat-run-2019-record.jpg&quot; alt=&quot;Pocari Sweat Run 2019&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;2019 Pocari Sweat Run - 21km - 02:20:46
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;對於跑步這件事情，起初我的想法是喜歡這項運動，特別是執行長距離跑步時，是一種與自己對話的過程，從中學習沈澱自己的思緒和被工作跟一堆雜事打到紊亂的生活節奏，小小的萌生想成為市民跑者的想法，仍而，對於身份認同仍不夠強烈，所以訓練計畫總是斷斷續續的。&lt;/p&gt;

&lt;p&gt;直到 2020 年，開始轉變自己對於跑步的心態 — 「我希望自己老了也可以繼續跑步」，直到閱讀原子習慣後更確切自己的身份認同 — 「不以競賽為目標，我想成為一名馬拉松跑者並且不斷的突破自己，將馬拉松的哲學實踐在我的生活中，學習面對生活的各種挑戰」。於是跑步這件事情在我心中，不再是一個在社交圈中跟別人展示自己又參加了哪些馬拉松，心裡的想法十分單純明確：我就是想學習一直跑下去，並且持續的一點一點突破自己 (耐力多提升個 100 公尺、到終點前不要停下來，試著比昨天努力試著衝點間歇)，於是心態更加輕鬆了，身體修復的速度比不上課表，那就減量、降低強度，下週在試一次！於是，維持這項運動就像是一種樂趣。&lt;/p&gt;

&lt;p&gt;基於四項原則，我的實作方式如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;讓提示顯而易見 (Make it obvious): 例如我設定週跑量要滿足至少 20km，一週執行三天。我的做法是，預先在 Google Calendar 設定好重複性的提醒，如此一來，在特定時間 (比如：週日、週三、週五) 就會擁有這項提示，我不管是前一天檢視待辦或是當天，就會在固定時間接收這項提示已觸發行為。&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-reminder-on-google-calendar.png&quot; alt=&quot;Run reminder on Google Calendar&quot; /&gt;
  
    &lt;figcaption&gt;
      提示: 我 Google Calendar 上的跑步提醒項目

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;讓習慣有吸引力 (Make it attractive): 這一部分仰賴前面的身份認同，並且受跑步獨處的自由吸引 (以及聽振奮人心的音樂)，另一種輔助方式則是，我會 “想要” 增加自己 Nike Run 的成績紀錄，以及，我會預期自己跑完之後，會發一則&lt;del&gt;廢文&lt;/del&gt;到社交動態 (例如：Instagram)，或是預期自己未來藉由報名路跑證明自己可以完賽。因為會 “想要” 上傳社交動態，以展示自己很努力的一面，並且幫助大家建立對我自己想要成為的身份認同並且得到尊重，這使得在習慣觸發之前提供巨大吸引力。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;讓行動輕而易舉 (Make it easy)：買了一雙好跑的跑鞋，同時，去住家附近的場所跑步，走個幾步就到！&lt;/li&gt;
  &lt;li&gt;讓獎賞令人滿足 (Make it satisfying)：透過使用 Nike Run App 和上傳成績到社群軟體，有助於我追蹤習慣並且&lt;del&gt;滿足自己裝 B 的需求&lt;/del&gt;。一部分是完成習慣後我會願意獎賞自己特調的運動飲料、氣泡果汁，以補充失去的鹽分跟電解質，這刺激了完成習慣的渴望。並且，我會在 Google Calendar 在當天該項提醒事項標註為已完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/complete-reminder-example.png&quot; alt=&quot;Complete reminder example&quot; /&gt;
  
    &lt;figcaption&gt;
      獎賞: 在 Google Calendar 上標註完成項目

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;div class=&quot;feature__wrapper&quot;&gt;

  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-april-2020.jpg&quot; alt=&quot;2020 April&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2020 April - total 34.63 km / 3 runs&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-may-2020.jpg&quot; alt=&quot;2020 May&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2020 May - total 38.16 km / 4 runs&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-june-2020.jpg&quot; alt=&quot;2020 June&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2020 June - total 56.27 km / 7 runs&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/run-record-in-july-2020.jpg&quot; alt=&quot;2020 July&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;2020 July - total 142.3 km / 16 runs&lt;/p&gt;

            &lt;/div&gt;
          

          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;於是系統建立後，月跑量就逐步地趨近穩定，一週時間到，就很自動的離開座位、穿上鞋子、戴上耳機，暖身起跑。我在撰寫這篇內容才發現我七月的月跑量隨著這樣的執行，已經不自覺的突破 100km。當然，習慣的建立，其核心仍在於持續，我當然不會要求自己一定要每個月都要到達這個數字，但仍將跑步視為我生活的一部份，可以很自信的說，這確實是一種習慣。&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/nat-geo-run-2020.jpg&quot; alt=&quot;Nat Geo Run 2020&quot; /&gt;
  
    &lt;figcaption&gt;
      Aug 9 - 2020 國家地理路跑 - 21km - &lt;a href=&quot;https://bravelog.tw/html/race/record/3/01880&quot;&gt;半馬大會成績 02:17:37&lt;/a&gt;

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h3 id=&quot;大力士計畫&quot;&gt;大力士計畫&lt;/h3&gt;

&lt;p&gt;我的健身計畫嚴格說起來是 2020 開始學習執行的，我年初連怎麼操作動作真的完全沒有概念。其實背後有類似前面提及的動機，都是想培養一輩子的習慣，礙於篇幅跟打得有點累，以下就快速分享我個人習慣建立的幾項重要記事：&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/2020/08/atomic-habits-reading-feedback/start-workouut-in-2020.jpg&quot; alt=&quot;Start workouut in 2020&quot; /&gt;
  
    &lt;figcaption&gt;
      2020 Jan 開始學習成為一名健人

    &lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;大力士習慣系統建立的方式如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;讓提示顯而易見 (Make it obvious): 如同前面的操作雷同，我一部分使用了 Google Calendar 的提醒標註以提供提示&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;讓習慣有吸引力 (Make it attractive): 操作與前面雷同，一部分仰賴前面的身份認同，我會 “想要” 增加自己健力三項的成績紀錄、打造更強壯的身體，想要有更強健的肌力，獲得更好的生產力、提升跑步表現。並且，預期自己執行完成後，一樣會發一則&lt;del&gt;廢文&lt;/del&gt;到社交動態 (例如：Instagram) 凸顯我的力量巨大無比！！！！因為會 “想要” 上傳社交動態，以展示自己很努力的一面，並且幫助大家建立對我自己想要成為的身份認同並且得到尊重，這使得在習慣觸發之前提供巨大吸引力。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;讓行動輕而易舉 (Make it easy)：健身房在辦公室附近、下班想轉換心情去信義區晃晃先順路去摔一下槓 … 等等&lt;/li&gt;
  &lt;li&gt;讓獎賞令人滿足 (Make it satisfying)：透過使用 Strong (App) 追蹤自己的重量變化，為了幫助自己校正動作，我會一併錄下自己的動作。於是，獎賞系統可以變成：&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;努力訓練 -&amp;gt; 在 Strong 紀錄自己的重量觀察自己的力量成長 -&amp;gt; 完成後來杯高蛋白乳清補充能量 -&amp;gt; 發佈自己的執行動作到社交動態 -&amp;gt; 追蹤自己的成長&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2020 由於疫情爆發，大部分時間還是會 WFH (Work From Home)，去健身房瞬間變成一項成本很高的行為 (從我家搭捷運到原本的健身房快 30 分鐘)。但如同前面提及上述習慣系統的建立 (我在去往健身房的路上也加入了一些習慣綑綁的行為，這項時間變成一項額外的享受)，由於健身對於我來說是一項有十足吸引力的習慣，同時完成訓練後可以立即發佈的相應獎賞機制，利用科技助於我追蹤習慣，並且某種程度上滿足讓其他人認同我身份的渴望，仍有十足動力可以驅使我一週至少三天準時往健身房跑。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Squat: 77kg - (1RM: 90kg)&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/kUWJSPmS_Kc&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Deadlift: 119kg - (1RM: 119kg)&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/LL88fpwuo_k&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;Note: 上述節錄自近期 (2020 Aug) 的限時動態，健身運動仍是一項長而持續的累積，基於動作習慣，上面的操作仍有進步空間，還在努力提升動作品質，不完全是正確的動作要領。&lt;/p&gt;

&lt;h2 id=&quot;總結&quot;&gt;總結&lt;/h2&gt;

&lt;p&gt;原子習慣列舉了眾多淺而易懂的實例，描述在生活中習慣建立的前因後果，包含科學及研究理論的導論，具體提供非常多的實務性建議。本書列舉了習慣系統建立的方法，幫助讀者有效的打造習慣，同時提供了破除壞習慣的持續的導引。同時，我在自己的日常生活中嘗試實踐，其理論某種程度上，得以於實務上被印證，該系統能有效幫助習慣的建立，並且得以持續。&lt;/p&gt;

&lt;p&gt;本書仍提及眾多習慣建立的準則和注意事項，細節無法一一列舉，作為習慣導引的工具書，是一本實踐後，值得一再閱讀審視的參考文獻。&lt;/p&gt;

&lt;h2 id=&quot;相關資源&quot;&gt;相關資源&lt;/h2&gt;

&lt;p&gt;如果你想獲得更多具體的習慣建立方式，可以透過下列連結取得相關的電子、實體書籍版本&lt;/p&gt;

&lt;div class=&quot;feature__wrapper&quot;&gt;

  
    &lt;div class=&quot;feature__item&quot;&gt;
      &lt;div class=&quot;archive__item&quot;&gt;
        
          &lt;div class=&quot;archive__item-teaser&quot;&gt;
            &lt;img src=&quot;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;amp;MarketPlace=US&amp;amp;ASIN=B07VRLS2LG&amp;amp;ServiceVersion=20070822&amp;amp;ID=AsinImage&amp;amp;WS=1&amp;amp;Format=_SL250_&amp;amp;tag=eason06b-20&quot; alt=&quot;原子習慣: 細微改變帶來巨大成就的實證法則 (Traditional Chinese Edition)&quot; /&gt;
            
          &lt;/div&gt;
        

        &lt;div class=&quot;archive__item-body&quot;&gt;
          
            &lt;h2 class=&quot;archive__item-title&quot;&gt;原子習慣: 細微改變帶來巨大成就的實證法則&lt;/h2&gt;
          

          
            &lt;div class=&quot;archive__item-excerpt&quot;&gt;
              &lt;p&gt;數位版售價 $7.84&lt;/p&gt;

            &lt;/div&gt;
          

          
            &lt;p&gt;&lt;a href=&quot;https://amzn.to/2ZmQL1S&quot; class=&quot;btn btn--inverse&quot;&gt;Buy on Amazon&lt;/a&gt;&lt;/p&gt;
          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

&lt;/div&gt;</content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="book" /><category term="reading" /><category term="habit" /><category term="atomic" /><category term="kindle" /><category term="amazon" /><summary type="html">今年 5 月 (May, 2020) 我讀完 安靜的力量 一書就在 Instagram (aka. IG) 上面問問有沒有人可以推薦書籍來一起交換心得，本書是在美國 UC 念研所的高材生 Chia-Tien 的真心推薦，一講完我就刷中文版進到我的 Kindle 書庫了 (Kindle 中文版竟然還比原文便宜！會中文真好 XD)。</summary></entry></feed>