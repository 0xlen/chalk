var store = [{
        "title": "2016 不平凡的一年",
        "excerpt":"         2016年9月 升上大三   大學生涯邁入1/2，其實在這個階段，   我的確迷失方向，但是我相信自己的無懼。      承認自己的失敗，修正，然後繼續前進。    是我今年最大的體悟。   過去與現在   今年，   進入學校計中工作，幸運的是，進入了網路組。   我喜歡這份工作，除了對於網路層面的技術有更深的了解，   重要的是，學習如何處理問題的能力。   可貴的是，有一群認真的老師及學長不吝為我分享他們自己的經驗，   讓我從中獲得不一樣的啟發。   大一，其實一切都好，但是漸漸的，似乎跟自己預期走的方向越來越遠。   大二，接了系學會幹部。   接了幹部這年，課業、能力得過且過，   當然，也不能說大二的一切都是白費。   對於活動上的付出，   就像跟Steve Jobs一樣，開發Mac時想到了旁人看起來無用的書法課。   雖然說現在做的一切，也許無用；或許十年後，能有所幫助。   但是，站在大三的時間軸上，我還是迷惘了。   一次的計網工作下班後，跟老師暢談的夜晚，   仔細想想，我的確錯過了那個學習、瘋狂陶醉技術的自己。   重新檢視自己的道路，   也許走得路並不是這麼的直，中間偏離了自己預期的方向，   不可否認的是，我的確也從接任幹部的這一年學習到很多，   不管是想法的改變、對事情的見解，   以及與人際的相處、溝通能力，企劃及變通的能力。   覺得可惜的是，課都沒有認真上，   考試也沒有認真考，忙著弄系學會的事情。   當然， 也是有一些深刻的事情，   特別是接下資工營的總召， 實在是很特別的經驗，也間接實踐了自己的一些想法，   希望能藉由自己的經驗傳承，創造系上的一些不同，與改變。   不怕失敗   整年最深刻的成長，大概就是願意承認自己的錯誤。   人們總是害怕自己承認失敗的後果，   覺得自己會被責備、或是面對與論的壓力。   在課業上，我的確失敗；在過去活動驗收時，我遲到，我的確做得不好。   但是我願意承認自己的錯誤，   所以在2016快要進入尾聲的這刻，   決定重新找回遺失的自己，   更無懼的為自己的想法負責，努力執行。   重新繼續瘋狂的生活， 回歸最原本的道路，   才是我心中最渴望的本質，而且努力挑戰自己。   2016尾聲 後記   不要小看我買domain這個動作，   現在又開了GitHub page用jekyll自言自語，當然最期許的是自己能開始做出很多改變。   最近也投了AWS的internship，   希望有機會能進到面試階段、期待自己能進去看看，   也想試試看不同的公司、不同的文化，   更讓自己有更多不同的想法與成長。   對研究所也開始有一些見解與看法，   慶幸的是，至少沒這麼迷惘了，   心態也沒這麼失衡了，希望能找回更多動力與前進的契機。      2016.11.01 無懼，然後繼續前進   ","categories": [],
        "tags": ["mood"],
        "url": "https://easoncao.com/2016-year/",
        "teaser": "https://easoncao.com/assets/images/posts/2016/11/2016-year/direction.jpg"
      },{
        "title": "中華電信hinet 申請及設定IPv6 (DSL-7740C)",
        "excerpt":"                  Ping facebook using IPv6            前情提要   因為近期hinet的路由走國外海纜一直發生掉包的情形,   不管是facebook, slideshare很多有走相關國外CDN都傳出災情，   所以就不小心申請了IPv6,   希望這陣子藉著走IPv6的路由節點壓壓驚,   當然最主要的原因還是想申請IPv6玩玩看,   申請的程序也不需要跑中華電信營業門市,   藉這個機會就給他申請了.   申請IPv6   這部份原本想截圖, 不過發現申請過後沒辦法照正常的程序跑一遍,   所以申請步驟還是麻煩詳細參照一下這邊的步驟,   後面就主要著墨在Router端的設定,   當初11/3半夜線上申請,   以為收到mail通知申請程序就以為馬上就開通IPv6 dual stack的功能了,   還在傻傻的想說設定怎麼跑出來,   隔天11/4早上課上到一半hinet透過電話主動通知我,   告知機房已經完成IPv6 dual stack的設定了,   效率其實蠻快的.   進入D-Link DSL-7740C設定頁面   當初申請光世代100M/40M裝機人員所使用的機種正是D-Link DSL-7740C   我目前申請的位置沒辦法直接跑100M/40M,   必須透過50M兩條並行傳輸, 對我來說是蠻新鮮的,   特別是拿到Router後, 一連進去設定頁面發現其實功能蠻強大的,   只是一直沒有好好玩它XD   (要連進去小烏龜通常使用192.168.1.1這組IP在LAN的環境下應該就能正常進入)   (若不行則要先檢查一下default gateway是什麼去找出小烏龜的IP)   相關的帳號密碼會依據申請hinet的營業位置有不同的設定,      帳號: cht  密碼(北區): chtnvdsl  密碼(中區): chtcvdsl  密碼(南區): chtsvdsl    2021 更新 (DSL-7740C 適用帳號密碼)   最近注意到上述的密碼基於韌體更新有一些變動，如果無法登入的朋友可以嘗試以下訊息：      帳號: cht   密碼: 740C+LAN MAC 末四碼   MAC 位置可以翻下機器背面會有相關的資訊，例如：                     DSL-7740C MAC 位置            像上述範例密碼可以為：740CBA87   確認設定的帳號   申請hinet網路服務後, 營業處應該都會發一組HN號碼及HN密碼,   用於PPPoE驗證連線使用,   一般光世代浮動用戶帳號格式為: HN號碼@hinet.net (固定ip用戶則為@ip.hinet.net)   礙於個資法規定, 通常hinet裝機人員到府裝機設定時都會用一組@wifi.hinet.net的帳號密碼,   wifi.hinet.net跟hinet.net兩組的密碼是不同的,   wifi.hinet.net是方便裝機人員能直接進行網路設定, 密碼通常只有該營業處能得知,   而且稍微爬過資料得知這組帳號可能會發生沒有辦法拿到IPv6位址的情況 (當然說不定也可以),   帳號設定就請使用@hinet.net為主,   WAN介面及帳號設定   進到WAN Setup頁面可以看到4個interface (WAN1_4是我自己設定的),                     WAN設定            其中WAN1_2是原本hinet裝機人員設定的,   裡面設定的帳戶就是@wifi.hinet.net.            因為在設定的時候不希望影響原有的網路設定,   在WAN Setup頁面按下Add按鈕,   另外新增了一個interface WAN1_4, 帳號使用@hinet.net,            以下是相關設定 (如果是直接更改原本就有的 wifi.hinet.net 帳號則只須更改 2. 即可)      WAN Setting 選擇 PPPoE   username填入 HN號碼@hinet.net   password/confirm password填入對應的密碼   Default Route打勾 (確定能正常connect之後建議再將這個選項打勾)        Connect mode select       (可以先選Manual, 確認測試沒問題再設定成Connect-on demand)          若要手動測試interface可不可以正確透過PPPoE獲得IP,     至 STATUS &gt; INTERNET STATUS 內的 Connection 選擇對應的界面後,     按下Connect / Disconnect按鈕進行測試    設定IPv6           確認帳戶設定沒問題後, 將新增的interface設定為Defalt Route (參照前面的帳號設定步驟 4.)       (若是直接改裝機人員預設的帳號密碼則不用做這個動作, 因為預設就有勾選這個設定)            原本上網的介面就會被設定為新介面上網, 確認是否有正確從hinet機房獲得到IPv4的位址            進入IPv6設定頁面, 照以下方式設定              IPV6 CONNECTION TYPE                    My IPv6 Interface is : 設定hinet帳戶的介面 (預設就是Default Route)           My IPv6 Connection is : PPPoE (以PPP連線進行驗證並以dual stack方式上網)           IPv6 Enable : 勾選                       IPV6 DNS SETTINGS                    選擇 Use the following IPv6 DNS servers           Primary IPv6 DNS Address : 2001:4860:4860::8888    (Google public IPv6 DNS)           Secondary IPv6 DNS Address : 2001:4860:4860::8844  (Google public IPv6 DNS)                       LAN IPV6 ADDRESS SETTINGS                    Enable DHCP-PD : 勾選                       ADDRESS AUTOCONFIGURATION SETTINGS                    Autoconfiguration Type : SLAAC+Stateless DHCP (也可以使用不同的方式, 建議使用SLAAC+Stateless DHCP, 支援的設備較多)                           設定參考                  &lt;/figure&gt;           按下 Add/Apply            Reboot 重新啟動 DSL-7740C            進入STATUS &gt; IPv6就會顯示相關IPv6資訊了                   IPv6 Status            &lt;/figure&gt;           至test-ipv6.com進行測試                   &lt;/figure&gt;   部份問題      透過Wi-Fi上網的設備有時候可能會發生DHCP無法正確給DNS的狀況,  可能是DSL-7740C firmware bug, 建議手動綁一下DNS就可以解決了.   ","categories": [],
        "tags": ["hinet","ipv6","dlink"],
        "url": "https://easoncao.com/setup-hinet-ipv6-with-DSL-7740C/",
        "teaser": "https://easoncao.com/assets/images/posts/2016/11/hinet-ipv6/ping-facebook-v6.png"
      },{
        "title": "讀大學：回歸本質，找尋自己",
        "excerpt":"                  What is YOU?            (Photo credit: kodomut)   思考？你會思考嗎？   從國小到國中、國中到高中， 師長們其實默默的都為我們瘋狂的洗腦大學多美好的印象， 說著說著，其實默默的心裡自己也信了，   為了進入好學校、好大學，所以努力讀書、追求好成績， 特別是台灣教育最大的特色就是升學考試，把我們自己訓練的跟考試機器一樣。   從小其實我不愛讀書，也很討厭制式考科的教育制度，   但是因為父母、老師的期待，你還是會把該唸的本分給唸完，   寫著一堆不知道哪裡來的教科書、考卷、課堂隨時的小考，   搭配一堆無腦的畫叉否定跟訂正，不知道在訂正什麼，只為了下次不要再錯這題，   甚至為了配合出題者的答案，要去”背”這個形式的答案。   班上總是有那種就是沒辦法讀書的同學，比較愛玩，無法專心讀一大堆教科書，所以成績差不停的被老師勸說。   但是相對的，你只要乖乖讀書，成績都不會太差，大家還會很愛你，甚至把你當神膜拜。   從考試過程中我們不停的猜測、與同學競爭，成績好的同學就會被老師拿來當範本， 不愛唸書的同學就會被貼上沉淪的標籤。   國中，老師跟你說考上好高中就有好前景…   高中，考上好大學就可以玩四年，找工作不用煩惱…   就這樣一路來到大學，才發現事與願違，   當你進到大學， 若有機會教授在課堂上突然要求你自己思考，   漸漸的，你才會意識到其實你已經忘記 怎麼思考 這件事情，   什麼是思考，你會思考嗎？   這問題雖然好像有點像在開玩笑，可是如果你去問問現在普遍大學生，有幾位認真想過這個問題？   從我自己的感受是，他們甚至無法真正了解自己在做什麼。   因為高中前這樣教育制度下的訓練，其實真的大家都被訓練成不必思考，只因為誰誰誰叫我這樣做，所以做到符合標準的要求就好，而不是怎麼做能更好。   甚至不知道唸大學的目的為何，可能只是因為家長老師說要讀大學，所以來拿個學歷，   唸大學的過程中腦袋還會退化，不懂的如何思考問題、思考的方式也沒有邏輯，只是在盲目的鑽牛角尖，   最後課程還是只能淪為：考古題、筆試、標準答案，   反而是教授要求你做Project，特別是Team project，才有可能會意識到自己的不足。   回歸本質   我認為上大學應該是一個非常好的機會找回：真正會思考的自己   進大學還是會發現大家都是為了修課而修，大部分人努力挑了爽過的課程，而不是藉這個機會去學習、訓練自己如何思考問題，   一直沒辦法跳脫自己的舒適圈，我想這就是為什麼在討論一些Team Project時，產生大家都說自己沒想法的最大原因。   大三，這學期我修了一門很特別的課程：法國文化與紙藝應用，   老實說這門課十足震撼，在別人口中這是一門 “硬課” ，班上同學對老師及課程的評價也十分兩極化，   印象深刻的是，剛開學兩星期修課人數，從約40人撤選到學期現在過半，剩下不足20幾人，   這門課程真正重新帶領我學習如何”思考”， 並且從不同的切入點去看待事情及問題， 像是一張紙的顏色、纖維的脈絡、輕盈度、不同材質產生出什麼樣的變化，   甚至是滴一滴水在紙張的擴散程度，都是課堂中你可以關注的問題，   困難的地方在於，全憑你如何用最簡單、最細微的角度觀察出他們的”本質”，   講起來雖然抽象，但其實這就是當我們在解決問題時最缺乏的能力，   常常看問題時我們都會從很宏觀、多角度的方式去看待問題，   但當你從最基礎的地方開始出發，才能逐漸的將細節慢慢擴大。   主體意識（主觀認定）   雖然課程中，只講求從材質最真正的”本質”出發，課程的材料也偏向設計面的培養，   不管是老師從外面邀請的老師(業師)協助課程評論，或是要求我們用課外的時間親訪林業試驗場，   其實額外花很多課堂外的時間，但能堅持修這門課的學生，我想大概也不在乎那兩學分了。   矛盾的是，有時候可能教授們強調的「學分及成績，並不是代表你能力價值的東西」這句話聽起來很有道理，但還是很多人以主體意識的方式，去認定這些東西是比較具有參考價值、並且能夠評估能力的指標之一。   而紙藝這門課程最特別的地方就在於：強調主觀意識的重要性(指的是對於事物認定的想法)   在課堂中評估藝術品及學生任何「行為價值」的這些過程，常常發生老師跟邀請的業師會因為主觀認定的不同，造成評價矛盾的情況，   也讓我們在上臺報告時不知所措，可是老師及業師還是很有風度的願意承認自己的疏失，讓我們了解主觀認定上的不同都是需要被尊重的。   身為理工學生的我，往往會優先用客觀的角度分析事情，自從我修了這門課程，開始會對事情陷入哲學性的思考，   甚至不時會質疑事情的正確性，並且會用主觀意識的方式去切入問題，試著從問題中找出自己的答案，   這個答案，不是標準答案，但也不是錯誤的答案，但是我相信，對我自己一定是一個好答案。   勇於發問、不怕犯錯   也因為這門課程做出了一些小小突破，而近期的最大突破，就是開始學著認識真正的自己，   過去很感謝有機會能跟一些新創公司的負責人談話，談話過程中漸漸讓我意識到其實每個人都有自己的特質，以及自己是這麼的不足，   也很感謝他們讓我了解”發問”這件事情，是需要積極訓練的，   當你遇到問題時，提出問題的動機本來就沒有好跟壞之分，所以不要害怕發問，   這句話不知道有多少大學生能真正實踐（包括我自己）， 透過發問能力的訓練，你才能體悟到其實犯錯真的沒什麼，無知也是正常的事情，   但是如果已經知道自己不懂了，還不透過向他人發問去克服你自己的問題，那實在是太對不起自己了。   從談話中最獲益的一句話就是：   不管你現在能力如何，最重要的是，你如何去突破自己，你是否願意突破自己？   我自己因為擔任系學會幹部、在計網中心工讀的過程，漸漸的從面對人事物的過程中，才開始慢慢建立起自己的自信， 並且學會如何檢討自己，   這些經驗都是磨練自己的契機，過程從開始、當下、結束也有非常多的挑戰跟困難，但是當有機會的時候，一定要期許自己不要因為害怕就不敢做了。   當你將視野放大，你才會意識到自己的渺小與平凡。   只能不停的期許自己學習、學習再學習， 畢竟唯有意識到自己犯錯與缺失，才會有不一樣的成長與改變。   Next … ?   總結：思考 + 發問 = 解決問題   當你開始面臨思考如何克服現下的挑戰時，少不了的就是你發問及突破的勇氣，   不過，問題來了，   先問問自己，思考一下：你願不願意挑戰現在的 你自己？      突破現在的自己，往下個階段的前進吧。   ","categories": [],
        "tags": ["mood"],
        "url": "https://easoncao.com/what-is-you/",
        "teaser": "https://easoncao.com/assets/images/posts/2016/11/what-is-you/you.jpg"
      },{
        "title": "Stay hungry, stay foolish",
        "excerpt":"                  Stay hungry, stay foolish            (Photo credit: rob patrick)   一個人在外唸書第三年了，   2016年即將結束，   每當遇到低潮，總會想起家鄉，甚至不禁質疑自己為什麼要這麼辛苦，   長大了，自己讀書，自己規劃人生，自己學著跌倒站起來，   因為長大任憑叛逆的想法茁壯，心中卻又有一絲想依靠家裡的渴望。   即使這麼辛苦，   回頭翻翻一路成長的一些照片跟紀錄，   總是會意識到自己得不斷長大，   因為長大，所以接觸更深、更廣的世界，   知識更艱澀、學習更無涯，   學習中又會因為自己的無知感到悲憤、始終摸不著頭緒，   意識到自己能力還必須得不斷提昇，基礎知識還要不斷的熟悉、了解，   過程辛苦、迷茫，一無所獲，總期盼自己能有頓悟的那刻，   真希望我也能享受一下因為知識滿足的成就感啊，   但我想既然我這麼貪婪於知識，是不會滿足的，   每每總覺得自己十分不足，最後還是回頭告訴自己，   繼續長大吧。   ","categories": [],
        "tags": ["mood"],
        "url": "https://easoncao.com/stay-hungry-stay-foolish/",
        "teaser": "https://easoncao.com/assets/images/posts/2016/12/stay-hungry-stay-foolish/quote.jpg"
      },{
        "title": "2016 End",
        "excerpt":"                  See you, 2016!            台灣時間，2016年將進入尾聲，   在這個時間點，有很多感觸， 希望能藉由紀錄的方式摘要我這一年的感受，   今年邁入20，也是具備民法全責資格的年齡， 從弟弟、同學，到先生，不僅是稱謂上的不同，更代表著必須長大的殘酷事實，   可以大搖大擺的自己去銀行辦了張信用卡，簽署再也不用父母同意了， 享受刷卡便利的同時也必須開始學著如何支出， 意味著這個年齡開始，是好是壞做任何事情前都必須要思考清楚，   受惡房東氣，換了一個室友， 終於換了自己還滿意的新家，跟著室友一起搬家逃跑，（叫lalamove搬家還蠻便宜的） 自己買了新的鍋具，開始點煮菜技能（雖然還是很不會煮）， 把家裡的線路升到IPv6壓壓驚，   當幹部、奉獻青春、接下活動總召， 還記得當時活動前累到每天都睡沙發， 早上被嚇醒、晚上累到半夜再回家，或是洗個澡就又回學校，就怕漏掉每個細節（雖然現在也好像常睡客廳….咦！？）   今年也扛起迎新的任務，自己學著怎麼帶領團隊，在茫然中摸索， 歷經很多挫折跟難過，以及厭煩，厭煩總是做不好的無力感、沒有任何動力想前進跟繼續當幹部的念頭。   做一些我自己都難以理解的行為，因為我本來就不是會去主動帶人的角色， 也沒什麼社交能力，卻還是冒著生命危險硬把團隊炒熱絡，   對於團隊風氣不抱任何期待、任憑團隊意志被擊潰， 過程中還是得不斷的告訴自己撐下去、要把大家帶好帶滿， 所以再累再不願意，也是時時提醒自己，因為這個身份，我需要做的更多，   活動結束後其實很多感觸，但也非常疲乏，   一是今年天氣比較不給面子，辛苦辦的活動就這樣砍了。 二是很遺憾今年帶給我的感觸不比我前幾年來的深刻，可能因為我換了個位子也換了個腦袋吧。 三是活動辦完才發現，自己已經不小了，腦袋一直浮出「好像該做點什麼其他事情了」的想法。   其實很驚訝活動結束後竟然會有人願意給我這個邊緣人寫背卡（痛哭流涕了），   雖然活動結束後各自鳥獸散， 我就是比較懶，現在才感謝好像也有點晚，但感謝終究不嫌晚， 很慶幸2016有這群人陪著我一起成長，更高興的是，看見你們的成長，   自完成階段性的任務， 今年最大的收穫就是學習接納失敗，學著如何吸取教訓，適應變化， 還有練就如何面對千奇百怪的人、莫名其妙的通識課堂作業、期末如何用指數成長的方式爆肝， 期末每天跟deadline賽跑，發現自己真的很嫩，沒有那麼多能力（三下一學期做3、4個project，今年真的很特別）。   今年學習曲線也開始慢慢亂走，從學著怎麼寫測試， 開始放下自己code寫很好的成見，逼自己面對自己寫的爛碼， 又弄一堆雜七雜八的東西，做一些自己也不知道是不是正確的決定，   因為當沒有人幫你決定的時候，大部分沒有任何方向的人，我想都會跟我一樣茫然的，   也因為自從修了某紙藝課，有越來越感覺「不知道自己為什麼在這」、發現自己其實沒有對未來有太多規劃， 以及慢慢的開始發現自己跟別人的不同，   開始面對自己在很多方面確實比別人差，是一件很難接受的事情，老實說我也很難接受，   最重要的是，要找出自己最特別、最厲害的地方，了解自己的想法，自己真正渴望的東西， 因為盲目的追求新東西實在是太累人了，做得也並不一定比其他人得心應手，   我想這就是我2016最大的感觸了， 這一年要感謝的人太多，就感謝2016吧， 2017繼續加油，身為邊緣人的我要繼續在家陪我的project跨年了。  ","categories": [],
        "tags": ["mood"],
        "url": "https://easoncao.com/2016-end/",
        "teaser": "https://easoncao.com/assets/images/posts/2016/12/2016-end/see-you-2016.png"
      },{
        "title": "在 PHP FastCGI 環境下自訂使用者的 php.ini 設定",
        "excerpt":"         前言   因為自己興趣使然有在玩 VPS，雖然不排斥指令界面，   但有時候會沒睡飽不小心下錯指令，也有時候避免不了各設定檔互向關聯，當腦袋還沒清醒時去動 Production 的機器簡直超級悲劇，   因為玩玩的機器，沒打算買 cPanel 授權，所以我過去一直使用 Kloxo 當作我的 Panel，除了能更直覺的了解現在的設定外也能避免一些設定上的慘案。   自從歷經幾次的 Kloxo 漏洞被打實錄後，有了一些慘痛的教訓，   而且官方團隊也在 6.1.19 版本後正式宣告不維護，所以就跳槽到現在的 webmin + virtualmin 環境一陣子了。   Virtualmin 出乎我意外的好用，而且設定相比 Kloxo 也直覺多了，   從 Kloxo 到現在在用 Virtualmin 的過程中，不免會對設定很好奇，所以花了一點時間研究設定檔，紀錄一下。   環境   Server version: Apache/2.4.6 (CentOS) PHP: PHP 5.4.16 (Default) / PHP 5.5.21 / PHP 5.6.5 Panel: Webmin (1.831) + Virtualmin (5.05)   (關於多個 php 版本的設定有機會再來補個文章，可詳見Red Hat Software Collections)   設定   Apache (http)   &lt;Directory /home/user/public_html&gt;     Options -Indexes +IncludesNOEXEC +SymLinksIfOwnerMatch +ExecCGI     allow from all     AllowOverride All Options=ExecCGI,Includes,IncludesNOEXEC,Indexes,MultiViews,SymLinksIfOwnerMatch     Require all granted     AddType application/x-httpd-php .php     AddHandler fcgid-script .php     AddHandler fcgid-script .php5     AddHandler fcgid-script .php5.5     AddHandler fcgid-script .php5.6     FCGIWrapper /home/user/fcgi-bin/php5.5.fcgi .php     FCGIWrapper /home/user/fcgi-bin/php5.fcgi .php5     FCGIWrapper /home/user/fcgi-bin/php5.5.fcgi .php5.5     FCGIWrapper /home/user/fcgi-bin/php5.6.fcgi .php5.6 &lt;/Directory&gt;   新增 wrapper script /home/user/fcgi-bin/php5.fcgi   #!/bin/bash PHPRC=$PWD/../etc/php5 export PHPRC umask 022 export PHP_FCGI_CHILDREN PHP_FCGI_MAX_REQUESTS=99999 export PHP_FCGI_MAX_REQUESTS SCRIPT_FILENAME=$PATH_TRANSLATED export SCRIPT_FILENAME exec /bin/php-cgi   複製 php.ini 至 /home/user/etc/php5   cd /home/user/etc/php5 cp -a /etc/php.ini . chown user:user php.ini   確認FastCGI wrapper script權限是否設定正確   chown user:user php5.fcgi &amp;&amp; chmod 0755 php5.fcgi   重新啟動Apache   service apache restart   參考資料：     Using Custom php.ini with PHP5 under FastCGI  ","categories": [],
        "tags": ["php","apache"],
        "url": "https://easoncao.com/php-fastcgi-config-with-custom-php-ini/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/01/php-fastcgi-config-with-custom-php-ini/setting.png"
      },{
        "title": "Apache / PHP 上傳大檔的注意事項",
        "excerpt":"         現在頻寬越來越大，ISP 提供上傳跟下載的費用也越來越便宜，   以目前大眾常使用的 100Mbps/40Mbps 家用頻寬一次傳個 300MB 的大檔也稀鬆平常。   但是像 Apache 2.4.18 及 php 預設設定仍然還卡在 20M 甚至 8M 內，   有時候應用程式需要傳大檔時這些設定不免成為瓶頸，   並不是程式有 bug ，很可能是設定上需要做一些調整，   所以自己整理一些設定的經驗作為參考，如果未來在做 trubleshooting 時可以朝以下方向著手。   追蹤問題   通常可以透過 Debug 錯誤訊息，/var/log/apache, /var/log/apache2/, /var/log/httpd 等等找到相關的錯誤及紀錄。   當問題比較難追蹤，可以試著多模擬幾次同樣的出錯流程。藉由重複產生問題，能在紀錄中找到比較明顯的表徵，並且進一步除錯。   PHP   php.ini : 預設 php 設定檔內可以檢查以下設定   upload_max_filesize 10M post_max_size 15M max_input_time 300 max_execution_time 300      upload_max_filesize : 限制最大上傳檔案大小   post_max_size : 這邊指的是 POST method 封包允許的大小，很多設定都設跟 upload_max_filesize 一樣，                   但照官方的建議應該是要設得比 upload_max_filesize 來得大，                   而一般來說 memory_limit 應該也必須比 post_max_size 大，才能分配足夠的記憶體空間給 POST 方法使用的變數。   max_input_time : 最大的上傳允許時間。   max_execution_time : PHP script 正常呼叫時允許的最大執行時間。通常上傳大檔時，因為傳輸頻寬影響，會導致接收檔案的 PHP script 執行時間過長。                        若太短會讓伺服器中斷仍在執行傳輸的作業，使傳輸連接中斷。   如果是 shared host，主機商沒有提供更動設定檔的權限，也可以試著透過 .htaccess 檔案方式直接設定。   (注意必須要 Apache 內環境設定啟用 AllowOverride All 的設定才會生效，若不確定建議直接聯絡主機商確認。)   .htaccess 設定如下   php_value upload_max_filesize 10M php_value post_max_size 10M php_value max_input_time 300 php_value max_execution_time 300   Apache   除了 PHP 本身的設定外，也要注意是不是 apache 本身的設定造成傳輸瓶頸。   遇到 apache mod_fcgid: HTTP request length xxxxx (so far) exceeds MaxRequestLen (xxxxx)   因為傳輸大檔超過 apache 允許的最大請求長度，所以 apache 會吐這個問題。      /etc/apache2/mods-available/fcgid.conf (ubuntu / debian)   在設定檔中加入或調整 MaxRequestLen 參數即可   MaxRequestLen 15728640   如果環境不同，注意設定檔案通常在以下地方：     /etc/apache2/conf-enabled (ubuntu / debian)            注意是不是有主要的 apache2.conf 會 include /etc/apache2/conf-available 底下的設定， 如果沒意外應該會 link 到 /etc/apache2/conf-available 底下。           /etc/httpd/conf/httpd.conf (RedHat / CentOS)            一樣要注意是不是有 /etc/httpd/conf.d/* 之類的設定檔， 通常需要檢查一下主要的設定檔看是不是有 Include 關鍵字再進一步去檢查。 (推薦使用grep -ri 'Include'指令)           其他關於大檔案上傳的可以參考 ownCloud 官方文件 Uploading big files &gt; 512MB 一節，   裡面包含了部份 nginx 的設定，十分值得參考。   ","categories": [],
        "tags": ["php","apache"],
        "url": "https://easoncao.com/apache-and-php-upload-large-file/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/01/apache-and-php-upload-large-file/box.png"
      },{
        "title": "讓 git 不必 commit 而暫存特定檔案的修改",
        "excerpt":"         git 最常用的指令應該就屬 commit 了，   然而修改特定檔案後 (untrack)，想要回到修改前的狀態 (HEAD)，卻又想暫存現在的修改，   我們並不想要真正 commit ，因為如果只是一點試驗性的修改，就又多一筆 commit 會感覺有點多餘。   這時候常見的作法是 stash ，便能暫存目前的修改。   git stash   要回復只要使用 pop 即可   git stash pop   但是這不是本篇文章的重點，如果一次更改多處程式，只想暫存特定檔案的修改，   最簡單的方式可以利用 diff 達成。   git diff app.c &gt; stashed.diff git checkout app.c   這樣會多出一個 stashed.diff 檔案，利用 checkout 就能將修改倒帶回去，只要使用 apply 就能回復修改。   git apply stashed.diff   另一種作法則是將要保存修改的檔案全部 git add 一遍，透過 --keep-index 也能達到同樣暫存特定檔案修改的效果。   touch FileNotStashed.txt stashed.txt git add FileNotStashed.txt git stash --keep-index   參考資料：     Stash only one file out of multiple files that have changed with Git?  ","categories": [],
        "tags": ["git"],
        "url": "https://easoncao.com/git-stash-only-one-file/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/01/git-stash-only-one-file/git.png"
      },{
        "title": "jQuery Selector 使用逗點之間的差異",
        "excerpt":"                  jQuery! write less, do more.            雖然好像有點舊飯重炒了，現在前端也打的這麼火熱，   這些紀錄出來的時機好像有點短，不過接觸 jQuery 也蠻常一段時間了，   好像沒什麼注意到有人區別逗號 “,” 這個很特別的存在，也常常是讓初學者混淆摸不著頭緒的一個符號，   jQuery selector 一般常見的寫法都是   $('button')   如果是選擇 class name 會利用 “.” 點符號表示， id 則是使用 “#” 井字符號表示，跟 css 的 selector 用法一樣。   $('.myClass') $('#id')   而逗點如果放在 ‘’ (單引號) 或 “” (雙引號) 間 (兩者在 jQuery 中其實沒有太大區別)， 代表的是 multiple selector，會同時選擇兩個以上的元素 (element)。   $('button, div')   上述程式同時選擇了 DOM 裡的 button 及 div。   但要注意的是，如果 “,” 逗號放置在 ‘’ (單引號) 或 “” (雙引號) 外，就變成有點像 function(param1, param2) 的概念，成兩個實體參數，   寫法為：   jQuery( selector [, context ] )   context 可以是 DOM element、Document 或是 jQuery Object，   這樣 jQuery 會縮小選擇範圍，以第二個參數作為選擇對象縮小搜尋，像以下範例。   $('div').on('click', function(e) {     $('p', this).addClass('active'); });   也就是說，下段程式碼，均有同等的效果。      Example 1     var $container = $('div'); $('button', $container)           Example 2     $('div button')           Example 3     $('div').find('button')           Example 4     var $container = $('div'); $container.find('button')           參考資料：     jQuery()   Multiple Selector (“selector1, selector2, selectorN”)  ","categories": [],
        "tags": ["jQuery","javascript"],
        "url": "https://easoncao.com/jquery-selector-using-comma/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/01/jquery-selector-using-comma/jquery.png"
      },{
        "title": "讓PHPUnit測試單一個程式",
        "excerpt":"                  PHPUnit            在跑單元測試總有遇到亮紅燈的時候，健全一點的測試幾十個 assertion 都很正常，   但是如果 phpunit 只有幾個測試沒過全部重測也很浪費時間，所以稍微找了一下怎麼只測特定的 method 。   只測一個單元測試檔案   phpunit tests/testone.php   只跑 test/testone.php 裡的 testMethod()   phpunit --filter testMethod tests/testone.php   --filter 也支援 Namespace / Class name   phpunit --filter 'TestNamespace\\\\TestCaseClass::testMethod' phpunit --filter 'TestNamespace\\\\TestCaseClass' phpunit --filter TestNamespace phpunit --filter TestCaseClass   正規表達也沒問題   phpunit --filter '/::testMethod .*\"my named data\"/'   也有快速便捷的測法   phpunit --filter 'testMethod#2' phpunit --filter 'testMethod#2-4'   其他用法詳見：      PHPUnit 官方文件 Filter pattern  ","categories": [],
        "tags": ["phpunit","unit-test","php"],
        "url": "https://easoncao.com/let-phpunit-test-only-one-function/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/01/let-phpunit-test-only-one-function/phpunit.png"
      },{
        "title": "Laravel 5 自訂分頁 (Pagination) 樣式",
        "excerpt":"Laravel 5.3   Laravel 5.3 開始官方將這部份重寫成 method，可以利用 links('view.name') 完成自訂分頁。   {{ $paginator-&gt;links('view.name') }}   透過 vendor:publish 指令很快的就能在 resources/views/vendor 產生對應的 Blade 。   php artisan vendor:publish --tag=laravel-pagination   詳見 Laravel 5.3 Customizing The Pagination View   Laravel &lt;= 5.2   Laravel 5.2 以下的版本可以利用實做 PresenterContract 類別達成，   重寫 render, getAvailablePageWrapper, getDisabledTextWrapper, getActivePageWrapper methods 的樣式。   新增 App\\Pagination\\CustomPresenter.php ， 範例程式如下。   &lt;?php  namespace App\\Pagination;  use Illuminate\\Contracts\\Pagination\\Paginator as PaginatorContract; use Illuminate\\Contracts\\Pagination\\Presenter as PresenterContract; use Illuminate\\Pagination\\BootstrapThreeNextPreviousButtonRendererTrait; use Illuminate\\Pagination\\UrlWindow; use Illuminate\\Pagination\\UrlWindowPresenterTrait;  class CustomPresenter implements PresenterContract {     use BootstrapThreeNextPreviousButtonRendererTrait, UrlWindowPresenterTrait;      /**      * The paginator implementation.      *      * @var \\Illuminate\\Contracts\\Pagination\\Paginator      */     protected $paginator;      /**      * The URL window data structure.      *      * @var array      */     protected $window;      /**      * Create a new Bootstrap presenter instance.      *      * @param  \\Illuminate\\Contracts\\Pagination\\Paginator  $paginator      * @param  \\Illuminate\\Pagination\\UrlWindow|null  $window      * @return void      */     public function __construct(PaginatorContract $paginator, UrlWindow $window = null)     {         $this-&gt;paginator = $paginator;         $this-&gt;window = is_null($window) ? UrlWindow::make($paginator) : $window-&gt;get();     }      /**      * Determine if the underlying paginator being presented has pages to show.      *      * @return bool      */     public function hasPages()     {         return $this-&gt;paginator-&gt;hasPages();     }      /**      * Convert the URL window into Bootstrap HTML.      *      * @return string      */     public function render()     {         if ($this-&gt;hasPages()) {             return sprintf(                 '&lt;ul class=\"article-pages db\"&gt;' .                 '&lt;div class=\"head\"&gt;%s&lt;/div&gt;' .                 '&lt;div class=\"tail\"&gt;%s&lt;/div&gt;' .                 '&lt;div class=\"pages\"&gt;%s&lt;/div&gt;' .                 '&lt;/ul&gt;',                 $this-&gt;getPreviousButton(),                 $this-&gt;getNextButton(),                 $this-&gt;getLinks()             );         }          return '';     }      /**      * Get HTML wrapper for an available page link.      *      * @param  string  $url      * @param  int  $page      * @param  string|null  $rel      * @return string      */     protected function getAvailablePageWrapper($url, $page, $rel = null)     {         $rel = is_null($rel) ? '' : ' rel=\"'.$rel.'\"';          return '&lt;li class=\"pg\"&gt;&lt;a href=\"'.htmlentities($url).'\"'.$rel.'&gt;'.$page.'&lt;/a&gt;&lt;/li&gt;';     }      /**      * Get HTML wrapper for disabled text.      *      * @param  string  $text      * @return string      */     protected function getDisabledTextWrapper($text)     {         return '&lt;li class=\"pg disabled\"&gt;&lt;span&gt;'.$text.'&lt;/span&gt;&lt;/li&gt;';     }      /**      * Get HTML wrapper for active text.      *      * @param  string  $text      * @return string      */     protected function getActivePageWrapper($text)     {         return '&lt;li class=\"pg now\"&gt;&lt;span&gt;'.$text.'&lt;/span&gt;&lt;/li&gt;';     }      /**      * Get a pagination \"dot\" element.      *      * @return string      */     protected function getDots()     {         return $this-&gt;getDisabledTextWrapper('...');     }      /**      * Get the current page from the paginator.      *      * @return int      */     protected function currentPage()     {         return $this-&gt;paginator-&gt;currentPage();     }      /**      * Get the last page from the paginator.      *      * @return int      */     protected function lastPage()     {         return $this-&gt;paginator-&gt;lastPage();     } }   在 Blade 內利用 with() 配合 paginate 物件即可達成，如以下程式碼。   &lt;ul&gt; @foreach ($items as $item)   &lt;li class=\"item\"&gt;$item&lt;/li&gt; @endforeach &lt;/ul&gt;  {!! with(new \\App\\Pagination\\CustomPresenter($items))-&gt;render() !!}   ","categories": [],
        "tags": ["php","laravel"],
        "url": "https://easoncao.com/custom-pagination-in-laravel-5/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/02/laravel-5.4-work-with-vue-notice/laravel.png"
      },{
        "title": "在 Laravel 中簡易的擴充 Blade Helper function",
        "excerpt":"在 App/Providers/AppServiceProvider.php 檔案內 register() 新增 loader 程式碼。   &lt;?php  namespace App\\Providers;  use Illuminate\\Support\\ServiceProvider;  class AppServiceProvider extends ServiceProvider {     /**      * Bootstrap any application services.      *      * @return void      */     public function boot()     {         //     }      /**      * Register any application services.      *      * @return void      */     public function register()     {         // Reigister each helper files         foreach (glob(app_path().'/Helpers/*.php') as $filename){             require_once($filename);         }      } }   搜尋 config/app.php 中的 providers 陣列，確認有正確載入 AppServiceProvider。   'providers' =&gt; [      /*      * Application Service Providers...      */     App\\Providers\\AppServiceProvider::class,  ]   在 app/Helpers/ 內新增自訂的 helper function 即可。(注意不要與官方提供的 helper function 衝突)   Helper function 可以很方便的用來像自訂時間顯示格式   app\\Helpers\\TimeElapsedString.php   &lt;?php  /**  * Get time eplapsed string (4 months, 2 weeks, 3 days, 1 hour, 49 minutes, 15 seconds ago)  *  * Example:  *     echo time_elapsed_string('2013-05-01 00:22:35');  *     echo time_elapsed_string('@1367367755'); # timestamp input  *     echo time_elapsed_string('2013-05-01 00:22:35', true);  // get full string  *  * @param  Datetime  $datetime  * @param  boolean   $full  * @return string  */ function time_elapsed_string($datetime, $full = false) {     $now = new DateTime;     $ago = new DateTime($datetime);     $diff = $now-&gt;diff($ago);      $diff-&gt;w = floor($diff-&gt;d / 7);     $diff-&gt;d -= $diff-&gt;w * 7;      $string = array(         'y' =&gt; 'year',         'm' =&gt; 'month',         'w' =&gt; 'week',         'd' =&gt; 'day',         'h' =&gt; 'hour',         'i' =&gt; 'minute',         's' =&gt; 'second',     );     foreach ($string as $k =&gt; &amp;$v) {         if ($diff-&gt;$k) {             $v = $diff-&gt;$k . ' ' . $v . ($diff-&gt;$k &gt; 1 ? 's' : '');         } else {             unset($string[$k]);         }     }      if (!$full) $string = array_slice($string, 0, 1);     return $string ? implode(', ', $string) . ' ago' : 'just now'; }   在 Blade 內只要使用   {{ time_elapsed_string($time) }}   即可很容易的顯示出自訂的時間格式。   當然，利用寫為 Service 能夠將容易重用的 method 自訂為 Helper function。   以下是用於自訂 Response json 格式的 Helper function 範例   app\\Helpers\\Response.php   &lt;?php  /**  * Response messages json format  *  * @param  integer  $code  * @param  string   $messages  * @param  array    $extend  * @return \\Illuminate\\Http\\Response  */ function responseJSON($code, $messages, $extend = []) {     $data = [         'code'  =&gt;  $code,         'messages'  =&gt;  $messages,     ];      return response()-&gt;json(array_merge($data, $extend)); }   另一種作法則是透過 composer.json 設定 autoload 載入自訂的 Helper，   缺點則是較為缺乏彈性，若有多個 Helper 需要新增較不易管理及修改。   在 composer.json 中找到 autoload 選項後新增   \"autoload\": {     \"files\": [         \"app/BladeHelper.php\"     ] },   並且新增 app/BladeHelper.php 檔案，在裡面新增自訂的 Helper function 即可。   ","categories": [],
        "tags": ["php","laravel"],
        "url": "https://easoncao.com/extend-blade-helper-function-in-laravel/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/02/laravel-5.4-work-with-vue-notice/laravel.png"
      },{
        "title": "Laravel 5.4 版使用官方內建 Vue.js 入門開發的一些注意事項",
        "excerpt":"         Laravel 5.4 最明顯由原本的預設自動化整合由 gulp 更換為 webpack，   同時 Laravel Elixir 也換成 Laravel Mix，   並且也整合 Vue.js 至官方預設的 Component，   如果不用官方 VM 開發的話過程中真的蠻容易踩不少雷。   安裝   安裝過程不詳細贅述，詳見官方文件。   laravel new app npm install   執行 npm run dev 遇到錯誤   ERROR  Failed to compile with 5 errors  These dependencies were not found in node_modules:  * ../fonts/bootstrap/glyphicons-halflings-regular.eot * * ../fonts/bootstrap/glyphicons-halflings-regular.woff2 * * ../fonts/bootstrap/glyphicons-halflings-regular.woff * * ../fonts/bootstrap/glyphicons-halflings-regular.ttf * * ../fonts/bootstrap/glyphicons-halflings-regular.svg   主要是 bootstrap-scss load glyphicons fonts 發生路徑問題，   如果翻一下 node_modules/bootstrap-sass/assets/stylesheets/bootstrap/_variables.scss ，   在 81 行開始可以找到這邊，   // [converter] If $bootstrap-sass-asset-helper if used, provide path relative to the assets load path. // [converter] This is because some asset helpers, such as Sprockets, do not work with file-relative paths. $icon-font-path: if($bootstrap-sass-asset-helper, \"bootstrap/\", \"../fonts/bootstrap/\") !default;   主要是 $icon-font-path 沒辦法正確的指定 bootstrap 正確的路徑，這個問題有三個解法，           啟用 bootstrap-scss 內建的修正 function (推薦)       可以直接開啟 resources/assets/sass/_variables.scss 後新增       bootstrap-sass-asset-helper: true;           給予絕對路徑       開啟 resources/assets/sass/_variables.scss 後新增一行指定 glyphicons 的絕對路徑       $icon-font-path: /path/of/app/node_modules/bootstrap-sass/assets/fonts/bootstrap/           取消整合 bootstrap       開啟 webpack.mix.js 後，將 .sass('resources/assets/sass/app.scss', 'public/css'); 刪除       const { mix } = require('laravel-mix');  /* |-------------------------------------------------------------------------- | Mix Asset Management |-------------------------------------------------------------------------- | | Mix provides a clean, fluent API for defining some Webpack build steps | for your Laravel application. By default, we are compiling the Sass | file for the application as well as bundling up all the JS files. | */  mix.js('resources/assets/js/app.js', 'public/js')    .sass('resources/assets/sass/app.scss', 'public/css');   用範例的 Vue components 動不了   出現 Uncaught TypeError: Cannot read property 'csrfToken' of undefined   在 app.js (bootstrap.js) 被載入前預先在 Blade 內定義以下程式即可解決。   &lt;!-- Scripts --&gt; &lt;script&gt; window.Laravel = &lt;?php echo json_encode([ 'csrfToken' =&gt; csrf_token() ]); ?&gt;  &lt;/script&gt;   Blade 的程式碼大致上如下   &lt;div id=\"app\"&gt;     &lt;example&gt;&lt;/example&gt; &lt;/div&gt;   &lt;!-- Scripts --&gt; &lt;script&gt; window.Laravel = &lt;?php echo json_encode([ 'csrfToken' =&gt; csrf_token() ]); ?&gt; &lt;/script&gt;  &lt;script src=\"{{ asset('js/app.js') }}\"&gt;&lt;/script&gt;   問題在於如果沒有跑過 php artisan make:auth 的話，   在 Laravel 5.4 預設 resources/assets/js/bootstrap.js 內中有使用到 axios ，   其中注意 'X-CSRF-TOKEN': window.Laravel.csrfToken。   22 /** 23  * We'll load the axios HTTP library which allows us to easily issue requests 24  * to our Laravel back-end. This library automatically handles sending the 25  * CSRF token as a header based on the value of the \"XSRF\" token cookie. 26  */ 27 28 window.axios = require('axios'); 29 30 window.axios.defaults.headers.common = { 31     'X-CSRF-TOKEN': window.Laravel.csrfToken, 32     'X-Requested-With': 'XMLHttpRequest' 33 };   window.Laravel 則是是透過使用 make:auth 才會產生出的，   並且定義在 src/Illuminate/Auth/Console/stubs/make/views/layouts/app.stub 內，   引述官方文件   To use the component in your application, you may simply drop it into one of your HTML templates. For example, after running the `make:auth` Artisan command to scaffold your application's authentication and registration screens, you could drop the component into the home.blade.php Blade template   詳細的程式碼可以參考github commit   ","categories": [],
        "tags": ["php","laravel","vuejs"],
        "url": "https://easoncao.com/laravel-5.4-work-with-vue-notice/",
        "teaser": "https://easoncao.com/assets/images/posts/2017/02/laravel-5.4-work-with-vue-notice/laravel.png"
      },{
        "title": "在 Linux 底下刪除以連接符號 - (dash) 開頭的檔案",
        "excerpt":"在 Linux 使用 CLI 刪除檔案直覺想到的就是 rm 指令，   是非常高頻率使用的程式，像刪除一般檔案的指令   rm yourfile   Linux 預裝程式多數提供許多參數選項，只要加上 - 符號 (英文為dash) 以及對應的選項就能發揮對應的功能，   像是 -r 可以讓 rm 這隻程式 recursive，-f 代表 force，   當放一起成為 -rf 參數時可以刪除整個目錄，但使用不慎很可能會發生難以想像的破壞，   rm -rf /myFolder   但如果檔案開頭是以 - 符號開始呢？例如(-sample.txt)   刪除檔案這種乍看簡單的問題就瞬間變很難了，一般直覺的作法是這樣，   rm -sample   但是這樣就會被 rm 認為是加上 -sample 選項，但 rm 並沒有提供對應的功能，而不是以檔名解析，   所以你可能會想到利用 \\ 跳脫，   rm \\-sample   其實這有點像是換行而已，但很遺憾的是這樣也沒辦法讓 rm 正常解析為檔名，   正確作法是加上兩個 dash 符號 --   rm -- -sample   這樣就能正常刪除了，   或是以這種方式也能夠刪除 (給予絕對路徑)   rm ./-sample  ","categories": [],
        "tags": ["Linux"],
        "url": "https://easoncao.com/remove-filename-start-with-dash-in-linux/",
        "teaser": null
      },{
        "title": "D-Link DSL-7740C 啟用 SNMP",
        "excerpt":"一般光纖 100M/40M 家用網路的使用者若想要進行網路流量、路由器監控，   可以在中華電信的 DSL-7740C 啟用 SNMP Agent 達到監控的目的。   首先進入 DSL-7740C 設定畫面，相關帳號密碼可詳閱 上一篇的設定，           進入 Advanced &gt; SNMP 頁面       將 SNMP Agent 設為 enable，並自行選擇是否要啟用 SNMP Trap 。                         DSL-7740C SNMP setting page                    注意 MAINTENANCE &gt; ACL 頁面       Service settings 底下的 Service，找到 SNMP 將 LAN 一欄必須勾選 enable 才能在區域網路連線中使用 SNMP 協定       (恰巧用 nmap 發現預設 ACL 會將 SNMP 過濾，踩了很久的雷)                         DSL-7740C ACL setting page            測試是否正確設定：   以下範例為使用 snmpwalk 進行尋訪 192.168.1.1, version 2c, community 為 public。   snmpwalk -c public -v 2c 192.168.1.1   執行結果：            ","categories": [],
        "tags": ["dlink","hinet"],
        "url": "https://easoncao.com/enable-snmp-agent-on-dlink-dsl-7740c/",
        "teaser": null
      },{
        "title": "從 vim pathogen 無痛轉移到 vundle",
        "excerpt":"取得 Vundle   git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim   在 .vimrc 內啟用 vundle 及 要安裝的Plugins   \" be iMproved, required set nocompatible  \" required filetype off  \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin()  \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here')  \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim'   相關範例可以參考我的vim設定檔得到更多資訊   安裝 Plugins   打開 vim 後運行 :PluginInstall   Reference:      我的vim設定檔   Vundle   Swapping Pathogen for Vundle   ","categories": [],
        "tags": ["vim"],
        "url": "https://easoncao.com/migrate-vim-pathogent-to-vundle/",
        "teaser": null
      },{
        "title": "SVM Quickstart",
        "excerpt":"Reference:      piaip 的 (lib)SVM 簡易入門   Support Vector Machine 簡介   ","categories": [],
        "tags": ["svm","machine-learning"],
        "url": "https://easoncao.com/svm-quickstart/",
        "teaser": null
      },{
        "title": "/etc/fstab file Quick view",
        "excerpt":"What is /etc/fstab      The fstab(5) file can be used to define how disk partitions, various other block devices, or remote filesystems should be mounted into the filesystem.    /etc/fstab 檔案是Linux kernel在開機過程能夠得知有哪些裝置必須被掛載 (mount) 的重要系統檔案。   檔案格式如下：   # &lt;device&gt;             &lt;dir&gt;         &lt;type&gt;    &lt;options&gt;             &lt;dump&gt; &lt;fsck&gt; /dev/sda1              /             ext4      defaults,noatime      0      1 /dev/sda2              none          swap      defaults              0      0 /dev/sda3              /home         ext4      defaults,noatime      0      2      &lt;device&gt;: 要被掛載的實體裝置，或是遠端的檔案系統。   &lt;dir&gt;: 要掛載的目錄，必須是先建立(mkdir)後才能正確掛載。   &lt;type&gt;: 檔案系統格式，如ext4、ext3、xfs、fat32等等。   &lt;options&gt;: 掛載選項，一般檔案系統在掛載時至少包含一種掛載選項(ro or rw)，ro 就是 read-only 僅讀取， rw 就是 read-write 可讀可寫，            defaults: use default options: rw, suid, dev, exec, auto, nouser, and async.       noauto: do not mount when “mount -a” is given (e.g., at boot time)       user:   allow a user to mount       owner:  allow device owner to mount       nofail: do not report errors for this device if it does not exist.       comment or x- for use by fstab-maintaining programs           UUID   一般檔案系統在完成格式化後會產生一組識別碼，稱為UUID (Universally Unique Identifier)，透過UUID掛載的方式能避免裝置因為更換順序(e.g. 更換硬碟的SATA插線順序)，造成原本掛載的設定因為裝置順序不同，產生跟原本預期設定不同的錯誤。   可以使用 `lsblk -f` 或是 `file -s &lt;device&gt;` 輕鬆查到裝置的UUID。   $ lsblk -f  NAME   FSTYPE   LABEL      UUID                                 MOUNTPOINT sda ├─sda1 vfat     ESP        3A32-950C                            /boot/efi ├─sda2 vfat     DIAGS      BCA2-19D3 ├─sda3 ├─sda4 ntfs     WINRETOOLS ECFAA35AFAA31FB6 ├─sda5 ntfs                C03E97DA3E97C7B4 ├─sda6 ntfs                B03C63223C62E2B8 ├─sda7 ntfs                DEC05B4FC05B2D51 ├─sda8 ext4                c3b19071-7163-41ea-9a8c-4069deb13649 / └─sda9 ntfs     PBR Image  D052AA0C52A9F6FE   $ sudo file -s /dev/sda8 /dev/sda8: Linux rev 1.0 ext4 filesystem data, UUID=c3b19071-7163-41ea-9a8c-4069deb13649 (needs journal recovery) (extents) (large files) (huge files)   掛載方式也很簡單，在 /etc/fstab 內使用 UUID= 即可。   # &lt;device&gt;                                &lt;dir&gt; &lt;type&gt; &lt;options&gt;                         &lt;dump&gt; &lt;fsck&gt; UUID=0a3407de-014b-458b-b5c1-848e92a327a3 /     ext4   rw,relatime,discard,data=ordered   0      1 UUID=b411dc99-f0a0-4c87-9e05-184977be8539 /home ext4   rw,relatime,discard,data=ordered   0      2 UUID=f9fe0b69-a280-415d-a03a-a32752370dee none  swap   defaults                           0      0   Label   如果裝置在格式化時有設定 Label，也能使用 LABEL= 進行掛載。   # &lt;device&gt;      &lt;dir&gt; &lt;type&gt; &lt;options&gt;                                                                                            &lt;dump&gt; &lt;fsck&gt; LABEL=EFI       /boot vfat   rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro 0      2 LABEL=SYSTEM    /     ext4   rw,relatime,discard,data=ordered                                                                     0      1 LABEL=DATA      /home ext4   rw,relatime,discard,data=ordered                                                                     0      2 LABEL=SWAP      none  swap   defaults                                                                                             0      0   Reference:      archlinux - fstab   fstab(5)   UUID   ","categories": [],
        "tags": ["linux"],
        "url": "https://easoncao.com/fstab-quick-view/",
        "teaser": null
      },{
        "title": "解析 .well-known 資料夾",
        "excerpt":"What is /.well-known/ ?   繼 Let’s Encrypt 提供公眾能夠使用免費SSL簽章服務，在安裝過程發現會產生 .well-known 資料夾，於是好奇之下就展開了解答尋找之路。   .well-known 目錄正式在 RFC5785 被提出，   隨著 WWW 的 Protocol 越來越多，   若 Server 與 Client 雙方需要進行資訊交換時通常必須建立在 TCP / IP 的基礎上進行，常見的作法像是 HTTPS 以 443 Port 進行資訊上的交換、Handshake等，   即使是 HTTP / HTTPS Protocol ，共同定義的 Header 無法提供更足夠的資訊滿足不同 Client-side 的需求，   更何況網際網路的傳輸中，一個 Packet 長度基本上是有限的，單純的 Server-side application 越來越無法現今網際網路的環境，   Client 要足以得知 Server side 的 metadata ，便共同定義了 .well-known 是被許可的子目錄及 URI。   常見的例子像是：      /.well-known/acme-challenge/ : Let’s Encrypt 用於驗證憑證所有者的有效性，ACME 指的是 Automatic Certificate Management Environment   /.well-known/assetlinks.json : Digital Asset Links，像是用於網站告知 Android 系統應該用什麼樣的 app 開啟   /.well-known/apple-app-site-association : Universal Links，類似 Digital Asset Links，主要用於 iOS 識別使用的 metadata 。   Reference:      For what is the “.well-known”-folder?   The “.well-known” directory on webservers (aka: RFC 5785)   ","categories": [],
        "tags": ["networking"],
        "url": "https://easoncao.com/what-is-well-known-folder/",
        "teaser": null
      },{
        "title": "AWS Certified Solution Architect Associate 認證考試準備及心得",
        "excerpt":"                  AWS Solutions Architect Associate            關於考前準備   自考試之前其實我完全沒有過 AWS ，直到去年年底才開始接觸並且使用 AWS 平台，   準備這張大概花了一個月的時間準備，希望能給有意願準備這張認證的人一點參考方向。   認證有效期   因為雲端技術不斷的推陳出新，Amazon Web Services 設定這張認證的有效期為兩年。   費用   考試費用為 150 USD 並不便宜，建議大家可以準備充裕點再去考試才不會白白浪費了錢。   測驗語文、形式、題數及時間      考試語言：我報名時選擇的是英文 (不過現在也有提供中文的試卷)   考試形式：選擇題、多選題   考試題數：約 60 題   考試時間：80 分鐘   及格分數：AWS 認證會依照每次考試的統計分佈進行篩選，當然及格分數抓在 70% 比較保險。   範例試題   作答完會詢問是否要填寫問卷，我直接跳過了。   重點準備   準備考試前建議先了解一次考試的幾個大方向，考試官方頁面可以參考 AWS 官方的認證頁面 ，      (1.0) Designing highly available, cost-efficient, fault-tolerant, scalable systems (60%)   (2.0) Implementation/Deployment (10%)   (3.0) Data Security (20%)   (4.0) Troubleshooting (10%)   詳細內容可以讀過一次 Blueprint 得到更詳細的資訊。   其中我個人考試的經驗，第一項 HA 及 第四項 Security 真的非常重要，當然考什麼樣的應用還是運氣成份為重，   尤其像我考的時候 IAM 的部份考蠻大的比重，連 Active Directory 都出現了，Snapshot、EBS Encryption 等等也考得我滿身汗，   或是像 ec2 instance 如果應用於 MySQL database 的情境，如何同時兼顧 cost-efficient 及 fault-tolerant 等等問題，這部份除了會有用 AWS 的 Solutions 外可能也會出現像是使用 RAID 等等的情境。   如果像我一樣完全沒有使用過 AWS ，非常推薦趁 Udemy 有特價的時候一次買好 acloud guru 推出的 AWS Certified Solutions Architect - Associate 2017，   能夠有系統性的幫助入門了解整個 AWS Cloud ，而且 Udemy 時不時有特價及優惠碼，一次購入原價 NTD$ 7, 000 的課程 NTD$ 300 左右就可以買到。   利用 Udemy App ，我每天早上坐捷運的時候就看個一兩個片段，累積半個月下來其實能夠很有效的理解整個 AWS Cloud 。   而且 Solution Architect Associate 的考試範圍非常的廣，廣到一個非常的難以想像，且考試會有非常多的使用情境，個人考試當天做試題也是戰戰兢兢的慎選每個答案。   推薦大家仔細閱讀過幾個重點服務的 FAQ 常見問答集，有些問題很容易被考出來。      VPC (Internet Gateway / NAT 會出現在 Troubleshooting 的問題)   EC2 (AMI / AutoScaling / ELB / ALB / Spot, Reserved, On-demand 差別)   EBS (Snapshot)   S3 (Glacier / RRS / IA / Eventual Consistency)   IAM (User / Group / Role)   RDS   SQS   DynamoDB   Route53 (Alias / Zone Apex / Routing Policies)   Whitepaper   很多心得可能都推薦大家讀 Whitepaper ，但是 Whitepaper 其實內容不少，所以我考前也是抓 Security Whitepaper 大略看看而已，   當然還是推薦大家仔細閱讀裡面的內容，在解決架構上的設計能有一定的幫助。      AWS whitepapers   AWS Overview   AWS Security Best Practices   考試過程建議大家遇到太長或是不懂的題目先做個 Mark 跳過，先把握拿到比較容易拿的分數，做過一遍後再回來檢查也會比較沒壓力。   最後，祝大家考試順利。   ","categories": [],
        "tags": ["aws","certificate"],
        "url": "https://easoncao.com/AWS-Certified-Solution-Architect-Associate-Preparation/",
        "teaser": "https://easoncao.com/aws-saa-cert.png"
      },{
        "title": "Memory Management and Paging Quick Note",
        "excerpt":"對於 Paging 的疑惑   在讀清大作業系統開放式課程 Memory Management - Paging 一章時對於裡面一個範例產生了疑問，在這邊簡單紀錄一下。      Given 32 bits logical address, 36 bits physical address and 4KB p    age size, what does it means?       Page table size = 2^32 / 2^12 = 2^20 entries   Max program memory : 2^32 = 4GB   Total physical memory size: 2^36 = 64GB   Number of bits for page number: 2^20 pages, 20bits   Number of bits for frame number: 2^36 / 2^12 = 2^24, 24bits   Number of bits for page offset: 4KB page size, 2^12, 12bits   在閱讀這些資料的時候不免疑惑，為什麼是 2^36 Bytes (64GB) 的 physical memory，不是 36 bits logical address， 應該可用的記憶體應該不是 2^36 bits 嗎，而且 Max program memory 可以到 4GB !?   更讓我疑惑的是在計算 Page entries ，明明是不同單位的東西 (32 bits 與 4KB, 4KB 應該是 4096 bytes * 8 = 32768 bits)。   後來經過一些資料的查詢，發現我對於記憶體的管理上有一些誤解，這邊有提到logical address，注意這邊指的是 address ，   可參考 wikipedia 對於 memory address 的解釋：      the more bits used, the more addresses are available to the computer. For example, an 8-bit-byte-addressable machine with a 20-bit address bus (e.g. Intel 8086) can address 2^20 (1,048,576) memory locations, or one MiB of memory, while a 32-bit bus (e.g. Intel 80386) addresses 2^32 (4,294,967,296) locations, or a 4 GiB address space. In contrast, a 36-bit word-addressable machine with an 18-bit address bus addresses only 2^18 (262,144) 36-bit locations (9,437,184 bits), equivalent to 1,179,648 8-bit bytes, or 1152 KB, or 1.125 MiB—slightly more than the 8086.    由此可知，代表說一個 physical memory address 對應到的可用空間為 1 byte ，   我們市面上常見的 32 bits 或 64 bits 處理器指的是處理器核心所能存取及表示的最大記憶體位址，各有 2^32 bits 及 2^64 bits 種組合，詳見 [2]。   總結   所謂的 36 bits physical address，可表示實體記憶的位址為 2^36 bits，因為一個位址可用的記憶體空間大小為 1 Byte，共可用的記憶體大小為 2^36 Bytes。   Reference      Memory address   how long is a memory address typically in bits   國立清華大學開放式課程OpenCourseWare(NTHU, OCW) - 作業系統   ","categories": [],
        "tags": ["operating-system"],
        "url": "https://easoncao.com/memory-management-and-paging-quick-note/",
        "teaser": null
      },{
        "title": "AWS Certified Developer Associate 認證考試準備及心得",
        "excerpt":"                  AWS Solutions Architect Associate            關於考前準備   相比前一個 SA 認證，其實沒什麼準備，這樣講好像有點不負責任。的確，因為考試時間跟學校期末考疊在一起，所以沒有像考 Solution Architect Associate 那麼仔細的唸。   題目的範圍考比較多 DynamoDB 及 SDK / API 的使用，如果之前跟我一樣有先考過 Solution Architect Associate 準備起來會比較輕鬆一點。   考試前一定要花點時間了解 DynamoDB 的 Read / Write Provision Throuput 怎麼計算，考試會考非常多的 Global Index / Sencondary Index 相關的問題。   認證有效期   與 Solution Architect Associate 一樣是兩年。   費用   考試費用為 150 USD (建議大家可以準備充裕點再去考試才不會白白浪費了錢)   考試地點   恆逸教育訓練中心有提供 AWS 認證的考試考場，詳細內容可以參考考試報名。   測驗語文、形式、題數及時間      考試語言：英文 (如果英文怕影響作答可以在報名時選擇中文的試卷)   考試形式：選擇題、多選題   考試題數：約 60 題   考試時間：80 分鐘   及格分數：建議及格分數抓在 70% 比較保險。   範例試題   作答完會詢問是否要填寫問卷，我一樣直接跳過了。   考試報名   AWS 認證目前都從 Kryterion (Webassessor) 的界面移轉到 AWS training，可以透過官方的界面進行考試的報名。   重點準備   考試前建議先了解一次考試的幾個大方向，考試官方頁面可以參考 AWS 官方的認證頁面 ，Blueprint (必讀)。      (1.0) AWS Fundamentals (10%)   (2.0) Designing and Developing (40%)   (3.0) Deployment and Security (30%)   (4.0) Debugging (20%)   Professional experience using AWS technology     Hands-on experience programming with AWS APIs   Understanding of AWS Security best practices   Understanding of automation and AWS deployment tools   Understanding storage options and their underlying consistency models   Excellent understanding of at least one AWS SDK   General IT Knowledge     Understanding of stateless and loosely coupled distributed applications   Familiarity developing with RESTful API interfaces   Basic understanding of relational and non-relational databases   Familiarity with messaging &amp; queuing services   如果具備 Solution Architect Associate / Professional 的背景的話來考這個基本上可以省一半的力氣，Developer Associate 著墨在 Architect 的部份不會太多，   AWS Fundamentals，涵蓋 EC2 / VPC / DynamoDB / SWF (Simple Work Flow) / S3 (Consistency read-after-write) / 基本的網路Troubleshooting，建議針對這些 Developer 考試內容內重點使用的服務進行比較詳細的了解。可以透過官方文件及常見問題得到很多相關的資訊。   如果像我過去沒有使用過 AWS 的經驗，可以透過 acloud guru 推出 AWS Certified Developer - Associate 2017 課程 來快速入門，   能夠有系統性的幫助入門了解 AWS 各式各樣的服務，完整的濃縮。(而且 Udemy 時不時有特價及優惠碼)   推薦大家仔細閱讀過幾個重點服務的 FAQ 常見問答集，因為常見的問題確實就是成為考題的必要。      VPC (Internet Gateway / NAT 會出現在 Troubleshooting 的問題)   EC2 (AMI / AutoScaling / ELB / ALB / Spot, Reserved, On-demand 差別)   EBS (Snapshot)   S3 (Glacier / RRS / IA / Eventual Consistency)   IAM (User / Group / Role)   RDS   SQS   SWF   Lambda   API Gateway   DynamoDB (Index)   Route53   Serverless   目前考試內容我個人是沒有看到太多跟 Serverless 相關的問題，但是畢竟使用雲服務 Serverless 是不可避免的一種解決方案，有興趣的話可以多閱讀 Lambda 等服務相關的資訊。   考試內容會比較多著墨在 SDK 的使用，以及一些會使用的 API，同時 DynamoDB 一直是特別要強調一定會出現的。   考試中建議大家遇到太長或是不懂的題目先做標記跳過，先把握拿到比較容易拿的分數，做過一遍後再回來檢查也會比較沒壓力。   祝大家順利通過認證！   ","categories": [],
        "tags": ["aws","certificate"],
        "url": "https://easoncao.com/AWS-Developer-Associate-Preparation/",
        "teaser": "https://easoncao.com/assets/images/aws-da-cert.png"
      },{
        "title": "Fix node.js cannot update to 8.x or other version on ubuntu 16.04",
        "excerpt":"Node.js upgrade issue   When I was working with few projects with latest npm or related dependencies, sometimes we have to upgrade our node.js version in order to jump out of the dependencies hell especially you are migrating to the latest node.js.   However, when I am updating the node.js with the command apt-get upgrade or apt-get install nodejs on ubuntu 16.04 or other linux distrubition. It always telling me that my nodejs is already up-to-date.   Therefore, here are my few notes regarding the upgrade issue and troubleshooting footprint, hope it would be helpful to you:   0. Check the node.js version and download the latest version via package manager   You can use the option -v to check your current node.js version in your system.   node -v   Also, you may try to install/upgrade your node.js via the package manager:   curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash - sudo apt-get install -y nodejs   If your node.js still stay at the oldest version, please check the next steps.   1. Check /etc/apt/sources.list.d/nodesource.list   If you installed the node.js before, or ran the update command before. In the /etc/apt/sources.list.d/nodesource.list, you may see the following contents:  deb https://deb.nodesource.com/node_5.x trusty main deb-src https://deb.nodesource.com/node_5.x trusty main   If you are cuurently using the version 5.x, congratulations, that’s why your nodejs always telling you that your node.js is up-to-date but it actually not.   Please change the version to 8.x as the example below:  deb https://deb.nodesource.com/node_8.x trusty main deb-src https://deb.nodesource.com/node_8.x trusty main   Now, you can install the latest node.js via the following command (Debian and Ubuntu based Linux distributions).  sudo apt-get update sudo apt-get install -y nodejs   For more detail on other distributions, you can read the document on node.js.   Reference:      Installing Node.js via package manager   ","categories": [],
        "tags": ["nodejs"],
        "url": "https://easoncao.com/fix-nodejs-cannot-update-to-8-on-ubuntu-16-04/",
        "teaser": null
      },{
        "title": "快速建立 ubuntu 應用程式啟動圖示",
        "excerpt":"建立 ubuntu 應用程式啟動圖示   在以下任意目錄建立 .desktop 檔案能夠協助你建立 ubuntu 應用程式啟動圖示      ~/.gnome/apps   ~/.local/share/applications/   /usr/share/applications/   例如: 在 ~/.local/share/applications 建立 myapp.desktop   myapp.desktop   [Desktop Entry] Name=MyApp Comment=Application launcher for my execution Exec=/path/to/executable_file Icon=/path/to/icon Terminal=false Type=Application Categories=Network;   Reference:      ubuntu documentation - UnityLaunchersAndDesktopFiles   ","categories": [],
        "tags": ["ubuntu"],
        "url": "https://easoncao.com/quickly-create-the-ubuntu-application-launch-icon/",
        "teaser": null
      },{
        "title": "設定 zsh 為 tmux 預設的 shell",
        "excerpt":"設定 tmux 使用的 shell   自從從 screen 無痛轉移到 tmux 後，tmux 強大的功能性實在是令人愛不釋手，這邊筆記一下如何更換 tmux 啟動新的 session 時預設的 shell 。   in ~/.tmux.conf or . Works on Fedora.   使用編輯器開啟 ~/.tmux.conf，如果沒有可以新增一個。或是修改 /etc/tmux.conf 設定所有使用者預設的 tmux 設定檔。   新增以下內容:  set-option -g default-shell /bin/zsh   如果不是使用 zsh ，也可以將 /bin/zsh 換成慣用的 shell，例如: /bin/bash、/bin/csh 等。   Reference:      tmux   How can I make tmux use my default shell   ","categories": [],
        "tags": ["Linux"],
        "url": "https://easoncao.com/set-the-zsh-as-the-default-shell-for-tmux/",
        "teaser": null
      },{
        "title": "Python threading.Thread 錯誤筆記",
        "excerpt":"最近在試著用 Python 寫多線程的程式遇到一點小狀況，範例程式如下：   import time import threading from multiprocessing import Queue  def subProgram(queue):     for i in range(10):         queue.put(i)  def main():      queue = Queue()      thread = threading.Thread(target=subProgram, args=(queue))     thread.start()      time.sleep(2)     while not queue.empty():         print queue.get()  main()   看似沒問題的程式碼，執行後卻發生錯誤：   Exception in thread Thread-1: Traceback (most recent call last):   File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner     self.run()   File \"/usr/lib/python2.7/threading.py\", line 754, in run     self.__target(*self.__args, **self.__kwargs) TypeError: subProgram() argument after * must be an iterable, not Queue   在 stackoverflow 找到了類似的問題跟解法：      You need to add , after s sending just s to args=() is trying to unpack a number of arguments instead of sending just that single arguement.    但是因為對 Python 太不熟了，惡補了一下基本語法跟翻了一下手冊，確定 args 是 tuple 的型態      class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}) This constructor should always be called with keyword arguments. Arguments are:          group should be None; reserved for future extension when a ThreadGroup class is implemented.     target is the callable object to be invoked by the run() method. Defaults to None, meaning nothing is called.     name is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number.     args is the argument tuple for the target invocation. Defaults to ().     kwargs is a dictionary of keyword arguments for the target invocation. Defaults to {}.       If the subclass overrides the constructor, it must make sure to invoke the base class constructor (Thread.init()) before doing anything else to the thread.    tuple 如果只有單一元素 (element) 的話必須加個,逗號才行，不然就會被解讀成單一物件。   參考以下範例會比較了解：   &gt;&gt;&gt; a = (111,222) &gt;&gt;&gt; a (111, 222) &gt;&gt;&gt;  &gt;&gt;&gt; a = (111, 222) &gt;&gt;&gt; a (111, 222) &gt;&gt;&gt; b = (111) &gt;&gt;&gt; b 111 &gt;&gt;&gt; c = (111,) &gt;&gt;&gt; c (111,)   可以看到 b = (111) 被解讀成 111 了，這並不是預期的狀況，改成 c = (111, ) 就會正常解析成 tuple 型態了。   同理，只要在   thread = threading.Thread(target=subProgram, args=(queue))   args=(queue) 補上 , 符號就解決這項錯誤：   thread = threading.Thread(target=subProgram, args=(queue,))   最後修正的程式如下：   import time import threading from multiprocessing import Queue  def subProgram(queue):     for i in range(10):         queue.put(i)  def main():      queue = Queue()      thread = threading.Thread(target=subProgram, args=(queue,))     thread.start()      time.sleep(2)     while not queue.empty():         print queue.get()  main()   Reference:      Thread Objects   stackoverflow  ","categories": [],
        "tags": ["python"],
        "url": "https://easoncao.com/fix-python-thread-args-error-note/",
        "teaser": null
      },{
        "title": "Laravel PHP 優化之路：效能瓶頸的解決方案",
        "excerpt":"前言   最近在開發一些後端專案程式的時候，遇到了一些效能上的挑戰，從功能實作後到重構整個過程蠻有趣的，藉由文字的紀錄自己的優化過程。   案例一：檢查資料庫內重複的資料   在一次的案例中，必須檢查使用者新增的資料在資料庫內是不是有重複的內容，這邊以簡單的留言系統為例，所以就寫了這樣的邏輯並且抽離出 checkCommentIsDuplicate 這樣的 method ，用來檢查該筆資料是不是有在資料表內重複：   foreach ($newComments as $key =&gt; $comment) {     // Check the comment is duplicate in database     if ($this-&gt;CommentManager-&gt;checkCommentIsDuplicate($comment)) {         // Do something, like remove the item from array         unset($newComments[$key]);     } }   一開始暴力解都是用以下作法：   class CommentManager {     public function checkCommentIsDuplicate($insertComment)     {         $condition = [             ['title', '=', $insertComment['title']],             ['content', '=', $insertComment['content']],         ];          return (Comment::where($condition)-&gt;count() &gt; 0);     } }   當然，在本機的資料庫跑得好好的，一切都很理想，連線既沒有延遲，更不用煩惱 PHP Timeout 的問題。   一佈署到 AWS 上，資料庫有小於 100~500 筆的資料看似一切還好，一插入 1000 筆資料問題就來了。   因為 AWS 的 RDS (Relational Database Service) 與 PHP 程式間的連線會有延遲，一個 SQL 查詢其實就是非常昂貴的運算資源。   假設資料庫內有 500 筆資料，今天我要新增 300 筆資料，如果要檢查新增的資料是不是與資料庫有重複。這樣每一筆新增的資料，就要下一次 SQL 查詢，這樣至少要產生 300 次 SQL 查詢，就算資料庫可接受的連線時間，PHP 還是很快的就超過預設的 30 秒執行時間，逾時中止了。   就算調大執行時間，新增的資料如果成長成 1000 筆、甚至 5000 呢？就算程式跑得完使用者不見得等的下去。   於是乎，就會想到，我可以一次查詢後在程式內檢查啊！於是乎程式又被改成了這樣：   class CommentManager {     public function checkCommentIsDuplicate($currentCommentInDB, $insertComment)     {         foreach ($currentCommentInDB as $currentComment) {             if ($currentComment['title'] == $insertComment['title'] &amp;&amp; $currentComment['content'] == $insertComment['content']) {                 return true;             }         }          return false;     } }   開心的改好之後在測試機上測試一下，嗯，500 筆好像還行。   但是當新增的資料量超過 1000 筆，當資料庫內有 4000 筆以上的資料，可能的效能瓶頸又出現了。   因為每一筆資料必須與查詢後的 4000 筆資料進行循序比較，這樣最壞的情況 (都沒有重複的資料)，需要比較 1000 * 4000 次，這個運算量還是挺費時的。   善用 Hashing 解決大量資料的效能瓶頸   於是，不外乎就試著將資料表內的每筆資料雜湊後作為 Hash key，建立 Hash table，利用這個 Hash table 進行 duplicate 的檢查，這樣大幅減少比較的次數，沒有碰撞的情況下(也不應該會碰撞，資料庫內的資料寫入時不會 Duplicate )，比較的時間複雜度可以優化到 O(1)，在本機的測試環境連接 ap-northeast-1 的 RDS (完全能明顯感受到資料庫連線延遲) 得到不錯的執行時間。   測試環境：     CPU: Intel Core i7-4510U (2.00 GHz x 4)   Memory: 8 GB   OS: ubuntu 16.04 LTS x64 (Linux 4.4.0-97-generic #120-Ubuntu SMP Tue Sep 19 17:28:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux)   PHP 7.0.22-0ubuntu0.16.04.1 (cli) ( NTS )   實測後，新增 4615 筆資料，並與與資料庫內 9230 筆進行 Duplicate 進行比較，整體從使用者送出 Request 至 Response 執行時間約 6.558937 s，因為是在開發端進行連線，連接到 RDS 有蠻大的延遲，佈署到 AWS 上後應該可以更快。   實測本機單元測試的結果，使用 Laravel 內建的 Factory 產生 90010 筆測試資料：      [1st] stress test: compare 90010 data in database, total use 0.000049 s.   [2nd] stress test: compare 90010 data in database, total use 0.000004 s.   除了第一次產生 Hash table 稍微耗時需 0.000049 s ，第二次開始的執行時間就會明顯降低。   基本上 PHP 的 Array 就是一個很棒能實作 Hash table 的方式，執行速度非常的不錯，以下是實作的概念性程式碼：   class CommentManager {     public function __construct()     {         $this-&gt;CommentHashTable = null;     }      /**      * Check the comment is duplicated in database or not      *      * $currentCommentInDB: the comments in the database.      * $insertComment: the comment will insert, check the      *                 duplication here.      *      * @param  \\App\\Models\\Comment $currentComment      * @param  array $insertComment      * @return bool      */     public function checkCommentIsDuplicate($currentCommentInDB, $insertComment)     {         $hashItem = ['title', 'content'];          if ($this-&gt;CommentHashTable == null) {              // Normalize the $currentCommentArray             if ($currentCommentInDB-&gt;count() == 0) {                 $currentCommentArray = [];             } else if ($currentCommentInDB-&gt;count() == 1) {                 $currentCommentArray = [ $currentCommentInDB-&gt;toArray() ];             } else {                 $currentCommentArray = $currentCommentInDB-&gt;toArray();             }              $this-&gt;createCommentHashTable($currentCommentArray, $hashItem);         }          $insertCommentHashKey = $this-&gt;HashKey($insertComment, $hashItem);          return isset($this-&gt;CommentHashTable[$insertCommentHashKey]);     } }   案例二：後端程式使用 Eloquent ORM Relationships 進行資料多重關聯查詢   另一個效能瓶頸在於 Laravel 的 Relationships，Laravel 5.5 一樣提供了 Eloquent relationships 方便查詢不同資料表內的資訊，以下是使用 Eloquent model 常見的操作：   class UserController extends Controller {     public function getLatestUser()     {         $users = User::orderBy('id', 'desc')-&gt;take(1000)-&gt;get();          foreach ($users as $index =&gt; $user) {             $users[$index]['id'] = $user-&gt;id;             $users[$index]['name'] = $user-&gt;name;             $users[$index]['email'] = $user-&gt;email;             $users[$index]['updated_at'] = $user-&gt;updated_at;             $users[$index]['created_at'] = $user-&gt;created_at;             $users[$index]['recent_post_id'] = $user-&gt;posts-&gt;first()-&gt;id;             $users[$index]['recent_comment_id'] = $user-&gt;comments-&gt;first()-&gt;id;             $users[$index]['recent_orders_id'] = $user-&gt;orders-&gt;first()-&gt;id;         }     } }   因為 Eloquent 動態 Property 屬於 lazy loading 的特性，Laravel Eloquent relationships 的 SQL 查詢是在 foreach 迴圈內每次執行時被建立的。      Dynamic properties are “lazy loading”, meaning they will only load their relationship data when you actually access them. Because of this, developers often use eager loading to pre-load relationships they know will be accessed after loading the model. Eager loading provides a significant reduction in SQL queries that must be executed to load a model’s relations.    也就是說上述的程式如果被轉譯成 SQL 查詢會像是這樣：   select * from users  select id from posts where id = 1 select id from comments where id = 1 select id from orders where id = 1  select id from posts where id = 2 select id from comments where id = 2 select id from orders where id = 2  select id from posts where id = 3 select id from comments where id = 3 select id from orders where id = 3  ... 略   大致列舉查詢的語句，完整語句應該與實際 Laravel 轉譯的結果有些不同。   可見的是，如果今天有 1000 筆的 User 和其關聯性的資料(post, comment, order)，就必須得產生至少 1000 * 3 次的關聯查詢，這樣的做法在 SQL 建立連線和查詢的過程是十分耗時的。   使用 Eager Loading 減少 SQL 查詢的次數   Laravel 5.5 提供了 Eager Loading 可以協助平衡這樣的效能瓶頸，使用 with() 配合 Eloquent relationships 的優點在於資料的查詢方式是一次性的，Laravel 會預先將所有關聯的資料一次性的完成查詢，避免上述迴圈執行時會再重新建立一個新的 SQL 查詢連線，修改後的邏輯大致上是這樣：   class UserController extends Controller {     public function getLatestUser()     {         $users = User::orderBy('id', 'desc')-&gt;with(['posts', 'comments', 'orders'])-&gt;take(1000)-&gt;get();          foreach ($users as $index =&gt; $user) {             $users[$index]['id'] = $user-&gt;id;             $users[$index]['name'] = $user-&gt;name;             $users[$index]['email'] = $user-&gt;email;             $users[$index]['updated_at'] = $user-&gt;updated_at;             $users[$index]['created_at'] = $user-&gt;created_at;             $users[$index]['recent_post_id'] = $user-&gt;posts-&gt;first()-&gt;id;             $users[$index]['recent_comment_id'] = $user-&gt;comments-&gt;first()-&gt;id;             $users[$index]['recent_orders_id'] = $user-&gt;orders-&gt;first()-&gt;id;         }     } }   在 Query 內使用了 with(['posts', 'comments', 'orders']) ，這樣 Laravel 就會在執行查詢時也查詢關聯性的資料。   依照文件上的解釋這樣 Laravel 透過 Eloquent ORM 轉譯成 SQL 語句查詢時會執行像這樣的語句：   select * from users select * from posts where id in (1, 2, 3, 4, 5, ...) select * from comments where id in (1, 2, 3, 4, 5, ...) select * from orders where id in (1, 2, 3, 4, 5, ...)   這樣將原本的查詢縮減到非常小的次數，同時降低資料庫連線建立的成本和避免連線建立延遲放大 PHP 程式等待的速度。   經過這樣小小的更正後，約 600 ~ 1000 筆的資料從原本的處理速度約 10 ~ 20s 不等下降至 0.5ms ~ 2s，這點微妙的時間差就能讓使用者體驗有完全不一樣的感受。使用 Eager Loading 也能減少伺服器查詢的耗時及次數，若有遇到同樣的效能瓶頸可以試著使用這樣的方式來達到效能使用上的平衡。   Reference:      Relationship Methods Vs. Dynamic Properties   Laravel Eloquent Eager Loading   ","categories": [],
        "tags": ["php","laravel"],
        "url": "https://easoncao.com/php-Laravel-optimization-boost-your-code/",
        "teaser": null
      },{
        "title": "AWS Certified SysOps Administrator - Associate 認證考試準備及心得",
        "excerpt":"                  AWS Certified SysOps Administrator - Associate            關於考前準備   相比前面的 Solution Architect 及 Developer 認證，SysOps 比較著重在災難復原和系統監控的部分，比較困難的地方在於這個認證會有比較多長的情境題，做題時需要多花一點時間做準備。   像是 ELB 發生效能瓶頸時會需要看哪些 metrics，基本的 VPC 和 EC2 等還是會考一部分，但是比較多著墨的重點在備份的解決方案以及升級的問題。安全性面，在 Audit 的部分也會考很多 IAM 相關的問題，如果之前跟我一樣有先考過 Solution Architect Associate 和 Developer Associate 準備起來就可以聚焦在比較重點性的服務，像我考試的時候就問到不少關於 CloudWatch 和 OpsWork 之類的問題。   認證有效期   與 Solution Architect Associate、Developer Associate 一樣是兩年。   費用   考試費用為 150 USD (建議大家可以準備充裕點再去考試才不會白白浪費了錢)   考試地點   今年度九月開始 AWS 官方授權的考試代理商從 Kryterion 轉為 PSI ，這次我選擇的考試在 Global Education Association in Taiwan (AMP) (對不起我不確定中文叫什麼，應該是托福相關語言中心相關的考試代理商)。因為是語言相關的考試中心，我去考的時候，是本年度第五場，所以考場設備並沒有像恆逸提供隔間的考場，只有一台筆電和約 30 人的小教室考試。   這次考場比較嚴謹需要查驗護照，這部分在試後跟親切的監考姊姊聊了一下得知前面有三場的應試人因為沒有帶護照被拒絕考試，不過當初恆逸只需要備有身份證件即可，建議大家攜帶護照應考，如果沒有護照建議考前跟考試中心電話確認一下。   當時考試單位在考試的時候沒有提供紙筆是比較困擾的地方，考場若能使用紙筆是非常有用的，特別是題目需要畫出拓墣的時候才能勾勒出對應的系統架構。比較慘的是考試中途網路還發生斷線，虛驚一場。   不過，上述問題在考完之後也跟親切的監考姊姊反應了一下這些部分，包含反應僅需查驗身份證件和過去不用護照的問題，避免大家不要再踩到跟我一樣的坑啊！   測驗語文、形式、題數及時間      考試語言：英文 (如果英文怕影響作答可以在報名時選擇中文的試卷)   考試形式：選擇題、多選題   考試題數：約 55 題   考試時間：80 分鐘   及格分數：建議及格分數抓在 70% 比較保險。   範例試題   作答完會詢問是否要填寫問卷，我一樣直接跳過了。   考試報名   可以透過官方的界面 AWS training 進行考試的報名，考試代理商為 PSI exam，   重點準備   考試前一定要花點時間了解一些基本服務的 metrics     Elastic Compute Cloud   Elastic Block Storage   Elastic Load Balancer   RDS   考試前建議先了解一次考試的幾個大方向，考試官方頁面可以參考 AWS 官方的認證頁面 ，Blueprint (必讀)。      (1.0) Monitoring and Metrics (15%)   (2.0) High Availability (15%)   (3.0) Analysis (15%)   (4.0) Deployment and Provisioning (15%)   (5.0) Data Management (12%)   (6.0) Security (15%)   (7.0) Networking (13%)   Introduction The AWS Certified SysOps Administrator – Associate Level exam validates a candidate’s ability to:     Deliver the stability and scalability needed by a business on AWS   Provision systems, services and deployment automation on AWS   Ensure data integrity and data security on AWS technology   Provide guidance on AWS best practices   Understand and monitor metrics on AWS   AWS Knowledge     Minimum of one year hands-on experience with the AWS platform   Professional experience managing/operating production systems on AWS   A firm grasp of the seven AWS tenets – architecting for the cloud   Hands on experience with the AWS CLI and SDKs/API tools   Understanding of network technologies as they relate to AWS   Good grasp of fundamental Security concepts with hands on in experience in implementing Security controls and compliance requirements   General IT Knowledge     1-2 years’ experience as a systems administrator in a systems operations role   Experience understanding virtualization technology   Monitoring and auditing systems experience   Knowledge of networking concepts (DNS, TCP/IP, and Firewalls)   Ability to collaborate with developers and the general business team/company wide   AWS Fundamentals，涵蓋 EC2 / VPC / DynamoDB / SWF (Simple Work Flow) / S3 (Consistency read-after-write) / 基本的網路 Troubleshooting，一樣建議針對這些考試內容內重點使用的服務進行比較詳細的了解。可以透過官方文件及常見問題得到很多相關的資訊。   這張認證最重要的在於 HA (High Availability) 和災難復原，所以像是 RDS 會考非常多關於 Backup window / Backup strategies 相關的問題，以及 AutoScaling。   如果對於 AWS 的使用並不熟悉經驗，可以透過 acloud guru 推出 AWS Certified SysOps Administrator - Associate 2017 來快速入門，   能夠有系統性的幫助入門了解 AWS 各式各樣的服務，完整的濃縮。   同樣推薦大家仔細閱讀過幾個重點服務的 FAQ 常見問答集，因為常見的問題確實就是成為考題的必要。      VPC (Internet Gateway / NAT 會出現在 Troubleshooting 的問題)   EC2 (AMI / AutoScaling / ELB / ALB / Spot, Reserved, On-demand 差別)   EBS (Snapshot)   S3 (Glacier / RRS / IA / Eventual Consistency)   IAM (User / Group / Role)   RDS   SQS   Lambda   API Gateway   DynamoDB (Index)   Route53   Cloudwatch   OpsWork   還有   考試中建議大家遇到太長或是不懂的題目先做標記跳過，特別是 SysOps 題目非常落落長，先把握拿到比較容易拿的分數，做過一遍後再回來檢查也會比較沒壓力。   祝大家順利通過認證！   ","categories": [],
        "tags": ["aws","certificate"],
        "url": "https://easoncao.com/AWS-SysOps-Administrtor-Associate-Preparation/",
        "teaser": "https://easoncao.com/assets/images/aws-sysops-cert.png"
      },{
        "title": "使用 AWS Lambda 建立 Line bot",
        "excerpt":"這篇文章主要是記錄如何透過 AWS Lambda 的服務，同時利用 AWS API Gateway，配合 Line 提供的 Bot API，打造屬於自己的 Line Bot。   利用 API Gateway 及 Lambda 建立 webhook 用於接收 Line API 發送的資訊   Please refer: Creating an AWS Lambda Function and API Endpoint - Slack   建立一個 Line bot   在正式使用 Line Bot 前，首先會需要一個 Line developer 的帳戶，並且建立一隻 Line 的帳戶。   首先登入 Line developer console 建立一個使用 Messaging API 的帳戶。   如果是第一次建立，會需要新增一個 Provider ，用來提供使用 Line Messaging API 的帳戶的相關資訊。   詳細的快速建立步驟可以參考官方的文件。   創建完 Channel 後，有幾項選項可以在設定內看到:     Channel access token (long-lived) : 用來操作 Channel 的 token   Use webhooks : 開啟或關閉 Webhook 選項，接續下面的 Webhook URL 設定   Webhook URL : 若有向機器人發送訊息、邀請加入聊天室等都會觸發到我們自訂的 Webhook 位址，並傳送對應的 API 訊息   Allow bot to join group chats : 是否允許 Bot 加入群組聊天   為了測試發送訊息的功能，只要在 webhook 一欄填入前面建立的 API Gateway endpoint，並且利用其他帳戶發送訊息或是加入到群組內，   就會觸發 Lambda 接收相關的資訊，可以在 Cloudwatch 內獲得對應的 User ID   Send messages:   curl -X POST \\ -H 'Content-Type:application/json' \\ -H 'Authorization: Bearer {ENTER_ACCESS_TOKEN}' \\ -d '{     \"to\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",     \"messages\":[         {             \"type\":\"text\",             \"text\":\"Hello, world1\"         },         {             \"type\":\"text\",             \"text\":\"Hello, world2\"         }     ] }'   利用 virtualenv 包裝 python 相依的套件   mkdir ~/linebot docker run -v ~/linebot:/root/linebot python:3.6.3 /bin/bash   install.sh  #!/bin/bash  curl -sL https://bootstrap.pypa.io/get-pip.py -O python get-pip.py pip install virtualenv   利用 virtualenv 建立 Lambda 部署套件   mkdir /root/linebot/virtualenv virtualenv /root/linebot/virtualenv source /root/linebot/virtualenv/bin/activate pip install requests json exit   完成建立後，就可以離開 virtualenv 的 shell，進入 ~/linebot/virtualenv/lib/python3.6/site-packages 將主程式 lambda_function.py 放入並進行打包:   cp ~/linebot/lambda_function.py ~/linebot/virtualenv/lib/python3.6/site-packages/ cd ~/linebot/virtualenv/lib/python3.6/site-packages/ zip -r linebot.zip .   完成打包之後，將打包的程式上傳至 AWS Lambda 即可。   Reference:      Line developer: Send push message   AWS Lambda: Creating a Deployment Package (Python)   ","categories": [],
        "tags": ["aws","python"],
        "url": "https://easoncao.com/create-a-line-bot/",
        "teaser": null
      },{
        "title": "一個關於我大學專題的故事 - 四軸無人機專案 Parrot Bebop Drone 和 Python library Katarina",
        "excerpt":"前言   大學四年的學習歷程即將告一個段落，邁入人生下個階段，趁著幾天空閒的時間將這些記憶跟過程付諸文章，紀錄自己那些年不凡且獨特的過往。   一個關於無人飛機的專案   在台灣普及的大學學程裡面，大部分都納入了專題研究相關的課程內容，一旦升上大三，通常不管是哪個系所，幾乎開始動工進行專題的工作。可能有很多學生或是系所會將專題視為大學四年內的學習成果，非常重視，不過，畢竟每個學期都在完成不同類型的專案，相對於我自己大一每學期都有專案跟每週都有寫不完的程式相比，對於投入專題工作的心情，其實感受不大，覺得只是在未完成清單裡面多加一項「專題」的待辦事項，加上在實習公司內也有其他重要的事情需要完成，所以對於專題的投入比例並沒有太高，純粹當個好玩的業餘項目在研究。偶而有報告或是接近截止日期時相對投入些，所以就更無憂無慮的可以專心研究自己想深鑽的內容。   雖然心裡這樣說說，但是還是會想著：當然可以也稍微努力做一下，有得獎是好事，沒得獎就算了。於是抱持著這樣的心態選定我自己有興趣的專案題目，尤其我對於影像視覺相關的研究領域很感興趣，從高中開始就使用 OpenCV 做相關的應用，不過這個領域並不是我最擅長的項目，於是乎，既然有這樣的研究機會，我二話不說在大三差不多能找專題老師的時機，就跟同學組隊找了影像視覺應用研究領域的教授來指導我們的專題內容。當初與實驗室教授選定專案題目時，異想天開想搞跟電梯有關的專案，不過後來想想還是有點無用所以中途就換了其他題目，當然也是搞個自己想玩的項目：無人機。   這一搞還好，一玩下去不得了，坑特別多。首先，一般市售的家用無人機，基本上是除了飛行之外能應用於研究實驗用的功能真的是極為稀少，我們從實驗室借到的無人機產品是 2015 主打一般消費市場的飛機，基本上，就是一台遙控飛機在天上飛附上一組鏡頭，而對於飛機所擁有的感測器及軟體，有非常大的不足需要克服。                     Parrot Bebop Drone            (Photo credit: Parrot)   Specification     Video: full HD 1080p   GPS: Yes   Processor: Dual core processor with quad-core GPU   Storage: 8 GB flash storage system   Video resolution: 1920 x 1080p (30 fps)   Photo resolution: 4096 x 3072 pixels   Video encoding: H264   Wi-Fi 802.11a/b/g/n/ac   Wi-Fi Aerials: 2.4 and 5 GHz dual dipole aerials   SDK Release   Product reference     Amazon: Parrot Bebop drone   你可以下載官方提供的 App，就可以透過 Wi-Fi 的方式與無人機進行連線，以手持的方式遙控飛機。那麼一般手持裝置是如何做到控制飛機的？其實背後技術原理是透過發送含有指令的 UDP 封包進行飛機的控制，例如前後左右、旋轉角度等。   一開始打算使用 GitHub 上一些新奇的專案和語言來實做，但是考量我與夥伴由於同時要實習還要兼顧一大堆作業課業報告的緣故，每週能夠開發研究的時間就顯得特別寶貴，於是為了提高開發的速度，我決定使用彼此熟悉的語言和豐富成熟的框架來進行應用的實作：      Python 2.6   OpenCV: 負責影像運算和辨識   Flask: 網頁控制介面開發   Python library for Bebop Drone - katarina: 處理無人機控制飛行和決策   Katarina 是一個開源的 Python 函式庫，是由一群捷克的開發者組成的 Robotika 底下的一項項目，老實說我不是特別清楚團隊的成員背景，但是秉持開源的精神，還是十分感謝他們貢獻這項專案。該項目主要是針對 Parrot 無人機公司釋出的 SDK 提供 Python 的接口，使得 Python 開發者能夠利用熟悉的語言控制該公司的無人機產品。   於是，我改改了幾行代碼   但是這個坑一踩，才知道有多大洞。整個專案最讓我詬病的是，飛機本身的限制造成在專案研究過程很大的障礙，需要逐一克服：      一旦飛機超出一定範圍，Wi-Fi 連線延遲會拉高，影像的回傳就會產生延遲，此時進行的飛行決策很可能是錯誤的。   無人飛機透過四個軸的飛行槳控制，保持飛行的穩定以維持一定的平衡，然而，對於家用的無人機來說，並不能保證其穩定的順暢，很可能因為風阻或是干擾等其他因素飛機會不斷飄移。   由於帶動四軸飛行槳時，馬達運轉需要大量的功耗，然而，家用無人機所攜帶的電池飛行的時間有限，一顆要充將近一小時的電池僅夠支撐室內約 15 - 20 分鐘的飛行，往往研究還沒正式開始測試或是取得需要的數據就得先行換備用電池，不斷的在考驗人性。   飛機為了保持飛行的穩定，其機身設計採用流線型並且限制重量，一旦超出負重，當在空中起飛並盤旋時，飛機會失去本身機身的穩定，所以難以附掛額外的零組件或是電池。   飛機的本身運算能力有限，飛行同時要在飛機本身分析影像內容是一項考驗運算能力和電力的一項挑戰。   同時，最嚴重的是，我發現 Katarina 固然提供豐富的控制方法，然而，他原本使用的影像串流方式十分、十分的慢。回傳的影像可以說是飛機五秒前的位置，一旦 Wi-Fi 傳輸距離與影像串流分析的機器過遠，基本上你就不知道飛機目前飄到那去了。由於我的專案需要即時的影像回饋進行飛機的飛行決策，一旦偵測到目標物體就會往物體靠近飛行並且滯留。如果現在回傳的影像是五秒前的，由於上述提到的致命缺點，飛機很可能因為不穩定已經往左右或其他方向飄移，這時若飛行決策的邏輯還送出飛行指令驅使飛機往該方向前進，一旦附近有障礙物，肯定撞機。所以一旦有幾秒以上的延遲，都是十分致命的。   就算將所有障礙物清空，撇除障礙物問題，還是必須要克服影像串流造成的延遲，因為飛機的位置不一定會是影像當時回饋的相對位置，這都會影響飛行決策是否能正確的往目標物體靠近。但是 Katarina 本身的串流方式就有很大的延遲問題，在 GitHub 上面也有一些關於這個問題的討論：      GitHub issue#3: Improving Speed for Receiving Real-Time Image   經過一番深入研究和測試，我注意到 Parrot 官方提供的 SDK 內其實有配置 RTP 串流的協議和對應的連線資訊：   #define ARDISCOVERY_CONNECTION_JSON_ARSTREAM2_CLIENT_STREAM_PORT_KEY \"arstream2_client_stream_port\" #define ARDISCOVERY_CONNECTION_JSON_ARSTREAM2_CLIENT_CONTROL_PORT_KEY \"arstream2_client_control_port\" #define ARDISCOVERY_CONNECTION_JSON_ARSTREAM2_SERVER_STREAM_PORT_KEY \"arstream2_server_stream_port\" #define ARDISCOVERY_CONNECTION_JSON_ARSTREAM2_SERVER_CONTROL_PORT_KEY \"arstream2_server_control_port\"      libARDiscovery   Parrot-Developers-Stream   此外，我也在網路上找到很有趣的一篇文章：Stream Bebop Video With Python Opencv，這個範例使用 Python 去執行另一個開源一樣提供給 Bebop 以 Node.js 寫成的 node-bebop 專案，利用呼叫 Node.js 版本的函式庫內提供的串流方法進行 RTP 串流，實際上，是蠻有趣的作法。   於是，我就嘗試改改了幾行代碼，經過我的測試並應用在我的專題內，送了一個 Pull Request 給了作者，在與無人機連接時送出必要的影像訊息，在執行時能夠使用 RTP 串流的方式接收影像：      GitHub pull request#14: Enable RTP Streaming on Bebop and add sample code   不久，我就收到了來自 Robotika 的問候：                     Email from Robotika about Katarina project            在我回覆不久，Robotika 的 Martin 就將我的 Pull request 合併進專案內，成為該專案目前唯一一個 Pull request。   收穫？   串流問題解決了，一般來說，OpenCV 的應用程式往往都是單執行緒的運算方式，獲取單張影像後進行運算和特徵判斷，但是這樣的方法往往會在運算速度的緣故發生一些效能瓶頸。於是，我又透過改寫程式利用一些多執行緒的處理技巧，搭配在應用程式裡面設計 Queue 的邏輯，使得接收串流影像是一個單元，真正進行影像判斷和飛行決策的則是另外的單元，並且加入一些判斷機制，盡可能降低因為串流產生的延遲造成的錯誤飛行決策。   不過由於飛機的限制，並沒有深度等等的感測器，整個專案困難的部分也包含影像飛行對於實際距離換算的方法，經過一系列的微調和測試，完成了一個 PoC 的版本：               (這個影片給幾個同學看過後，都被開玩笑其實我們的專題不是無人機收費，比較像是無人飛行武器開發，精準投遞摧毀目標 … lol)   專題展示前幾天至當天早上，我還與夥伴玩起黑客松，逐一的將沒有完成的介面進行收尾以利當天的展示，十分感謝這些年來大大小小專案的磨練，已經練就開始前幾個小時甚至 Demo 當下還在送 Commit / 改程式的功力，不過既然是當成業餘的項目，開心好玩並且從中收穫最重要，比賽反而是其次了。   最後，我並沒有得獎，也許在評審委員教授的眼裡，看見的應用價值是商機和獲獎的必要條件。但是，對我來說，這個專題帶給我的並不是這樣表面的效益和回饋，而是在重重的困難和限制下對於盡力完成研究的方法和價值。我的幾行代碼，卻是貢獻是數個全世界使用 Bebop drone 和 Katarina 的無人機玩家們，即便在不同的國家，我們卻用著相同的語言和同樣的精神在交流、共同解決相同的問題，其所成就感已經超越單一個專題的獎項所帶來的滿足。   我想，這就是一項成就解鎖吧!   ","categories": [],
        "tags": ["python","drone"],
        "url": "https://easoncao.com/a-story-about-my-project-using-bebop-drone-and-katarina/",
        "teaser": null
      },{
        "title": "救回 Windows 上意外關閉且遺失的 Atom 筆記 (1.27.2 x64)",
        "excerpt":"這篇文章主要是記錄下在 Windows 上使用將意外關閉的 Atom 筆記復原。   一般來說，Atom 內建自動復原的功能，但是在某些特殊情況遇到不小心意外關閉 Atom 筆記又很不巧沒有存檔，發生難以復原的情形，可以試試以下方式救回筆記 (版本 1.27.2 x64)。   至以下目錄尋找 Atom 的暫存檔案：   C:\\Users\\&lt;UserName&gt;\\AppData\\Roaming\\Atom\\IndexedDB\\   例如可能會看見像是這樣的目錄結構：  C:\\Users\\&lt;UserName&gt;\\AppData\\Roaming\\Atom\\IndexedDB\\file__0.indexeddb.leveldb   在目錄內，若 Atom 有確實的紀錄你的筆記，可以試著找找 000XXX.log 相關的紀錄檔案，並且使用 Linux 上的 dd 工具將檔案內容復原，就有機會找回遺失的內容：   dd conv=swab &lt; 000715.log &gt; file.txt   Reference:      Atom   Unsaved buffers aren’t restored if there is no project folder added to the window   ","categories": [],
        "tags": ["atom","windows"],
        "url": "https://easoncao.com/recover-atom-note-on-windows/",
        "teaser": null
      },{
        "title": "AWS Certified DevOps Engineer - Professional 認證考試準備及心得",
        "excerpt":"                  AWS Certified DevOps Engineer - Professional            關於考前準備   相比前面的 Solution Architect、Developer 及 SysOps 均屬於 Associate 級別的考試，考試內容相對通用且內容固定，大部分屬於問答式選擇題。   而 Professional 級別的考試難度是不同的檔次，題目不僅長且全部都是情境題，做題時間不僅拉長，做題前更是需要多花一點時間做準備。   認證有效期   與 Solution Architect Associate、Developer Associate、SysOps 一樣是兩年。   費用   考試費用為 300 USD (建議大家可以準備充裕點再去考試才不會白白浪費了錢)   考試地點   今年度九月開始 AWS 官方授權的考試代理商從 Kryterion 轉為 PSI ，台北的恆逸教育訓練中心 有提供相關的認證考場，地點靠近捷運站，於是乎我就選擇了恆逸作為考試地點。   測驗語文、形式、題數及時間      考試語言：英文   考試形式：選擇題、多選題   考試題數：約 80 題   考試時間：170 分鐘 (考試時間幾乎是 Associate 級別的兩倍，同時考驗作答時的耐性與穩定性)   及格分數：建議及格分數抓在 70% 比較保險。   範例試題   考試報名   可以透過官方的界面 AWS training 進行考試的報名，考試代理商為 PSI exam，   重點準備   考試前建議先了解一次考試的幾個大方向，考試官方頁面可以參考 AWS 官方的認證頁面 ，Blueprint (必讀)。      Domain 1: Continuous Delivery and Process Automation (55%)   Domain 2: Monitoring, Metrics, and Logging (20%)   Domain 3: Security, Governance, and Validation (10%)   Domain 4: High Availability and Elasticity (15%)   Introduction The AWS DevOps Engineer - Professional exam is intended for individuals who perform a DevOps role.   This exam validates an examinee’s ability to: Implement and manage continuous delivery systems and methodologies on AWS     Understand, implement, and automate security controls, governance processes, and compliance   validation     Define and deploy monitoring, metrics, and logging systems on AWS   Implement systems that are highly available, scalable, and self-healing on the AWS platform   Design, manage, and maintain tools to automate operational processes   The knowledge and skills required at the professional level include the majority of the following AWS and general IT knowledge areas:   Prerequisites     AWS Certified SysOps Administrator – Associate or AWS Certified Developer – Associate   AWS Knowledge     AWS Services: Compute and Network, Storage and CDN, Database, Analytics, Application Services, Deployment, and Management   Minimum of two years hands-on experience with production AWS systems   Effective use of Auto Scaling   Monitoring and logging   AWS security features and best practices   Design of self-healing and fault-tolerant services   Techniques and strategies for maintaining high availability   General IT Knowledge     Networking concepts   Strong system administration (Linux/Unix or Windows)   Strong scripting skillset   Multi-tier architectures: load balancers, caching, web servers, application servers, databases, and networking   Templates and other configurable items to enable automation   Deployment tools and techniques in a distributed environment   Basic monitoring techniques in a dynamic environment   準備 DevOps Engineer Professional 前，必須理解 Continuous Delivery and Process Automation 涵蓋了非常大的比重，考試前我會推薦熟悉以下服務：      CodeDeploy   CloudFormation   OpsWorks   Elastic Beanstalk   CodeCommit   CodeBuild   CodePipeline   Elastic Container Services   同樣推薦大家仔細閱讀過幾個重點服務的 FAQ 常見問答集，因為常見的問題確實就是成為考題的必要。   就我的考試經驗，CloudFormation 和 Elastic Beanstalk 兩個服務有非常大的機會和題目比重。   這張認證最重要的在於在 AWS 上實踐 DevOps 工作和最佳實務，所以會有很多版本控制、自動化測試整合以及 Deployment strategies 等等相關的問題，由於題目都是情境題，會有不同的實踐方式，所以必須要先對於上述 AWS 基本服務有個了解才比較能選擇最佳的實踐方式。   如果對於 AWS 的使用並不熟悉經驗，可以透過 acloud guru 推出 AWS Certified DevOps Engineer - Professional 來快速入門，   能夠有系統性的幫助入門了解 AWS 各式各樣的服務，完整的濃縮。   此外，準備 Professional 有非常重要的一點，我認為務必完整規劃讀書計畫和時間管理。例如，安排一個月後考試，務必每週的目標和 30 天的讀書計畫，積極實踐，利用每天固定觀看 acloud.guru 的 2 至 3 個影音外也需要花點時間閱讀文件和複習。同時，我也十分推薦上 YouTube 挑選對於 AWS DevOps 相關的 re:Invent 、 Tech talk 影音、閱讀 Whitepapers 等增強對於實務上的最佳實踐：      Whitepapers: Introduction to DevOps on AWS   My DevOps professional playlist   還有   考試中建議大家遇到太長或是不懂的題目先做標記跳過，特別是 DevOps 題目非常落落長，十分考驗耐心，平均每個題目的作答時間僅有 1 分鐘，先把握拿到比較容易拿的分數，做過一遍後再回來檢查也會比較沒壓力。   祝大家順利通過認證！   ","categories": [],
        "tags": ["aws","certificate","DevOps"],
        "url": "https://easoncao.com/AWS-Certified-DevOps-Engineer-Professional-Preparation/",
        "teaser": "https://easoncao.com/assets/images/aws-devops-pro-cert.png"
      },{
        "title": "[Book] 原子習慣：細微改變帶來巨大成就的實證法則 - 我的心得及實作",
        "excerpt":"今年 5 月 (May, 2020) 我讀完 安靜的力量 一書就在 Instagram (aka. IG) 上面問問有沒有人可以推薦書籍來一起交換心得，本書是在美國 UC 念研所的高材生 Chia-Tien 的真心推薦，一講完我就刷中文版進到我的 Kindle 書庫了 (Kindle 中文版竟然還比原文便宜！會中文真好 XD)。   原本抱著對未來總有時間的期待能一次把它讀完，無奈生活、工作一被很多事情塞滿 (說白了應該是偷懶，耍廢的優先級總能比閱讀這種太營養的事情來的高 …)，我其實閱讀這本書的時候都是用很碎片、零碎到不行的時間 (睡前讀個 5 分鐘、幾頁之類的 ….)，今天終於有時間一次完結 (算一算也已經八月了 XD)，順便把 Kindle 上面做的筆記、重點一次給它摘要起來。因此，本篇內容主要簡述我對原子習慣 (Atomic Habits) 的讀書心得及自己的實作分享。   前言                     原子習慣: 細微改變帶來巨大成就的實證法則 (Traditional Chinese Edition) - Buy kindle version on Amazon!            概覽   本書著重於習慣的建立，並且歸納出好習慣建立的四大法則。書中舉了非常多各領域透過微小習慣帶來巨大改變的例子，在開頭舉了一個很有趣的案例，是英國自行車選手自從聘用新的國家隊教練 (Dave Brailsford) 透過非常多微小 (小到無法理解的) 的改變，像是：      “重新設計單車座墊，使其坐起來更為舒適”   “輪胎上塗抹酒精，增加抓地力”   “穿上電熱式緊身褲，使肌肉在騎車時維持理想溫度”   “調整賽衣的布料，使選手在比賽時更加輕盈並且符合空氣力學”   “測試按摩油，看哪種能更快幫助肌肉恢復”   “請外科醫師教導選手洗手，以減少感冒機率”   “找出帶來最佳睡眠品質的枕頭與床墊”   等等等等 …. 並且納入選手的訓練中以作為日常習慣的一部分。   甚至是把後勤卡車的內壁刷成純白！以幫助查看到微小的塵埃以避免影響到精準調整的比賽用車！   這些細微的改善，累積起來，竟然大幅度提升英國自行車運動員的運動成績。原本一直吊車尾的英國車隊 (歐洲單車品牌製造商甚至拒絕販賣商品給英國車隊，以免對品牌造成負面影響)，才短短五年，英國自行車隊就在 2008 年北京奧運稱霸，拿下六成金牌，並且更在四年後創下九項奧運紀錄跟七項世界紀錄，聽起來都很不可思議。   以下分享幾項我認為十分有幫助的概念進行摘要。   習慣的複利效應   百分之一的改善並不特別值的注意 (有時候根本不被注意)，但隨著時間過去，微小改善所能造成的變化十分驚人，例如：如果每天都能進步百分之一 (1%)，持續一年，最後會進步 37 倍 [1]。      [1] Note: 我很好奇為什麼是 37 倍，網路上一堆分享似乎都只複製貼上書中的內容但是都沒說說為什麼，終於讓我找出答案是 1.01 ^ 365    相反地，若是每天退步百分之一 (1%)，持續一年，則其會弱化趨近於零 (0.99 ^ 365 ~= 0.03)。   換句話說，習慣就是「自我改善」這件事情的複利，並且效果會在你重複執行的過程中加倍。若隨便挑一天執行習慣的行為來看，其效應似乎很小，然而，一旦經過幾個月、甚至幾年下來，就有可能造成極巨大的影響。                     習慣帶來的複利效應            習慣的建立系統   要在日常生活中體會上述的概念並不容易，並且人的大腦天生設計出來就是通常特別喜歡偷懶、簡單、立即反饋性的事情。這正說明為什麼我們寧可躺在床上滑一小時社群媒體，而不願意走出門外出去跑步一個小時，因為後者相對耗費了更多精力，並且沒有立即性反饋的效應。   滑臉書一個小時，吸收訊息的同時滿足了大腦想接受感興趣訊息刺激的渴望，同時觸發了大腦的愉悅機制，立即被滿足。例如：發佈文章後，很迫切的關注自己的貼文有多少個讚，得到很多個讚後，可以滿足某種層次上想被關注的渴望，於是習慣機制被建立，你不自覺的反覆以下的習慣公式：      自然的拿起手機拍照及錄影 (例如跟朋友吃個飯仍彼此互拍對方及食物，即使在旁人眼裡無法理解，你還是會習慣這麼做) -&gt; 打開社群媒體 App -&gt; 修圖 / 美化內容 -&gt; 發佈文章 / 發佈限時動態 -&gt; 渴望得知獲得的讚數和評價    上述的動作可能短短付出幾分鐘就能得到反饋，這正是為什麼人機介面專家不斷地在努力優化即時反饋功能 (利用通知數及紅色觸發提示)，跟一群資料科學家們，認真的利用機器學習努力的學會識別你感興趣的內容並且嘗試投放你可能會喜歡的資訊，以奪走你的注意力。   反之，出去跑步一個小時，你可能得到的是，身體會非常疲憊，同時，若你剛接觸跑步，你也不可能跑一天就成為跑步大師，短短幾天就能在一小時內跑完 10 公里，甚至從中獲得讚美及成就感滿足你大腦渴望的需求。這個動作的反饋其帶來的成果總是來得不夠快，於是你仍決定回去滑社群軟體一小時而不是踏出門跑步。   你不會因為今天只吃一餐垃圾食物，體重計上的指針就突然大幅度的移動，特別是大腦往往漠視對於當下的單一決定其長期帶來的影響。然而，當日復一日重複百分之一的錯誤，複製不當決策及係為過錯，並且將小藉口合理化，這些小小的選擇隨著複利的威力，變成有害的後果。因此，本書舉了非常多的實例並且具體提出幫助檢視習慣的方式，以了解自己的習慣是否處於正確的軌道，同時提出多項方法，以建立系統化的方式建立習慣。   也許書中內容有一部分為了增加故事性，把很多枝微末節的東西湊起來，以不斷強調微小習慣建立的重要性。但本書核心仍著重於習慣的建立，並且歸納出下列好習慣建立的四大法則：      讓提示顯而易見 (Make it obvious)   讓習慣有吸引力 (Make it attractive)   讓行動輕而易舉 (Make it easy)   讓獎賞令人滿足 (Make it satisfying)   反之，若要學會破除壞習慣，則可以反轉上述法則      讓提示隱而不現 (Make it invisible)   讓習慣毫無吸引 (Make it unattractive)   讓行動困難無比 (Make it hard)   讓後果令人不滿 (Make it unsatisfying)   讓提示顯而易見   行為的改變始於察覺，必須先意識到自己的習慣，才能開始改變它。   我們觀察自己日常的小習慣被觸發的線索 (例如：起床順手拿起手機、肚子餓時就不自覺打開外送 App … etc)，並幫這個習慣評比，同時請考慮最長遠的益處，如果無法決定這個習慣是好還是壞，可以問自己：「這個習慣能幫助我成為我想要成為的那種人嗎？對於我渴望的身份認同，這個習慣是投下同意還是反對票？」。   讓習慣有吸引力   習慣就是多巴胺驅動的回饋迴路。極度容易養成習慣的行為──吸毒、吃垃圾食物、打電動、瀏覽社群媒體──都與較高濃度的多巴胺有關，亦適用於最基本的習慣：飲食、喝水及社交。讓我們採取行動的，是對獎賞的預期。大腦分配給「想要」獎賞的神經迴路 (腦幹、依、腹側被蓋區、背側紋狀體、杏仁核以及部分前額葉皮質)，遠比分配給「喜歡」獎賞的要多。   也許你想要知道最新名人的八卦，但又需要鍛鍊身材，你可以要求自己只能在健身房邊跑步邊看八卦新聞、看實境秀。善用誘惑綑綁，幫助你將獎賞與提示產生連結，以更好地吸引你養成該項習慣。   讓行動輕而易舉   習慣的養成取決於頻率，而非時間，如同身體肌肉會對規律的重量訓練產生反應，大腦的特定區域也會在被使用時增長，在被拋棄時萎縮。每次重複一個行為，你就活化了跟那個習慣有關的神經迴路。   大腦設定會盡可能節省能量，人類的天性仍遵循「最小努力原則」，在兩個類似的選項中抉擇時，人自然傾向選擇花費最少力氣的那個。養成一個習慣所需的能量越少，養成的可能性越高。這意味著，讓習慣簡單到就算沒有意願也會執行至關重要。若能讓執行好習慣更加方便，你就更有可能貫徹。(健身房在上下班路上，你就比較容易去健身，因為順路停下來不會為原本的生活增添太多阻力。要是健身房不在通勤路上，就算只差幾公尺，也會變成像是「特地」去健身，養成固定去健身房的習慣就更加困難。)   讓獎賞令人滿足   生活在現代，你做的許多選擇都不會馬上得到好處。你在工作上表現優異，你會在幾週後收到薪水支票; 今天運動，也許明年就不會過重; 現在開始儲蓄，也許幾十年後就有足夠的錢享受退休。從遠古時期，人類的大腦必須響應立即性的回饋 (吃什麼、在哪裡睡覺、如何躲避獵食者，總是把焦點放在當下或是接近的未來) 這與大腦的設計背道而馳，正是為什麼延遲回饋的習慣特別難容易養成的原因。   因此，習慣必須讓人感受到愉快，我們才有可能持續重複某一行為。   「迴紋針策略」：1993年，加拿大一位股票經紀人，每天早晨，會把兩個罐子放在辦公桌上，一個是空的、另一個則放了 120 個迴紋針。每天一做好開工準備，他就會打一通業務拜訪電話，一旦講完，就立即的把一個迴紋針移動到空罐，重複整個過程。直到所有的迴紋針都被移動到另一個罐子裡。   這種視覺上的測量提供了進步的清楚證據，並且強化行為，且為活動增添一點立即的滿足感。透過追蹤習慣，更利於習慣的養成。   改變習慣最有效的方法，是改變身份認同   大部分人無法堅持一項習慣，通常是因為人們對於習慣的建立，是基於「目標」導向，例如：我必須培養運動的習慣，每週必須跑 10 公里，以幫助我能夠順利在三個月後的路跑活動中順利跑完半程馬拉松。然而，一旦完成該項目標後，原本培養的習慣就難以維持，便很容易復歸 (大腦總會有個聲音：啊！今天休息一下好了，於是在幾天後變成，下個月再跑好了！)，於是習慣的系統瞬間瓦解。   在本書中，特別強調「習慣」這件事，其實就是通往身份轉變的道路。每當你選擇做某個壞習慣，就投了一票給予你想成為的身份 (例如：我選擇每天下班去酒吧來好幾個 shot，這樣我允許我自己成為愛喝酒的人，並且這意味著我願意成為那樣的人。)。因此，想要維持一個習慣，首先，先決定你自己想要成為什麼樣的人：      你想要學習精進自己的習慣？那先改變你自己的身份認同吧！ — 「我是一個終身學習的人」   你想要保持健身的習慣？那先改變你自己的身份認同吧！ — 「我是一個擁有良好運動習慣的人」   你想要保持理財或是存錢的好習慣？那先改變你自己的身份認同吧！ — 「我是一個擁有理財紀律並且是擁有規劃財務習慣的人」   如果你想成為一個健康的人，隨時問問自己：      一個健康的人會怎麼做    這種想法變使得你的大腦在日常生活中進行決策時，不自覺地導入你想養成習慣的相應決策，並且成為你的行為指南：一個健康的人會選擇走路或搭計程車？ 一個健康的人會點墨西哥捲餅或是沙拉？。   擁有上述的自我認同後，便是不斷的透過生活中的各項小勝利來向自己證明 (今天又完成了這個習慣)，永遠將習慣的建立放置焦點在成為某一種人，而非得到某一種成果，如此一來，習慣的建立便輕而易舉。   我的實作成果   先說說我自己的心得，身為一名雲端工程師，工作性質不外乎就是長時間待在電腦前面弄得出神入化，可能一個小時內幫助很多很知名的品牌、線上服務提供非常實用的技術性建議，以處理很多系統故障的狀況、找出客戶開發團隊自己埋的坑，拯救全世界因為網路連結那些孤寂的靈魂和龐大的虛擬需求，但外人完全搞不懂你在衝啥。不管是軟體工程師、系統工程師、維運工程師、各種打雜工程師，總之，不論身在哪個職位、一般人對於工程師的印象，就是又宅又臭，並且讓人感覺除了自己專業上面的事情，其餘事情就是一竅不通。但我就是不甘願這種刻板印象 (沒錯，我就是要打！破！它！)，當然，這些習慣有一部分出於對自己所做事情的喜愛，因為不希望自己分享的內容又淪為空泛的談論，因此，本節以我自己為例，以自身的經驗分享我自己的實作成果，分為兩個部分：馬拉松跑者、大力士計畫   馬拉松跑者之路   我自己的目標，起初是希望擁有一個健康的身體，並且想要學習跨出舒適圈，挑戰自己。於是我在 2018 年底，就開始啟動自己的長跑訓練計畫，目標當然是能靠自己完成跑完半程甚至是全程馬拉松的距離。當然，這都是我在接觸這本書之前就有的計畫，但是真的就是一股腦的亂訓練，也沒有正確的學習培養習慣。在大學時期有時候想到運動就會去跑跑，但是距離上可能 5 公里內就差不多了，而且訓練非常看心情，跑步這件事，常常可以有非常多理由就中斷。這件事情在 2019 年初設下想要跑半馬的目標後，真的就是憑一股傻勁在維持：                                                                                                                                                  2018 - 2019 Jan                                                                                                                                                                                                         2019 May - June                                                                                                                                                                                                         2019 July - Aug                                                                  從上面的紀錄可以看到我的訓練其實是斷斷續續的，當初可能就想週末至少跑一下長距離 10 公里。但是仍缺乏維持的系統。所以可能跑一週，一次一股腦衝個兩天，但下週又總能找到理由休一週。直到接近賽季的時候才認真訓練起來 (我的初半馬在十月 - October, 2019)：                                                                                                                                                  2019 Sep                                                                                                                                                                                                         2019 Dec                                                                  然後我也如期完賽了，初半馬跑出 02:20:46 的成績，10 月跑完直接休快一個月，結果繼續到 2019 年底斷斷續續的在維持幾天：                                                                                          2019 Pocari Sweat Run - 21km - 02:20:46       對於跑步這件事情，起初我的想法是喜歡這項運動，特別是執行長距離跑步時，是一種與自己對話的過程，從中學習沈澱自己的思緒和被工作跟一堆雜事打到紊亂的生活節奏，小小的萌生想成為市民跑者的想法，仍而，對於身份認同仍不夠強烈，所以訓練計畫總是斷斷續續的。   直到 2020 年，開始轉變自己對於跑步的心態 — 「我希望自己老了也可以繼續跑步」，直到閱讀原子習慣後更確切自己的身份認同 — 「不以競賽為目標，我想成為一名馬拉松跑者並且不斷的突破自己，將馬拉松的哲學實踐在我的生活中，學習面對生活的各種挑戰」。於是跑步這件事情在我心中，不再是一個在社交圈中跟別人展示自己又參加了哪些馬拉松，心裡的想法十分單純明確：我就是想學習一直跑下去，並且持續的一點一點突破自己 (耐力多提升個 100 公尺、到終點前不要停下來，試著比昨天努力試著衝點間歇)，於是心態更加輕鬆了，身體修復的速度比不上課表，那就減量、降低強度，下週在試一次！於是，維持這項運動就像是一種樂趣。   基於四項原則，我的實作方式如下：      讓提示顯而易見 (Make it obvious): 例如我設定週跑量要滿足至少 20km，一週執行三天。我的做法是，預先在 Google Calendar 設定好重複性的提醒，如此一來，在特定時間 (比如：週日、週三、週五) 就會擁有這項提示，我不管是前一天檢視待辦或是當天，就會在固定時間接收這項提示已觸發行為。                     提示: 我 Google Calendar 上的跑步提醒項目                    讓習慣有吸引力 (Make it attractive): 這一部分仰賴前面的身份認同，並且受跑步獨處的自由吸引 (以及聽振奮人心的音樂)，另一種輔助方式則是，我會 “想要” 增加自己 Nike Run 的成績紀錄，以及，我會預期自己跑完之後，會發一則廢文到社交動態 (例如：Instagram)，或是預期自己未來藉由報名路跑證明自己可以完賽。因為會 “想要” 上傳社交動態，以展示自己很努力的一面，並且幫助大家建立對我自己想要成為的身份認同並且得到尊重，這使得在習慣觸發之前提供巨大吸引力。       讓行動輕而易舉 (Make it easy)：買了一雙好跑的跑鞋，同時，去住家附近的場所跑步，走個幾步就到！   讓獎賞令人滿足 (Make it satisfying)：透過使用 Nike Run App 和上傳成績到社群軟體，有助於我追蹤習慣並且滿足自己裝 B 的需求。一部分是完成習慣後我會願意獎賞自己特調的運動飲料、氣泡果汁，以補充失去的鹽分跟電解質，這刺激了完成習慣的渴望。並且，我會在 Google Calendar 在當天該項提醒事項標註為已完成。                     獎賞: 在 Google Calendar 上標註完成項目                                                                                                                                                           2020 April - total 34.63 km / 3 runs                                                                                                                                                                                                         2020 May - total 38.16 km / 4 runs                                                                                                                                                                                                         2020 June - total 56.27 km / 7 runs                                                                                                                                                                                                         2020 July - total 142.3 km / 16 runs                                                                  於是系統建立後，月跑量就逐步地趨近穩定，一週時間到，就很自動的離開座位、穿上鞋子、戴上耳機，暖身起跑。我在撰寫這篇內容才發現我七月的月跑量隨著這樣的執行，已經不自覺的突破 100km。當然，習慣的建立，其核心仍在於持續，我當然不會要求自己一定要每個月都要到達這個數字，但仍將跑步視為我生活的一部份，可以很自信的說，這確實是一種習慣。                     Aug 9 - 2020 國家地理路跑 - 21km - 半馬大會成績 02:17:37            大力士計畫   我的健身計畫嚴格說起來是 2020 開始學習執行的，我年初連怎麼操作動作真的完全沒有概念。其實背後有類似前面提及的動機，都是想培養一輩子的習慣，礙於篇幅跟打得有點累，以下就快速分享我個人習慣建立的幾項重要記事：                     2020 Jan 開始學習成為一名健人            大力士習慣系統建立的方式如下：      讓提示顯而易見 (Make it obvious): 如同前面的操作雷同，我一部分使用了 Google Calendar 的提醒標註以提供提示        讓習慣有吸引力 (Make it attractive): 操作與前面雷同，一部分仰賴前面的身份認同，我會 “想要” 增加自己健力三項的成績紀錄、打造更強壯的身體，想要有更強健的肌力，獲得更好的生產力、提升跑步表現。並且，預期自己執行完成後，一樣會發一則廢文到社交動態 (例如：Instagram) 凸顯我的力量巨大無比！！！！因為會 “想要” 上傳社交動態，以展示自己很努力的一面，並且幫助大家建立對我自己想要成為的身份認同並且得到尊重，這使得在習慣觸發之前提供巨大吸引力。       讓行動輕而易舉 (Make it easy)：健身房在辦公室附近、下班想轉換心情去信義區晃晃先順路去摔一下槓 … 等等   讓獎賞令人滿足 (Make it satisfying)：透過使用 Strong (App) 追蹤自己的重量變化，為了幫助自己校正動作，我會一併錄下自己的動作。於是，獎賞系統可以變成：      努力訓練 -&gt; 在 Strong 紀錄自己的重量觀察自己的力量成長 -&gt; 完成後來杯高蛋白乳清補充能量 -&gt; 發佈自己的執行動作到社交動態 -&gt; 追蹤自己的成長    2020 由於疫情爆發，大部分時間還是會 WFH (Work From Home)，去健身房瞬間變成一項成本很高的行為 (從我家搭捷運到原本的健身房快 30 分鐘)。但如同前面提及上述習慣系統的建立 (我在去往健身房的路上也加入了一些習慣綑綁的行為，這項時間變成一項額外的享受)，由於健身對於我來說是一項有十足吸引力的習慣，同時完成訓練後可以立即發佈的相應獎賞機制，利用科技助於我追蹤習慣，並且某種程度上滿足讓其他人認同我身份的渴望，仍有十足動力可以驅使我一週至少三天準時往健身房跑。   Squat: 77kg - (1RM: 90kg)               Deadlift: 119kg - (1RM: 119kg)               Note: 上述節錄自近期 (2020 Aug) 的限時動態，健身運動仍是一項長而持續的累積，基於動作習慣，上面的操作仍有進步空間，還在努力提升動作品質，不完全是正確的動作要領。   總結   原子習慣列舉了眾多淺而易懂的實例，描述在生活中習慣建立的前因後果，包含科學及研究理論的導論，具體提供非常多的實務性建議。本書列舉了習慣系統建立的方法，幫助讀者有效的打造習慣，同時提供了破除壞習慣的持續的導引。同時，我在自己的日常生活中嘗試實踐，其理論某種程度上，得以於實務上被印證，該系統能有效幫助習慣的建立，並且得以持續。   本書仍提及眾多習慣建立的準則和注意事項，細節無法一一列舉，作為習慣導引的工具書，是一本實踐後，值得一再閱讀審視的參考文獻。   相關資源   如果你想獲得更多具體的習慣建立方式，可以透過下列連結取得相關的電子、實體書籍版本                                                                                                                       原子習慣: 細微改變帶來巨大成就的實證法則                                                    數位版售價 $7.84                                                   Buy on Amazon                                        ","categories": [],
        "tags": ["book","reading","habit","atomic","kindle","amazon"],
        "url": "https://easoncao.com/atomic-habits-reading-feedback/",
        "teaser": null
      },{
        "title": "[Alexa] 靠一張嘴巴維運，我是如何成為出一張嘴的雲端工程師 - 使用 Alexa skill 管理 AWS EC2 防火牆 (Security Group) 規則",
        "excerpt":"隨著 COVID-19 疫情大爆發，科技圈再次掀起了一股遠端辦公的趨勢，遠距工作的形式也逐漸改變許多人的工作型態。身為一名工程師，因應疫情，今年也頻繁的在家辦公也好幾個月了 (還好台灣還很安全)，仍有很多與傳統地域限制型態的工作模式有很多不同的地方。   平常在辦公室，由於直接連接公司內部網路，通常都有特定的 IP 區段可以很簡單的掌握白名單，防火牆規則都非常好設置。但是自從開始遠端工作後，家裡的網路都是使用浮動 IP，有時候工程師的惰性驅使，又很懶得連上 VPN 在那邊拿 MFA Key  驗證身份，光是要設定自己 EC2 資源的白名單常常都要改來改去的。與此同時，在日常工作中也偶爾跟同事屁來屁去講一些垃圾話，注意到一些覺得可以進行自動化、增加生產力的一些想法及提案，秉持著發明家的精神，於是以下我就來說說我是如何靠一張嘴巴滿足我的懶惰的。   簡介   在沒有靠嘴巴維運之前，為了要正確的更改防火牆規則讓我能夠在家中工作時能夠連上跳板機器操作，我是這麼做的：      打開 AWS Management Console   輸入帳號密碼   彈出 MFA (Multi-Factor Authentication) 認證 -&gt; 打開手機 App 確認現在的動態碼 -&gt; 輸入動態碼   打開 EC2 Console   選擇 EC2 / Security Group   更新 EC2 規則 (獲取自己的外部 IP 地址更新上去)   上述的動作常常都要花我 3-5 分鐘，有時候手機丟在臥室不在身邊又要走超遠！為了滿足我的懶惰，以下是我目前的工作流程，先來看一段示範影片               上面的動作，只要正確的發出聲音指令後，Alexa Skill 便可以正確地更新相應的 Security Group 規則 (並且需要符合自己定義的 PIN Code，避免所有人都可以任意的發出命令更改)，並且只開放我家中正確對外的特定 IP 地址。如此一來，我就能直接地在家透過 SSH 連接上對應的 EC2 Instance 進行工作。   什麼是 Amazon Alexa ?   大概就像是 Apple 的 Siri、Google 推出的 Google Home …      Amazon Alexa，簡稱 Alexa，是亞馬遜公司推出的一款智能助理，最初用於 Amazon Echo 智能音箱。它具有語音交互、音樂播放、待辦事項列表、鬧鐘、流播播客、播放有聲讀物以及提供天氣，交通，體育和其他實時信息（如新聞）的功能[3]。Alexa還可以將自身用作智慧家庭系統來控制多個智能設備。該產品由Amazon Lab126開發，是一名女性語音助手。用戶可以通過安裝插件（由第三方供應商開發的其他功能）來擴展Alexa功能。       Alexa的大多數設備允許用戶通過一個特定的詞語（如「Alexa」或「Amazon」）來喚醒，剩下的（如IOS和Andriod上的Amazon移動應用與Amazon Dash Wand）則需用戶通過按按鈕來使之進入聆聽模式。一些其他廠商的手機也同樣支持用戶發出一些指令來喚醒屏幕，如「Alexa」或「Alexa wake」。到目前為止，Alexa的交流和應答僅可用英語、德語、法語、義大利語、西班牙語、葡萄牙語、日語和印地語。Alexa在加拿大可用於英語和法語（包括魁北克法語）。    來源: Wikipedia   架構概覽 (Architecture Overview)                     Alexa Skill - Security Group Manager 架構概覽            上述的架構在我看完 Alexa Skills Kit SDK for Python 的範例後，整體的實作到部署我大概花了 3 - 5 小時實作出來。目前含註解整個代碼不超過 300 行，難度並不會太高，以下具體描述一些實作細節。   實作細節   怎麼開發？   照著 Alexa Skills Kit SDK for Python 快速實作了一個可以動的版本。如果你是第一次玩 Alexa Skill，Color Picker 是一個不錯的範例，你可以找到很多不同語言的相應實做。   怎麼上傳到 AWS Lambda   建立完 AWS Lambda Function 後，便可以打包你的程式碼上傳到 Lambda 執行。   一般來說，AWS 文件可能會建議你使用 VirtualEnv 在本機建立 (如同在之前 使用 AWS Lambda 建立 Line bot 中提到的)，但我其實偷了一些懶，直接用 Docker image 加上一行 Docker 命令打包 deployment package (python):   docker run -v \"$PWD\":/var/task \"lambci/lambda:build-python3.6\" /bin/sh -c \"pip install -r requirements.txt -t package/; cp lambda_function.py package/; cd package; zip -r9 lambda.zip .\"   過程中會生成 package/ 目錄，上傳該目錄底下壓縮完的 lambda.zip 即可。   安全性設置 - IAM Policy   我在 Lambda Function 使用的 Execution Role 中定義了下列規則：   {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"UpdateSG\",             \"Effect\": \"Allow\",             \"Action\": [                 \"ec2:RevokeSecurityGroupIngress\",                 \"ec2:AuthorizeSecurityGroupEgress\",                 \"ec2:AuthorizeSecurityGroupIngress\",                 \"ec2:RevokeSecurityGroupEgress\"             ],             \"Resource\": \"arn:aws:ec2:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:security-group/sg-XXXXXXXX\"         },         {             \"Sid\": \"DescribeSG\",             \"Effect\": \"Allow\",             \"Action\": \"ec2:DescribeSecurityGroups\",             \"Resource\": \"*\"         }     ] }   如此一來可以避免我的應用程式未經授權操作其他的 Security Group 資源，並且只允許更改特定 Security Group ID 的相關 入站(Ingres) / 出站(Egress) 規則。   Alexa 如何接受指令？   在 Amazon Alexa Developer Console 中建立完 Alexa Skill 後，可以進行相關的定義。 Alexa 會根據 Interaction Model 設定很多不同的 Intent，每個 Intent 能夠對應的相應操作行為，以下是一個當我想要輸入 PIN code 各種不同的命令範例：                     Alexa Skill - Intent 設置            如此一來，在應用程式中便可以使用動態的名稱 EnterPINCodeIntent 和變數 PIN_CODE 來執行並且獲取一些邏輯。   Lambda 如何更新 Security Group?   當 Alexa 根據觸發 Lambda 的時候，例如：Alexa, update my security group!，根據我的設置，便觸發 “UpdateMySGIntent”，於是在相關的執行邏輯在包含 PIN Code 驗證的情況，可以是以下：   @sb.request_handler(can_handle_func=is_intent_name(\"UpdateMySGIntent\")) def update_my_sg_handler(handler_input):     \"\"\"Check if PIN code is provided in session attributes alues. If provided, then     update security group with invoking source IP address.     If not, then it asks user to provide the PIN code.     \"\"\"     # type: (HandlerInput) -&gt; Response     slots = handler_input.request_envelope.request.intent.slots      if PIN_CODE_SLOT_KEY in handler_input.attributes_manager.session_attributes and handler_input.attributes_manager.session_attributes[PIN_CODE_SLOT_KEY] == pre_defined_pin_code:         speech = (\"PIN code matches, I am updating the security group.\")         handler_input.response_builder.speak(speech)          update_response = update_security_group()          speech = (\"Security group has been updated. {}\").format(update_response)         reprompt = (\"You can ask me your security group setting by saying, \"                     \"what's my security group ?\")     else:         speech = \"You did not correctly specify the PIN code or the PIN code doesn't match\"         reprompt = (\"I'm not sure what your PIN code is, \"                     \"You can say, \"                     \"my PIN code is blah blah blah....\")      handler_input.response_builder.speak(speech).ask(reprompt)     return handler_input.response_builder.response   在 Python 應用程式中，使用 DDNS 解析獲取目前外部 IP 後，更新 Security Group 的行為主要使用了 EC2 - AuthorizeSecurityGroupIngress API 執行了這項操作：   def update_security_group():     ec2 = boto3.resource('ec2', region_name=security_group_region)     security_group = ec2.SecurityGroup(security_group_id)     source_ip = get_source_ip()     cidr_ip = source_ip + '/32'      security_group.authorize_ingress(IpProtocol=\"tcp\",CidrIp=cidr_ip,FromPort=22,ToPort=22)      response = (\"The address {} has been added to the security group {} in region {}\").format(source_ip, security_group_id, security_group_region)      print(response)      return response   Alexa Skill 如何觸發 Lambda?   在 Amazon Alexa Developer Console 中可以設置自定義的 Lambda function endpoint：                     Alexa Skill - 設置 Lambda endpoint            Lambda 如何知道目前的使用者外部來源 IP   根據我的觀察，由於 Alexa 觸發 Lambda Function 時，是由 Voice Server 去戳 Lambda，並且也沒有附帶相關的 IP 訊息。因此，為了達成我的目的，我主要使用了如同架構中描述的方式獲取 DDNS 中的紀錄取得真實外部 IP 地址後，進行更改。   透過路由器韌體通常都支援更改 DDNS 的設定，通常好一點的路由器都支持一些很奇耙的功能，如果你會設定的話可以完成蠻多有趣的事情 (例如：在中華電信提供的 D-Link DSL-7740C 啟用 SNMP)。我的網路環境使用了 D-Link DSL-7740C，透過 D-Link 韌體提供的功能，能夠輕鬆的設定並且直接幫助更新我的 Dynamic DNS record，再由 Lambda Function 邏輯中主動解析獲取。   同理，另一種方式是你可以在你的環境中運行一隻小程式 (agent) 或是透過 Cronjob 定義的 Shell Script，幫助你更新 DNS 紀錄，也是一樣的方法。   若你知道 API Gateway 是什麼的話，也許可以利用 API Gateway 搭建 endpoint 並且在上個動作的階段改用 Custom HTTPS endpoint 觸發，或許在交付客戶端的 Alexa device (比如 Echo dot) 觸發時，是由客戶端主動進行訪問，這種情況下，在附帶的請求訊息中也許能知道相關的 IP，但是我沒試過，如果你試了，也歡迎在底下分享你的發現。   總結   本篇內容簡介了一項使用 Alexa Skill 進行系統維運的實作方法，展示了工程師的懶惰成性，並且分享一項參考架構，提及如何靠一張嘴巴更改防火牆規則，同時提及相關的實作細節。透過聲音命令簡化並且自動化繁瑣的更新步驟，減少了每次花費 3-5 分鐘的操作時間 (操作 10 次省下 30 分鐘、100次省下 300 分鐘 … 請自行誇飾及想像)，幫助提升日常工作中的相應生產力。   若你對於這項實作感到興趣或是有其他建議，也歡迎在底下留言與我分享！  ","categories": [],
        "tags": ["aws","amazon web services","EC2","Elastic Compute Cloud","amazon","alexa","alexa skill","echo","echo dot"],
        "url": "https://easoncao.com/security-group-manager-alexa-skill/",
        "teaser": "https://easoncao.com/assets/images/posts/2020/08/security-group-manager-alexa-skill/security-group-manager-alexa-skill-architecture.png"
      },{
        "title": "[AWS][EKS] Zero downtime deployment(RollingUpdate) when using ALB Ingress Controller on Amazon EKS",
        "excerpt":"This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment and prevent 502 errors.   What’s ALB Ingress Controller   Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer. Therefore, if you would like to expose your container service with Application Load Balancer (ALB) on EKS, it is recommended to integrate with ALB Ingress Controller.   If you don’t know about what is the ALB Ingress Controller, here is an overview diagram to help you catch up:                     How ALB ingress controller works - source               (1) The controller watches for ingress events from the API server.   (2) An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal.   (3) Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.   (4) Listeners are created for every port detailed in your ingress resource annotations.   (5) Rules(ELB Listener Rules) are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.      Note: The new version of AWS ALB Ingress Controller is upcoming, while rename it to be “AWS Load Balancer Controller” with several new features coming out. For more detail, please refer the GitHub project - kubernetes-sigs/aws-alb-ingress-controller    How to deploy Kubernetes with ALB Ingress Controller (AWS Load Balancer Controller)?   Basically, the ALB Ingress Controller will be deployed as a Pod running on your worker node while continously monitor/watch your cluster state. Once there have any request for Ingress  object creation, ALB Ingress Controller will help you to manage and create Application Load Balancer resource. Here is a part of example for v1.1.8 deployment manifest:   apiVersion: apps/v1 kind: Deployment metadata:   labels:     app.kubernetes.io/name: alb-ingress-controller   name: alb-ingress-controller   namespace: kube-system spec:   selector:     matchLabels:       app.kubernetes.io/name: alb-ingress-controller   template:     metadata:       labels:         app.kubernetes.io/name: alb-ingress-controller     spec:       containers:         - name: alb-ingress-controller           args:             # Setting the ingress-class flag below ensures that only ingress resources with the             # annotation kubernetes.io/ingress.class: \"alb\" are respected by the controller. You may             # choose any class you'd like for this controller to respect.             - --ingress-class=alb              # REQUIRED             # Name of your cluster. Used when naming resources created             # by the ALB Ingress Controller, providing distinction between             # clusters.             # - --cluster-name=devCluster              # AWS VPC ID this ingress controller will use to create AWS resources.             # If unspecified, it will be discovered from ec2metadata.             # - --aws-vpc-id=vpc-xxxxxx              # AWS region this ingress controller will operate in.             # If unspecified, it will be discovered from ec2metadata.             # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region             # - --aws-region=us-west-1            image: docker.io/amazon/aws-alb-ingress-controller:v1.1.8       serviceAccountName: alb-ingress-controller   The deployment basically will run a copy of ALB Ingress Controller (pod/alb-ingress-controller-xxxxxxxx-xxxxx) in kube-system:   NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE kube-system   pod/alb-ingress-controller-5fd8d5d894-8kf7z   1/1     Running   0          28s  NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE kube-system   deployment.apps/alb-ingress-controller   1/1     1            1           3m48s   Depends on your environment, suggested installation steps may involve the configuration of IRSA (IAM Role for Service Account) to grant permission for the ALB Ingress Controller Pods in order to interact with AWS resources, so it is recommended to take a look official documentation to help you quickly understand how to install ALB Ingress Controller:      ALB Ingress Controller on Amazon EKS   In addition, the service can be deployed as Ingress Object. For example, if you tried to deploy the simple 2048 application:   kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml   The file 2048-ingress.yaml is mentioning the annotations, spec in format that supported by ALB Ingress Controller can recognize:   apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: \"2048-ingress\"   namespace: \"2048-game\"   annotations:     kubernetes.io/ingress.class: alb     alb.ingress.kubernetes.io/scheme: internet-facing   labels:     app: 2048-ingress spec:   rules:     - http:         paths:           - path: /*             backend:               serviceName: \"service-2048\"               servicePort: 80   The ingress object will construct ELB Listeners according rules and forward the connection to the corresponding backend(serviceName), which match the group of service service-2048, any traffic match the rule /* will be routed to the group of selected Pods. In this case, Pods are exposed on the worker node based on type=NodePort:   Content of the file 2048-service.yaml:   apiVersion: v1 kind: Service metadata:   name: \"service-2048\"   namespace: \"2048-game\" spec:   ports:     - port: 80       targetPort: 80       protocol: TCP   type: NodePort   selector:     app: \"2048\"   So … what’s the problem?   Zero downtime deployment is always a big challenge for DevOps/Operation team for any kind of business. When you adopt the ALB Ingress Controller as a solution to expose your service, it has couple things need to take care due to the behavior of Kubernetes, ALB and ALB Ingress Controller … you need to consider many perspectives. Especially, it can have some issue when you would like to roll out the new deployment for your Pods with ALB Ingress Controller.   Let’s use the 2048 game as example to describe the scenario when you are trying to roll out a new version of your container application. In my environment, I have:      A Kubernetes service service/service-2048 using NodePort to expose the service   The deployment also have 5 copy of Pods for 2048 game, which is my backend application waiting for connections forwarding by Application Load Balancer (ALB)   NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE 2048-game     pod/2048-deployment-58fb66554b-2f748          1/1     Running   0          53s 2048-game     pod/2048-deployment-58fb66554b-4hz5q          1/1     Running   0          53s 2048-game     pod/2048-deployment-58fb66554b-jdfps          1/1     Running   0          53s 2048-game     pod/2048-deployment-58fb66554b-rlpqm          1/1     Running   0          53s 2048-game     pod/2048-deployment-58fb66554b-s492n          1/1     Running   0          53s  NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE 2048-game     service/service-2048   NodePort    10.100.53.119   &lt;none&gt;        80:30337/TCP    52s  NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE 2048-game     deployment.apps/2048-deployment          5/5     5            5           53s   And for sure, once the ALB Ingress Controller correctly set up and provision the ELB resource, the full domain of ELB also will be recorded to the Ingress object:   $ kubectl get ingress -n 2048-game NAME           HOSTS   ADDRESS                                                                      PORTS   AGE 2048-ingress   *       xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com   80      11m   I can use the DNS name as endpoint to visit my container service:   $ curl -s xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com | head &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt;   &lt;meta charset=\"utf-8\"&gt;   &lt;title&gt;2048&lt;/title&gt;    &lt;link href=\"style/main.css\" rel=\"stylesheet\" type=\"text/css\"&gt;   &lt;link rel=\"shortcut icon\" href=\"favicon.ico\"&gt;   ...                     2048 Game deployed with ALB Ingress Controller            Like any kind of container application, as a administrator/SRE (Site Reliability Engineer)/part of operation team or DevOps engineer, the goal and our duty is: we always try to ensure the service can run properly without any issue and without any interruption, especially facing the challenges like: when your developers are saying that “Oh! we need to upgrade/update the application”, “we are going to roll out a bug fix”, “the new feature is going to online”, any service downtime can lead anyone of stakeholders(users, operation team or leadership) unhappy.   I am going to use a simple loop trick to continously access my container service via the endpoint xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com to demonstrate a scenario: This is a popular web service and we always have customer access to it. (Like social media service, bitcoin trading platform or any else, we basically have zero tolerance for any service downtime as it can impact our revenue.), as below:   $ while true;do ./request-my-service.sh; sleep 0.1; done HTTPCode=200_TotalTime=0.010038 HTTPCode=200_TotalTime=0.012131 HTTPCode=200_TotalTime=0.005366 HTTPCode=200_TotalTime=0.010119 HTTPCode=200_TotalTime=0.012066 HTTPCode=200_TotalTime=0.005451 HTTPCode=200_TotalTime=0.010006 HTTPCode=200_TotalTime=0.012084 HTTPCode=200_TotalTime=0.005598 HTTPCode=200_TotalTime=0.010086 HTTPCode=200_TotalTime=0.012162 HTTPCode=200_TotalTime=0.005278 HTTPCode=200_TotalTime=0.010326 HTTPCode=200_TotalTime=0.012193 HTTPCode=200_TotalTime=0.005347 ...   Meanwhile, I am using RollingUpdate strategy in my Kubernetes deployment strategy with maxUnavailable=25%, which means, when Kubernetes need to update or patch(Like update the image or environment variables), the maximum number of unavailable Pods cannot exceed over 25% as well as it ensures that at least 75% of the desired number of Pods are up (only replace 1-2 Pods if I have 5 copies at the same time):   apiVersion: apps/v1 kind: Deployment metadata:   name: 2048-deployment   namespace: 2048-game spec:   ...   selector:     matchLabels:       app: \"2048\"   ...   strategy:     rollingUpdate:       maxSurge: 25%       maxUnavailable: 25%     type: RollingUpdate   Scenario: Rolling the new container image to existing container application with potential service downtime   When rolling the new version of my container application (for example, I update my deployment by replacing the container image with image nginx), there potentially can have a period of time that would get HTTP Status Code 502 error in few hits:                     The HTTP 502 Error response from ELB during the rolling update deployment (instance mode)            By default, ALB Ingress Controller is using instance mode to register targets(Pods) to the ELB Target Group by using worker nodes’ instance ID with exposing NodePort. In this case, the traffic will the Kubernetes networking design to do second tier of transportation according to externalTrafficPolicy defined in Kubernetes Service object (No matter using externalTrafficPolicy=Cluster or externalTrafficPolicy=Local).   Due to the ALB Ingress Controller only care about to register Worker Node to the target group, so if the scenario doesn’t involve the worker node replacement, the case basically have miniumun even no downtime(expect that it is rare to have downtime if the Kubernetes can perfectly handle the traffic forwarding), however, this is not how real world operate, few seconds downtime still can happen potentially due to the workflow below:   This is the general workflow when the client reach out to the service endpoint (ELB) and how was traffic goes   Client ----&gt; ELB ----&gt; Worker Node (iptables) / In this step it might be forwarded to other Worker Node ----&gt; Pod   So, in these cases, you can see the downtime:      (1) The client established the connection with ELB, ELB is trying to but the Worker Node is not ready.   (2) Follow the iptables rules, the traffic forward to the Pod just terminated due to RollingUpdate (Or the Pod just got the reqeust but need to be terminated, it haven’t response back yet, caused the ELB doesn’t get the response from Pod.)   (3) ELB established connection with Worker Node-1, once the packet enter into the Worker Node-1, it follows the iptables then forward it to the Pod running on Worker Node-2 (jump out the current worker node), however, the Worker Node-2 just got terminated due to auto scaling strategy or any replacement due to upgrade, caused the connection lost.   Let’s say if you try to remove the encapsulation layer of the Kubernetes networking design and make thing more easier based on the AWS supported CNI Plugin (Only rely on the ELB to forward the traffic to the Pod directly by using IP mode with annotation setting alb.ingress.kubernetes.io/target-type: ip in my Ingress object), you can see the downtime more obvious when Pod doing RollingUpdate. That’s because not only the problem we mentioned the issues in case (1)/(2)/(3), but also there has different topic on the behavior of ALB Ingress Controller need to be covered if the question comes to zero downtime deployment:   Here is an example by using IP mode (alb.ingress.kubernetes.io/target-type: ip) as resgistration type to route traffic directly to the Pod IP   apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: \"2048-ingress\"   namespace: \"2048-game\"   annotations:     kubernetes.io/ingress.class: alb     alb.ingress.kubernetes.io/scheme: internet-facing     alb.ingress.kubernetes.io/target-type: ip   labels:     app: 2048-ingress spec:   rules:     - http:         paths:           - path: /*             backend:               serviceName: \"service-2048\"               servicePort: 80                     An example when using IP mode in ALB Ingress Controller - Can see my Pods all are registering with Pod owns IP address            Again follow the issue we mentioned (1) (2) (3), when doing the rolling update (I was replacing the image again in IP mode), similar problem can be observed, potentially you can have 10-15 seconds even longer downtime can show up:                     The HTTP 502 Error response from ELB during the rolling update deployment (IP mode)            When Kubernetes is rolling the deployment, in the target group, you can see ALB Ingress Controller was issuing old targets draining process(Old Pods) in the meantime                     Old targets were going to be draining state in target group            However, you still can see HTTP 502/504 errors exceed 3-10 seconds for single requset   HTTPCode=200_TotalTime=0.005413 2048 HTTPCode=200_TotalTime=0.009980 502 Bad Gateway HTTPCode=502_TotalTime=3.076954 2048 HTTPCode=200_TotalTime=0.005700 2048 HTTPCode=200_TotalTime=0.010019 502 Bad Gateway HTTPCode=502_TotalTime=3.081601 2048 HTTPCode=200_TotalTime=0.005527 502 Bad Gateway HTTPCode=502_TotalTime=3.070947 502 Bad Gateway HTTPCode=502_TotalTime=3.187812 504 Gateway Time-out HTTPCode=504_TotalTime=10.006324 Welcome to nginx! HTTPCode=200_TotalTime=0.011838 Welcome to nginx!   The issue and the workflow of ALB Ingress Controller   Let’s use this scenario as it is a edge problem we need to consider for most use case, the issue is that the workflow between the Kubernetes, ALB Ingress Controller and ELB can lead HTTP status code 502/503(5xx) erros during deployment when having Pod termination.   In short, when a pod is being replaced, ALB Ingress Controller registers the new pod in the target group and removes the old Pods. However, at the same time:      For the the new Pods, the target is in initial state, until it pass the defined health check threshold (ALB health check)   For the old Pods is remaining as draining state, until it completes draining action for the in-flight connection, or reaching out the Deregistration delay defined in the target group.   Which result in the service to be unavailable and return HTTP 502.   To better understand that, I made following diagrams, so it might be helpful to you understanding the workflow:   1) In diagram, I used following IP addresses to remark and help you recognize new/old Pods. Here is the initial deployment.      Old Pods: Target-1(Private IP: 10.1.1.1), Target-2(Private IP: 10.2.2.2)   New Pods: Target-3(Private IP: 10.3.3.3), Target-4(Private IP: 10.4.4.4)                     Deployment workflow of ALB Ingress Controller - 1. the initial deployment            2) At this stage, I was doing container image update and start rolling out the new copies of Pods. The ALB Ingress Controller made RegisterTarget API call to ELB on behalf of the Kubernetes.                     Deployment workflow of ALB Ingress Controller - 2. start rolling out the new copies of Pods and ALB Ingress Controller is going to issue RegisterTarget API call            3) Meanwhile, the DeregisterTarget API will be called by ALB Ingress Controller and new targets are in initial state.                     Deployment workflow of ALB Ingress Controller - 3. ALB Ingress Controller start to dereigster old targets on ELB Target Group            4) At this stage, anything could happen to cause service outage. Because the DeregisterTarget API call might take some time to process, but, Kubernetes doesn’t have any design to monitor the current state of the ELB Target Group, it only care about rolling the new version of Pods and terminate old one.   In this case, if the Pod got terminated by Kubernetes but Target-1 or Target-2 are still leaving in the ELB Target Group as Active/Healthy state (It need to wait few seconds to be Unhealthy once it reach out to the threshold of ELB HTTP health check), result in the ELB cannot forward the front-end request to the backend correctly.                     Deployment workflow of ALB Ingress Controller - 4. Note: issue cause by inconsistent state between Kubernetes and ELB            5) ELB received the DeregisterTarget request. So the ELB Target Group will start to perform connection draining(set old targets as draining), and mark the Target-1/Target-2 as draining state, any new connection won’t be routed to these old targets.                     Deployment workflow of ALB Ingress Controller - 5. ELB start to perform connection draining for old targets            6) However, here brings another issue: if the new targets (Target-3 and Target-4) are still working on passing the health check of ELB(Currently those are in Initial state), there has no backend can provide service at this moment, which can cause the ELB only can return HTTTP 5XX status code                     Deployment workflow of ALB Ingress Controller - 6. ELB response HTTP 5XX error due to no healthy targets in can provide service            7) Until the new Pods is in Running state as well as can react the health check reqeust from ELB through HTTP/HTTPS protocol, the ELB end up mark the targets as Active/Healthy and the service become available                     Deployment workflow of ALB Ingress Controller - 7. The service need to wait a period to recover until new targets passed the ELB health check            How to resolve the issue and meet zero-downtime?   As mentioned in the previous workflow, it is required to use several workarounds to ensure the Pod state consistency between ALB, ALB Ingress Controller and Kubernetes. Here are few things you can aware:      Since version v1.1.6, ALB Ingress Controller introduced Pod readiness gates: This feature can monitor the rolling deployment state and trigger the deployment pause due to any unexpected issue(such as: getting timeout error for AWS APIs), which guarantees you always have Pods in the Target Group even having issue on calling ELB APIs when doing rolling update.   Here is an example to add a readiness gate with conditionType: target-health.alb.ingress.k8s.aws/&lt;ingress name&gt;_&lt;service name&gt;_&lt;service port&gt;   (As it might be changed afterward,For more detail, please refer the documentation provided by ALB Ingress Controller (AWS Load Balancer Controller) project on GitHub):   apiVersion: v1 kind: Service metadata:   name: nginx-service spec:   clusterIP: None   ports:   - port: 80     protocol: TCP     targetPort: 80   selector:     app: nginx --- apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: nginx-ingress   annotations:     kubernetes.io/ingress.class: alb     alb.ingress.kubernetes.io/target-type: ip     alb.ingress.kubernetes.io/scheme: internal spec:   rules:     - http:         paths:           - backend:               serviceName: nginx-service               servicePort: 80             path: /* --- apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment spec:   selector:     matchLabels:       app: nginx   replicas: 2   template:     metadata:       labels:         app: nginx     spec:       readinessGates:       - conditionType: target-health.alb.ingress.k8s.aws/nginx-ingress_nginx-service_80       containers:       - name: nginx         image: nginx         ports:         - containerPort: 80      For existing connections(As mentioned in workflow-4), the case is involving the gracefully shutdown/termination handling in Kubernetes. Therefore, it is requires to use the method provided by Kubernetes.   You can use Pod Lifecycle with preStop hook and make some pause(like using sleep command) for Pod termination. This trick ensures ALB can have some time to completely remove old targets on Target Group (It is recommended to adjust longer based on your Deregistration delay):       lifecycle:       preStop:         exec:           command: [\"/bin/sh\", \"-c\", \"sleep 40\"]   terminationGracePeriodSeconds: 70      Note: If a container has a preStop hook configured, that runs before the container enters the Terminated state. Also, if the preStop hook needs longer to complete than the default grace period allows, you must modify terminationGracePeriodSeconds to suit this.    An example to achieve zero downtime when doing rolling update after applying methods above   apiVersion: apps/v1 kind: Deployment metadata:   name: \"2048-deployment\"   namespace: \"2048-game\" spec:   selector:     matchLabels:       app: \"2048\"   replicas: 5   template:     metadata:       labels:         app: \"2048\"     spec:       readinessGates:       - conditionType: target-health.alb.ingress.k8s.aws/2048-ingress_service-2048_80       terminationGracePeriodSeconds: 70       containers:       - image: alexwhen/docker-2048         imagePullPolicy: Always         name: \"2048\"         ports:         - containerPort: 80         lifecycle:           preStop:             exec:               command: [\"/bin/sh\", \"-c\", \"sleep 40\"]   Here is an example after following the practice I was getting a try. The deployment will apply the feature and can see the status of the readiness gates:   $ kubectl get pods -n 2048-game -o wide NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES 2048-deployment-99b6fb474-c97ht   1/1     Running   0          78s   192.168.14.209   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1 2048-deployment-99b6fb474-dcxfs   1/1     Running   0          78s   192.168.31.47    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1 2048-deployment-99b6fb474-kvhhh   1/1     Running   0          54s   192.168.29.6     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1 2048-deployment-99b6fb474-vhjbg   1/1     Running   0          54s   192.168.18.161   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1 2048-deployment-99b6fb474-xfd5q   1/1     Running   0          78s   192.168.16.183   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1   Once rolling the new version of the container image, the deployment goes smoothly and prevent the downtime issue as mentioned in previous paragraphs:                     Zero downtime with ALB Ingress Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update            In my scenario, the Kubernetes need to take at least 40 seconds termination period for single Pod, so the old targets are gradually moved out instead of remove all of them at once within few seconds, until entire target group only exists new targets.   Also, the client can get normal responses from old Pods/existing connection during the deployment:   HTTPCode=200_TotalTime=0.012028 2048 HTTPCode=200_TotalTime=0.005383 2048 HTTPCode=200_TotalTime=0.010174 2048 HTTPCode=200_TotalTime=0.012233 Welcome to nginx! HTTPCode=200_TotalTime=0.007116 2048 HTTPCode=200_TotalTime=0.010090 2048 HTTPCode=200_TotalTime=0.012201 2048 HTTPCode=200_TotalTime=0.005532 2048 HTTPCode=200_TotalTime=0.010107 2048 HTTPCode=200_TotalTime=0.012163 Welcome to nginx! HTTPCode=200_TotalTime=0.005452 Welcome to nginx! HTTPCode=200_TotalTime=0.009950 2048 HTTPCode=200_TotalTime=0.012082 Welcome to nginx! HTTPCode=200_TotalTime=0.005349 2048 HTTPCode=200_TotalTime=0.010142 2048 HTTPCode=200_TotalTime=0.012143 2048 HTTPCode=200_TotalTime=0.005507  ... HTTPCode=200_TotalTime=0.012149 Welcome to nginx! HTTPCode=200_TotalTime=0.005364 Welcome to nginx! HTTPCode=200_TotalTime=0.010021 Welcome to nginx! HTTPCode=200_TotalTime=0.012092 Welcome to nginx! HTTPCode=200_TotalTime=0.005463 Welcome to nginx! HTTPCode=200_TotalTime=0.010136 Welcome to nginx!   This is the practice in case having ALB Ingress Controller for doing graceful deployment with RollingUpdate. However, it is another big topic need to be discussed regarding what type of the application when rolling the update. Because maybe some kind of applications need to establish long connection with the ELB or have requirement for considering persistence data need to be stored on the backend. All these things can bring out other issues we need to talk about.   But in summarize, with the deployment strategy above, it is also recommended to design the client/backend application as stateless, implement retry and fault-tolerance. These mothod usually help to reduce the customer complain and provide better user experience in most common use case.   Conclusion   Due to the current design of Kubernetes, it is involving the state inconsistent issue when you are exposing the service with Application Load Balancer. Therefore, in this article, I mentioned the potential issue when doing rolling update in the scenario having container service integrating with ALB Ingress Controller (AWS Load Balancer Controller).   Even the technology is always in revolution, I am still willing to help people better handle the deployment strategy. I used couple hours to draft this content and tried to cover several major issues, metioned things you might need to aware, break down the entire workflow and shared few practical suggestions that can be achieved in ALB Ingress Controller in order to meet the goal when doing zero downtime deployment.   The article was written based on my working experience (Of course many communications back and forth with different customers using AWS), it might not be perfect but I hope it is helpful to you. For sure, if you find any typo or have any suggestions, please feel free and leave comment below.   References      ALB Ingress Controller on Amazon EKS   Using pod conditions / pod readiness gates   Issue#1124   Issue#814   Issue#1064  ","categories": [],
        "tags": ["aws","amazon web services","EC2","Elastic Compute Cloud","amazon","ELB","ALB","Load Balancer","Elastic Load Balancer","ALB Ingress Controller","Kubernetes","k8s","EKS","Elastic Kubernetes Service","AWS Load Balancer Controller"],
        "url": "https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/",
        "teaser": "https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-4-side-note-of-deregister.png"
      },{
        "title": "CoreDNS(kube-dns) resolution truncation issue on Kubernetes (Amazon EKS)",
        "excerpt":"This article is describing the thing you need to aware for DNS resolution issue can occur on Kubernetes. Especially when your Pod is relying on CoreDNS(kube-dns) to resolve DNS record when connecting to Amazon ElastiCache or target with large response payload, the issue potentially can happen.   Note: To help you understand the detail, I was using Amazon EKS with Kubernetes version 1.5 and CoreDNS (v1.6.6-eksbuild.1) as example.   What’s the problem   In Kubernetes, it provides an extra layer of the DNS resolution so containerized applications running on Kubernetes cluster basically heavily relying on using own DNS service to provide extra beneficial of service discovery by default, which is kube-dns add-ons.   And most common use cases are usually using CoreDNS as the default DNS resolution provider by deploying and running CoreDNS Pods within the cluster.   Containerized applications running as Pod can use autogenerated service name that map to Kubernetes service’s IP(Cluster-IP), such as my-svc.default.cluster.local. It provides more flexibility to Pods to do internal service discovery as they can use the hostname to resolve the record without remembering the private IP address within the application when deploying to other environment. The kube-dns add-ons usually have ability to resolve external DNS record so Pods can also access service on Internet.                     An overview of kube-dns running on Kubernetes - source / Service discovery and DNS (Google Cloud)            Symptom   Basically, everything runs good for general use case and you propely won’t see any issue when running your production workload. But if your application need to resolve some DNS record with large DNS response payload(like sometimes your endpoint of ElastiCache response larger payload), you might notice the issue happen and your application never correctly resolve them.   You can use some simple method by installing dig in your Pod to test and debug if you can see the difference from the result:   # Get it a try to see if the external DNS provider # # Please make sure your Pod should have accessibility to reach out to Internet # Otherwise you have to change 8.8.8.8 as your own private DNS server that Pod can reach out $ dig example.com @8.8.8.8   # Get it a try to see if CoreDNS can correctly resolve the record $ dig example.com @&lt;YOUR_COREDNS_SERVICE_IP&gt; $ dig example.com @&lt;YOUR_COREDNS_PRIVATE_IP&gt;  # Get it a try to see other record is having this issue or not $ dig helloworld.com @&lt;YOUR_COREDNS_PRIVATE_IP&gt;   For example if I have running CoreDNS Pods and kube-dns service in my Kubernetes cluster   NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE     IP              NODE                                              NOMINATED NODE   READINESS GATES kube-system   pod/coredns-9b6bd4456-97l97                   1/1     Running   0          5d3h    192.168.19.7    XXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           &lt;none&gt; kube-system   pod/coredns-9b6bd4456-btqpz                   1/1     Running   0          5d3h    192.168.1.115   XXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           &lt;none&gt;  NAMESPACE     NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE    SELECTOR kube-system   service/kube-dns     ClusterIP   10.100.0.10      &lt;none&gt;        53/UDP,53/TCP   5d3h   k8s-app=kube-dns   I can use commands like below to check which point can cause failure:   dig example.com @10.100.0.10 dig example.com @192.168.19.7 dig example.com @192.168.1.115  dig helloworld.com @10.100.0.10 dig helloworld.com @192.168.19.7 dig helloworld.com @192.168.1.115   If you are getting result which is both are getting failed but other DNS record is working even using CoreDNS as name server. As a DNS resolver(CoreDNS), we can identify something went wrong when CoreDNS is trying to help you forward the DNS quries for some specific target, and this is the issue we would like to discuss. However, If the Private IP of CoreDNS is working but Service IP (Cluster IP) is getting failure, or none of one are successful, you should pilot your investigation target on checking the Kubernetes networking encapsulation, such as: CNI plugin, kube-proxy, cloud provider’s setting or else that can break your Pod-Pod communication or host networking translation.   How to reproduce   In my testing, I was running Kubernetes cluster with Amazon EKS (1.5) with default deployments (such as CoreDNS, AWS CNI Plugin and kube-proxy). The issue can be reproduced when following steps with commands below:   $ kubectl create svc externalname quote-redis-cluster --external-name &lt;MY_DOMAIN&gt; $ kubectl create deployment nginx --image=nginx $ kubectl exec &lt;nginx&gt; $ apt-get update &amp;&amp; apt-get install dnsutils &amp;&amp; nslookup quote-redis-cluster   My deployment with one nginx Pod and two CoreDNS Pods already running within my EKS cluster:   NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE    IP               NODE                                                NOMINATED NODE   READINESS GATES default       pod/nginx-554b9c67f9-w9bb4    1/1     Running   0          40s    192.168.70.172   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           &lt;none&gt; ... kube-system   pod/coredns-9b6bd4456-6q9b5   1/1     Running   0          3d3h   192.168.39.186   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           &lt;none&gt; kube-system   pod/coredns-9b6bd4456-8qgs8   1/1     Running   0          3d3h   192.168.69.216   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           &lt;none&gt;   And here is the problem: when using nslookup to resolve the domain (set the DNS resolver as CoreDNS Pod 192.168.39.186 instead of using Cluster IP of kube-dns, this can exclude any issue can caused by Kubernetes networking based on iptables).   The test result only return the canonical name (CNAME), without IP address (example.com):   $ kubectl exec -it nginx-554b9c67f9-w9bb4 bash root@nginx-554b9c67f9-w9bb4:/# nslookup quote-redis-cluster 192.168.39.186 Server:         192.168.39.186 Address:        192.168.39.186#53  quote-redis-cluster.default.svc.cluster.local   canonical name = example.com.   Note: The domain name example.com basically has normal A record with many IP address. (The real case is that the example.com was an Amazon ElastiCache endpoint, with returning IP addresses of ElastiCache nodes)   However, when testing other domain, basically can successfully return the IP address even using nslookup, in the same container, with same DNS resolver (192.168.39.186):   $ kubectl create svc externalname quote-redis-cluster --external-name success-domain.com  $ kubectl exec -it nginx-554b9c67f9-w9bb4 bash root@nginx-554b9c67f9-w9bb4:/# nslookup my-endpoint 192.168.39.186 Server:         192.168.39.186 Address:        192.168.39.186#53  my-endpoint.default.svc.cluster.local     canonical name = success-domain.com. Name:   success-domain.com. Address: 11.11.11.11 Name:   success-domain.com. Address: 22.22.22.22   And this is the issue I would like to talk about. Let’s break down and understand why having difference in the result.   Deep dive into the root cause   Let’s start to break down what happen inside. To better help you understand what’s going on, it is required to know:      192.168.70.172 (Client - nslookup): The nginx Pod, in this Pod I was running nslookup to test the DNS resolution ability.   192.168.39.186 (CoreDNS): The real private IP of CoreDNS Pod, play as DNS resolver for kube-dns service.   192.168.69.216 (CoreDNS): The real private IP of CoreDNS Pod, play as DNS resolver for kube-dns service.   192.168.0.2 (AmazonProvidedDNS): The default DNS resolver in VPC and can be used by EC2 instances.   On EKS, the DNS resolution flow can be looked like:                     The DNS resolution flow on EKS            First, the Pod will issue a DNS query to CoreDNS. Once the CoreDNS receives the query, it will check if have any cache DNS record exists, otherwise, it would follow the setting mentioning in the configuration to forward the reqeust to upstream DNS resolver.   The following is that CoreDNS will look up the DNS resolver according to the configuration /etc/resolv.conf of CoreDNS node(In my environment it was using AmazonProvidedDNS):   apiVersion: v1 data:   Corefile: |     .:53 {         forward . /etc/resolv.conf         ...     }   Based on the model, responses will follow the flow and send back to the client. The normal case(happy case) is that we always can query DNS A records and can correctly have addresses in every response:                     Normal DNS resolution packet, can see A record responsed(Using nslookup)            Right now, let’s move on taking a look what’s going on regarding the issue:   1) On the client side, I was collecting packet and only can see the response with CNAME record, as we expected on nslookup output:                     The network packet collected from client - only get the response with CNAME even asking for A record.            2) On the CoreDNS node, I also collected the packet and can see:      The CoreDNS Pod was asking the record with AmazonProvidedDNS   In the response, it only have CNAME record   192.168.70.172 -&gt; 192.168.39.186 (Client query A quote-redis-cluster.default.svc.cluster.local to CoreDNS) 192.168.39.186 -&gt; 192.168.0.2    (CoreDNS query A quote-redis-cluster.default.svc.cluster.local to AmazonProvidedDNS) 192.168.0.2    -&gt; 192.168.39.186 (AmazonProvidedDNS response the record, with CNAME) 192.168.39.186 -&gt; 192.168.70.172 (CoreDNS Pod response the CNAME record)                     The network packet collected from client - only get the response with CNAME even asking for A record.            And if look the packet closer collected on CoreDNS node, here is they key point: The truncated flag (TC flag) was true, which means the DNS response is truncated.                     The message is truncated in the DNS query response (CoreDNS Node)            At this stage, the issue can be identified the DNS response was truncated.   Why the DNS response (message) was truncated?   When you see the message truncation, you probably will say: “Holy … sh*t, DNS response is missing, upstream DNS resolver must eat the payload and did not working properly! “. However, the truth is, this is expected behavior according to the design of DNS when it initially comes to first 19s.      DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests. Queries generally consist of a single UDP request from the client followed by a single UDP reply from the server. — wikipeida    The basic payload can be allowed in DNS query generally won’t 512 bytes. It basically can perfectly work in first 19s, however, lately 20s, with the scale of Internet was growing, people start to aware the original design unable to fully satisfy the requirement and they would like to include more information in DNS query(like the usage of IPv4). According to RFC#1123, it initiated the standard in case if a single DNS payload contain over 512 bytes limit for UDP:      It is also clear that some new DNS record types defined in the future will contain information exceeding the 512 byte limit that applies to UDP, and hence will require TCP. Thus, resolvers and name servers should implement TCP services as a backup to UDP today, with the knowledge that they will require the TCP service in the future.    Therefore, in RFC#5596 basically mentioning the behavior of “DNS Transport over TCP”. Solutions can be: use EDNS or retransmit the DNS query over TCP if truncation flag has been set (TC Flag):      In the absence of EDNS0 (Extension Mechanisms for DNS 0), the normal behaviour of any DNS server needing to send a UDP response that would exceed the 512-byte limit is for the server to truncate the response so that it fits within that limit and then set the TC flag in the response header.       When the client receives such a response, it takes the TC flag as an indication that it should retry over TCP instead.    Therefore, when the length of the answer exceeds 512 bytes(The upstream server should set TC flag to tell the downstream the message is truncated), when both client and server support EDNS, larger UDP packets are used including in additional UDP packet. Otherwise, the query is sent again using the Transmission Control Protocol (TCP). TCP is also used for tasks such as zone transfers. Some resolver implementations use TCP for all queries.   How to remedy the issue?   The symptom generally happen when Pods was trying to resolve the DNS record in UDP through CoreDNS. If the domain name contains payload exceeds 512 bytes, it can hit the default limit of UDP DNS query. When the payload over 512 bytes, it is expected to get the response with truncation has been set (TC flag).   However, when receiving payload with TC flag set up and can think of the response is truncated, by default(on Amazon EKS), CoreDNS doesn’t apply the retransmit behavior by using TCP, instead, CoreDNS only response the truncated message and sent it back to the client. This workflow cause the client can aware the DNS query message was missing.   Therefore, to remedy the issue, here are possible solutions can be adopted:   Solution 1: Using EDNS0   In common use case, the default 512 bytes generally can satisfy most usage because we will not expect that our DNS shouldn’t contain too much IP addresses information. However, in some cases, such as:      To balance my workload, I have the resolution will response many targets, because I am using DNS to do round-robin.   It is an endpoint of ElastiCache with many nodes.   Other use case can include long message in DNS response.   If you can expect the DNS response will have larger payload, basically, client still able to send additional EDNS information to increase the buffer size in single UDP response. This still able to ask CoreDNS to forward the additional section using EDNS0.   The workflow can be:      1) Resolve the domain by querying CNAME   2) Once get the mapping domain name, can send another A record query with EDNS option   (You can implement the logic in your own application. Because it has many different way to enable EDNS option, please refer to the documentation provided by your programming language or relevant library.)   Here is an example regarding the output can see when using dig   $ dig -t CNAME my-svc.default.cluster.local ... (Getting canonical name of the record, e.g. example.com) ...  $ dig example.com ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 51739 ;; flags: qr rd ra; QUERY: 1, ANSWER: 22, AUTHORITY: 0, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 7fb5776972bf2aa4 (echoed) ;; QUESTION SECTION: ; example.com. IN A  ;; ANSWER SECTION: example.com. 15 IN A 10.4.85.47 example.com. 15 IN A 10.4.83.252 example.com. 15 IN A 10.4.82.121 ... example.com. 15 IN A 10.4.82.186   In the output, we can see that, by default, the dig will add optional setting to increase allowed payload size in UDP query, in this case it was using 4096 bytes:  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096   Below is a sample packet using dig with EDNS:                     Request with EDNS0 (Query) - Can see the payload size has been set up as 4096 bytes in additional section                              Request with EDNS0 (Response) - Can see the total DNS response size is up to 3296 bytes, which exceeds default 512 bytes limit            As the method generally need to implement in the application. If you would like to increase the default buffer size for every UDP quries sent by CoreDNS, you can use bufsize plugin to limits a requester’s UDP payload size.   Here is an example to enable limiting the buffer size of outgoing query to the resolver (172.31.0.10):   . {     bufsize 512     forward . 172.31.0.10 }   However, it would ask CoreDNS to allow larger payload size in every DNS queries, it may cause DNS vulnerabilities potentially with performance degraded. Please refer to the CoreDNS documentation to get more detail.   Solution 2: Having TCP retransmit when the DNS query was truncated (Recommended)   As mentioned, the normal behaviour of any DNS server needing to send a UDP response that would exceed the 512-byte limit is for the server to truncate the response so that it fits within that limit and then set the TC flag in the response header. When the client receives such a response, it takes the TC flag as an indication that it should retry over TCP instead.   So far, by analyzing the network flow with packets, we can certainly sure CoreDNS doesn’t help us to perform the retransmit over TCP if getting message truncation. So the question is, can we ask the CoreDNS should retry with TCP if getting message is truncated?      And the answer is … YES!.   Since 1.6.2, CoreDNS should support syntax prefer_udp to handle the truncated responses when forwarding DNS queries:   The explanation of the option prefer_udp:     Try first using UDP even when the request comes in over TCP. If response is truncated (TC flag set in response) then do another attempt over TCP. In case if both force_tcp and prefer_udp options specified the force_tcp takes precedence.    Here is an example to add the option for forward plugin:   forward . /etc/resolv.conf {     prefer_udp }   This option generally can be added in your configuration if you can aware a truncated response is received but CoreDNS doesn’t handle it. However, it is still recommended to get it a try and see any performance issue can cause. Consider scale out your CoreDNS Pods in your environment would be helpful if having large scale cluster.   Conclusion   In this article, it dive deep into an DNS resolution issue regarding CoreDNS when running workload on Amazon EKS and break down the detail by inspecting network packet. The root cause relates to the message was truncated due to the response payload size exceeds 512 bytes, which result in the client (Pod) was unable to get the correct result with IP addresses. The response payload will return detail with TC flag set up.   This issue generally related to the design of DNS in UDP and it is expected to get the message truncation if having larger payload. In addition, as mentioned in RFC#1123, the requester should have responsibility for handling the situation if aware that TC flag has been set up. This article was mentioning the detail and the flow according to packet analyzing.   To remedy the problem, this article also mentioned several methods can be applied on client side or CoreDNS, by using EDNS or apply TCP retransmit, both are able to be applied in CoreDNS.   References      CoreDNS Pull Request#3110  ","categories": [],
        "tags": ["aws","amazon web services","amazon","Kubernetes","k8s","EKS","Elastic Kubernetes Service","EDNS","DNS","CoreDNS","kube-dns","RFC"],
        "url": "https://easoncao.com/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/",
        "teaser": "https://easoncao.com/assets/images/posts/2020/10/coredns-resolution-truncation-issue-on-kubernetes-kube-dns/eks-dns-resolution-flow.png"
      },{
        "title": "Run application on EC2 and gather metric to Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)",
        "excerpt":"What’s Amazon Managed Service for Prometheus (Amazon Prometheus / AMP)      Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale. With AMP, you can use the same open-source Prometheus data model and query language that you use today to monitor the performance of your containerized workloads, and also enjoy improved scalability, availability, and security without having to manage the underlying infrastructure. source    There has a good article was dscribing the service feature on AWS Blog Post:      Getting Started with Amazon Managed Service for Prometheus   What if I would like to gather metric without having Kubernetes/ECS cluster?   The most of example was using EKS/ECS or Kubernetes Cluster as example. What if I would like to simply gather metrics for my application and got benefit without managing Prometheus so AWS would ensure the high availability? Generally, the idea is simple, however, it still takes me some times to do some research and get it a try. So it inspired me to write down the detail steps here.   If you would like to , you should have:      Application: Your application should follow the data model that supported by Prometheus (e.g. go_gc_duration_seconds_count 62) and expose your metrics with HTTP server, end with path /metrics.   AWS SigV4 Proxy: By default, when using remote_write supported  The AWS SigV4 Proxy will sign incoming HTTP requests and forward them to the host specified in the Host header.   Standard Prometheus Server (or other agent to gather metrics): The Prometheus Server requires to be installed so it can from your application (/metrics) and ship metric to the URL as specified in remote_write section.   Overview                     Architecture overview               Monitoring Server (let’s say it is EC2 instance A): This instance is running Grafana dashboard to see my metrics   Application Server (let’s say it is EC2 instance B): The instance is running my application and standard Prometheus(gathering metrics)   A Workspaces in my Amazon Prometheus (AMP) in us-east-1: Used to receive metrics and provide consist readable endpoint so Grafana was able to query metrics   Configuration steps   To demonstrate the entire working flow, I am going to use several docker images to quickly show how to do that. If you requires to run standalone application, you can use those Docker images, or build the binary by yourself by referencing the Dockerfile and documentations.      prom/prometheus: Standard Prometheus container image, binaries can be found on official documentation here.   grafana/grafana: Grafana dashboard   prometheus-golang: Sample application from sysdig for Prometheus metrics. (Source)   public.ecr.aws/aws-observability/aws-sigv4-proxy: AWS SigV4 Proxy to sign incoming HTTP requests and forward them   1) Create a workspace in Amazon Prometheus (AMP)   If you got the preview access, the first step is to create a workspace in Amazon Prometheus:                     Create a workspace in AMP            So far it is simple and you can easily click one button to complete the creation. It usually takes few minutes as it was doing provisioning in the backend.   2) Set up IAM user/role permission for your Grafana dashboard   Access to Amazon Managed Service for Prometheus actions and data requires credentials as AMP is using IAM for ensuing the data security. Therefore, it is required to have SigV4/IAM authentication when accessing the endpoint as it provides a consistent query endpoint for your AMP resource once you created a workspace. For example, the AMP resource in us-east-1 region can provide endpoint below:      https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query   When adding the data source in my Grafana dashboard, it needs to follow the authentication model to access the data. To provide the access for my dashboard, in the example, I simply created a IAM user(amp-gra-user) with plain Access Key &amp; Secret Key. To esnure it has permission to access AMP resource, I attached managed policy AmazonPrometheusQueryAccess with following rules to provide readable access:   {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Action\": [                 \"aps:GetLabels\",                 \"aps:GetMetricMetadata\",                 \"aps:GetSeries\",                 \"aps:QueryMetrics\"             ],             \"Effect\": \"Allow\",             \"Resource\": \"*\"         }     ] }                     Create an IAM User for Grafana dashboard            You still can use IAM role/attached EC2 IAM role as Grafana generally can use those permission if enabled loading AWS SDK config, as in my next step.   As the managed policy might be changed once the service is generally available (GA), please refer to the Amazon Prometheus document - IAM permissions and policies to get more detail.   3) Run Grafana dashboard on my monitoring server (EC2 instance A)   This step I used Docker to run my Grafana dashboard. As Amazon Managed Service for Prometheus is integrated with AWS Identity and Access Management (IAM) to ensure that all calls to Prometheus APIs, such as query and ingest, are secured with IAM credentials.      By default, the Prometheus data source in Grafana assumes that Prometheus requires no authentication. To enable Grafana to take advantage of AMP authentication and authorization capabilities, you will need to enable SigV4 authentication support in the Grafana data source. Reference    To enable SigV4 on Grafana, I run my Grafana with the AWS_SDK_LOAD_CONFIG and GF_AUTH_SIGV4_AUTH_ENABLED environment variables set to true. The GF_AUTH_SIGV4_AUTH_ENABLED environment variable overrides the default configuration for Grafana to enable SigV4 support. sigv4_auth_enabled   $ docker run -d \\     -p 3000:3000 \\     --name=grafana \\     -e \"GF_AUTH_SIGV4_AUTH_ENABLED=true\" \\     -e \"AWS_SDK_LOAD_CONFIG=true\" \\     grafana/grafana   Once the Grafana is up and running, the next thing is that go to the AMP console then copy and paste the endpoint URL in your Grafana. You can find section Endpoint - remote write URL &amp; Endpoint - query URL in the console:                     Endpoints in AMP console            The Grafana requires to use query URL to read the metric, so go to the Grafana Dashboard and add a new Data Source. You can find the setting in the left navigation bar:                     Grafana Configurations            Select ‘Prometheus’ as new data source to add, fill the several information as below:                     Grafana add data source               HTTP &gt; URL: fill the endpoint URL you just copied (Endpoint - query URL), please remove the /api/v1/query string that is appended to the URL, because the Prometheus data source will automatically append it. For example:            Endpoint (query URL): https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-XXXXX-XXXX-XXX-XXXX-XXXXXX           Auth &gt; SigV4 auth: Enable the selection   SigV4 Auth Details: Select Authentication Provider (Using Access &amp; Secret key or else). In my testing I was filling Access Key ID &amp; Secret Access Key with the credential of my IAM User (amp-gra-user).   Then click Save &amp; Test, if everything works fine, you will see the message Data source is working:                     Data source is working            If not and shows other error messages, please refer to the following documentation to do troubleshooting:      Set up Grafana open source or Grafana Enterprise for use with AMP - Troubleshooting if Save &amp; Test doesn’t work   At this step we can ensure Grafana can query our metrics on AMP, but there has no data point yet, so the next step is to gather and write metrics to AMP.   4) On application server (EC2 instance B, with private IP address: 172.31.21.133)   I attached an IAM Role to my EC2 instance B with following IAM policy (To write metric, at least to ensure you have aps:RemoteWrite permission, as AmazonPrometheusRemoteWriteAccess), this can allow AWS SigV4 proxy can use the permission and writes metrics to AMP:   {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"aps:RemoteWrite\",                 \"aps:QueryMetrics\",                 \"aps:GetSeries\",                 \"aps:GetLabels\",                 \"aps:GetMetricMetadata\"             ],             \"Resource\": \"*\"         }     ] }   When your Prometheus server is ingesting metrics on your EC2 instance B, to secure the ingestion, it is requires to use SigV4 authentication when writing metrics to AMP. However, the Prometheus server generally only can use basic auth to do Authorization for remote_write. Therefore, it is required to run a SigV4 proxy to provide access on port 8005 when forwarding remote write traffic.   $ docker run -d -p 8005:8005 public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 --name aps --region us-east-1 --host aps-workspaces.us-east-1.amazonaws.com --port :8005   So the proxy will provide service on port 8005 and can be accessed via localhost:8005 or 172.31.21.133:8005.   Note: If you are not running resource in us-east-1 region, replace option --region us-east-1 and --host aps-workspaces.us-east-1.amazonaws.com as your own.   Once the SigV4 proxy is up and running, I am going to run a sample Go applcation:   $ git clone https://github.com/sysdiglabs/custom-metrics-examples $ docker build custom-metrics-examples/prometheus/golang -t prometheus-golang $ docker run -d --rm --name prometheus-golang -p 80:8080 prometheus-golang   The Go application will expose the metric with path/metrics, so I can query them if using curl:   $ curl http://localhost/metrics | head  # HELP go_gc_duration_seconds A summary of the GC invocation durations.-     0 # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 2.9336e-05 go_gc_duration_seconds{quantile=\"0.25\"} 3.1688e-05 go_gc_duration_seconds{quantile=\"0.5\"} 3.459e-05 go_gc_duration_seconds{quantile=\"0.75\"} 4.1515e-05 go_gc_duration_seconds{quantile=\"1\"} 8.7687e-05 go_gc_duration_seconds_sum 0.00251097 go_gc_duration_seconds_count 62 ...   5) Set up the Prometheus server to collect metrics   I created a configuration for Prometheus and scrape metric for Prometheus server itself (http://localhost:9090/metrics) and my Go application (Go application will expose metric through 172.31.21.133:80/metrics):      remote_write: The section below can send metric to the remote endpoint. In this case, the url need to be specified through SigV4 proxy.   [conf.yml]   #global config global:   scrape_interval:     15s   evaluation_interval: 5s   scrape_timeout:     10s   external_labels:     monitor: 'monitor'  # Scrape configs only contain one scrape target scrape_configs:   - job_name: 'prometheus'     # Override the global default and scrape targets from this job every 5 seconds.     scrape_interval: 5s     static_configs:       - targets: ['localhost:9090', '172.31.21.133:80'] remote_write:   - url: 'http://172.31.21.133:8005/workspaces/ws-XXXXXX-XXXX-XXX-XXXX-XXXXXXX/api/v1/remote_write'   Then, start the Prometheus server to scrape metric:   $ docker run \\      -p 9090:9090 \\      -v $PWD/conf.yml:/etc/prometheus/prometheus.yml \\      prom/prometheus   Once the Prometheus server is up and running, you would expect to see targets and know the health status:                     Prometheus targets            5) View the metric !   Right now, if the Prometheus server can correctly write metric through AWS SigV4 Proxy, you would expect to view the metrics on Grafana dashboard:                     View collected metrics on AMP in Grafana dashboard            Troubleshooting   How to use curl to query my Amazon Prometheus to check if the connectivity, or, IAM authentication is working or not?   To test the Sigv4, you can run the proxy and test the function with curl command. If you also would like to specify the IAM credential, choose either one of method to test the proxy and see if it can route the traffic for you:   # Env vars $ docker run --rm -ti \\   -e 'AWS_ACCESS_KEY_ID=&lt;YOUR ACCESS KEY ID&gt;' \\   -e 'AWS_SECRET_ACCESS_KEY=&lt;YOUR SECRET ACCESS KEY&gt;' \\   -p 8005:8005 \\   public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 --name aps --region us-east-1 --host aps-workspaces.us-east-1.amazonaws.com --port :8005  # Shared Credentials $ docker run --rm -ti \\   -v ~/.aws:/root/.aws \\   -p 8005:8005 \\   -e 'AWS_PROFILE=&lt;SOME PROFILE&gt;' \\   public.ecr.aws/aws-observability/aws-sigv4-proxy:1.0 --name aps --region us-east-1 --host aps-workspaces.us-east-1.amazonaws.com --port :8005   Once the aws-sigv4-proxy is up and running, you can simply to use curl to test the local endpoint:      An example with normal request but did not have query:   $ curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query -vvv  *   Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8005 (#0) &gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query HTTP/1.1 &gt; Host: localhost:8005 &gt; User-Agent: curl/7.53.1 &gt; Accept: */* &gt; &lt; HTTP/1.1 400 Bad Request &lt; Content-Type: application/json &lt; Date: Tue, 09 Feb 2021 15:02:56 GMT &lt; Server: amazon &lt; X-Amzn-Requestid: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX &lt; Content-Length: 125 &lt; * Connection #0 to host localhost left intact {\"status\":\"error\",\"errorType\":\"bad_data\",\"error\":\"invalid parameter 'query': 1:1: parse error: no expression found in input\"}      An example with normal request by feeding query according to HTTP API:   $ curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query?query=up -vvv  *   Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8005 (#0) &gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query?query=up HTTP/1.1 &gt; Host: localhost:8005 &gt; User-Agent: curl/7.53.1 &gt; Accept: */* &gt; &lt; HTTP/1.1 200 OK &lt; Content-Type: application/json &lt; Date: Tue, 09 Feb 2021 15:10:49 GMT &lt; Server: amazon &lt; X-Amzn-Requestid: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX &lt; Content-Length: 63 &lt; * Connection #0 to host localhost left intact {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[]}}      An example with failed response due to lack of IAM permission(aps:QueryMetrics) for my IAM user/role:   $ curl http://localhost:8005/workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query -vvv  *   Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8005 (#0) &gt; GET /workspaces/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/api/v1/query HTTP/1.1 &gt; Host: localhost:8005 &gt; User-Agent: curl/7.53.1 &gt; Accept: */* &gt; &lt; HTTP/1.1 403 Forbidden &lt; Content-Length: 200 &lt; Content-Type: application/json &lt; Date: Tue, 09 Feb 2021 15:07:55 GMT &lt; Server: amazon &lt; X-Amzn-Errortype: AccessDeniedException &lt; X-Amzn-Requestid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx &lt; X-Amzn-Requestid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx &lt; * Connection #0 to host localhost left intact {\"Message\":\"User: arn:aws:iam::111111111111:user/myIAMUser is not authorized to perform: aps:QueryMetrics on resource: arn:aws:aps:us-east-1:111111111111:workspace/ws-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"}   Conclusion   In this article, it shows a example to use Amazon Managed Service for Prometheus (AMP) to gather metrics when running standalone application on EC2 instance, rather than having Kubernetes to deploy. As it might be difficult to understand what things need to be noticed, so I shared configuration and steps. Giving the overview and share some tips you have to know if you are trying to push metrics to AMP.   References      Demo Video - Amazon Managed Service for Prometheus (AMP) &amp; Amazon Managed Service for Grafana (AMG)   AWS SigV4 Proxy   Prometheus configuration  ","categories": [],
        "tags": ["aws","amazon web services","amazon","Prometheus","AMP","Amazon Managed Service for Prometheus","EC2","Elastic Compute Cloud"],
        "url": "https://easoncao.com/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/run-app-on-ec2-and-gather-metric-to-amazon-prometheus-amp/ec2-with-amp-overview.png"
      },{
        "title": "身為 DevOps 你會想知道的 AWS 技巧 - 使用 AWS Lambda 和 Amazon SNS 取得來自 AWS CodeCommit (Git) 的檔案變更通知",
        "excerpt":"這篇內容主要是轉載我 2019 年 2 月公開發佈在 AWS 官方 DevOps 部落格的內容，在這篇內容中，主要展示了如何以 AWS Lambda 及 Amazon SNS 接收來自 AWS CodeCommit (Git Repository) 的檔案變更通知。由於屆時將滿 2 年 (時間過得真快)，覺得有必要翻譯成中文文件，以幫助中文的讀者也能夠了解這項內容。   由於原內容為英文，如果有興趣，原文請見：      Using AWS Lambda and Amazon SNS to Get File Change Notifications from AWS CodeCommit。   簡介   通知一直是 DevOps 工作流程中很重要的一環，幾乎很多牽扯維運相關的工作都少不了主動通知的行為。當然，你可以在任何 CI/CD 的階段中透過你已知的方法於任何邏輯中設置。但在這篇 Blog Post 中，我將會展示如何整合 AWS Lambda 和 Amazon SNS 以擴展 AWS CodeCommit 的功能性。特別的是，這篇解決方內容案描述了當 AWS CodeCommit 一旦有任何更新，如何從 Amazon SNS 中接收檔案變更和 Commit 訊息。以下主要簡介相應使用到的 AWS 服務：           Amazon SNS: Amazon Simple Notification Service (Amazon SNS) 是一項全受管簡訊服務，並且能夠幫助你發佈訊息至訂閱者。其非常容易使用並且支援任何規模大小的使用場景。            AWS Lambda: 是一種無伺服器的運算服務，可讓您執行程式但不必佈建或管理伺服器、建立工作負載感知叢集擴展邏輯、維護事件整合或管理執行階段。使用 Lambda，您可以透過虛擬方式執行任何類型的應用程式或後端服務，全部無需管理。在這篇內容中，我使用了 Lambda Function 以推送訊息至 Amazon SNS 以發佈檔案更新。 Amazon CloudWatch            Amazon CloudWatch 提供資料和可行的洞見以監控應用程式、回應整個系統的效能變化、優化資源使用情況，以及透過整合的檢視來查看運作狀態。你可以也設定簡易的規則來偵測 AWS 資源的變更。在 CloudWatch 捕捉到來自你 AWS 資源的事件更新後，便能觸發特定的目標執行相應的操作 (例如：觸發一個 Lambda Function)       為幫助你快入了解並且部署這項解決方案，我同時建立了一個 AWS CloudFormation template 以供這篇內容使用。AWS CloudFormation 是一個管理工具並且能夠使用通用的語法幫助你描述並且部署 AWS 相關的基礎建設和資源。   概覽 (Overview)                     架構概覽            AWS CodeCommit 支持了多項實用的 CloudWatch Event，透過 CloudWatch Event，這能夠幫助你監控 AWS 資源的使用事件變更並且進行通知。透過設定一些簡單的觸發規則，你便能夠偵測有關 branch 或是 Repository 的變更。   在這個範例中，我為一個 AWS CodeCommit Repository 建立了一個 CloudWatch event rule (事件規則)，如此一來，任何相應的變更事件便會觸發一個 Lambda Function。當對於 CodeCommit 的變更執行時，CloudWatch 便偵測該事件並且執行自定義 Lambda Function 的觸發。   當這個 Lambda Function 被觸發，下列的行為將被依序執行：      使用 CodeCommit API 中的 GetCommit 操作取得最後一次的 Commit 紀錄。因為我想要比較原先上一次的 Commit ID 和最新的一筆，以進行變更的檢查。   對每一個 Commit 紀錄，使用 GetDifferences API 操作取得任何紀錄追蹤檔案的新增、變更和刪除資訊。   從比較的結果中，合併相關的變更資訊，並且，將這項資訊依照定義好的 Email 訊息格式推送到 Lambda Function 中環境變數中定義的 Amazon SNS (SNS topic) 資源。   允許 Reviewers (可能是 Code Reviewers, operation team … 等) 訂閱該 SNS Topic。如此一來，任何有關 CodeCommit 相應的更新都能推播到相關的訂閱者。   這個範例使用了 Python 和 Boto3 實作這項功能。完整的程式碼已被公開在 GitHub 上，你可以在 AWS 官方的 GitHub 上找到 - aws-codecommit-file-change-publisher 該範例。   開始動手做   為了幫助你快速搭建這項解決方案，該專案包含了一個 AWS CloudFormation template (codecommit-sns-publisher.yml)。這個 template 使用了 AWS Serverless Application Model (AWS SAM) 用以定義 CodeCommit 通知 Serverless 應用程式必須的組建，並且使用簡潔的語法表示。   一旦部署後，這個 template 會被直接轉譯成 AWS CloudFormation stack 資源並且建立一個 SNS Topic, CloudWatch event rule 和一個 Lambda Function。這個 Lambda Function 程式碼已經展示了一個簡易的通知功能用例。你可以使用這項範例程式自行定義你的邏輯，甚至是使用其他 AWS SDK for Python (Boto3) 提供的 API 和方法擴展功能。   預先準備工作   在開始部署該範例之前，你必須先建立/擁有一個 CodeCommit repository，以便後續使用 AWS CloudFormation template 執行相應的操作。在這篇文章中，我在 Ohio 區域 (us-east-2) 建立了一個乾淨的 CodeCommit Repository (sample-repo) 以便展示一個 CodeCommit Repository 在特定 branch 上有檔案修改更新。   如果你已經擁有一個 CodeCommit Repository，你可以繼續往下閱讀下列步驟以部署 template 和 Lambda Function。   部署 AWS CloudFormation template 及 Lambda function           下載 aws-codecommit-file-change-publisher 上的原始碼            登入至 AWS Console 及 選擇你 CodeCommit Repository 所在的區域。並且，手動建立一個 S3 Bucket 並且上傳 AWS Lambda deployment package (封裝好的 zip 文件 - codecommit-sns-publisher.zip)。如果你不確定如何建立 S3 Bucket，請參考 Amazon S3 Console 的操作手冊 - How Do I Create an S3 Bucket? 以引導你完成            上傳 Lambda deployment package 至你剛剛建立好的 S3 Bucket       在這個範例中，我在相同區域 (Ohio, us-east-2) 建立了一個 S3 Bucket 名為 codecommit-sns-publisher 並且透過 Amazon S3 上傳 Lambda deployment package：                     上傳 Lambda deployment package 至 S3                    在 AWS Management Console, 選擇 CloudFormation 導引到該服務。你也可以直接使用這個連結訪問 - https://console.aws.amazon.com/cloudformation            選擇 Create Stack            在 Select template 頁面，選擇 Upload a template to Amazon S3，指定剛剛下載的 codecommit-sns-publisher.yml template 檔案上傳進行下一步                         選擇 CloudFormation template               在指定參數項目中，填寫以下資訊：            Stack Name: codecommit-sns-publisher (你可以指定自己的名稱)       CodeS3BucketLocation: codecommit-sns-publisher (指定你剛剛在建立 S3 Bucket 步驟中上傳 Lambda deployment package 的 S3 Bucket 名稱)       CodeS3KeyLocation: codecommit-sns-publisher.zip (這是上傳 Lambda deployment package 的名稱, 物件應為 zip 檔案)       CodeCommitRepo: sample-repo (你 CodeCommit repository 的名稱)       MainBranchName: master (指定你想觸發事件的 branch 名稱)       NotificationEmailAddress: user@example.com (指定要訂閱 SNS topic 的 Email 地址, 這個設置可以讓 CloudFormation template 建立一個 SNS topic 以推送通至訂閱者)                             指定參數 Parameters                    選擇 Next            在 Review 頁面中，於 Capabilities 項目底下，勾選以下選項：             I acknowledge that AWS CloudFormation might create IAM resources.       I acknowledge that AWS CloudFormation might create IAM resources with custom names.                             選擇 Capabilities               在 Transforms 項目，點擊 Create Change Set。AWS CloudFormation 便會開始執行 template 轉譯的工作並且建立一個 Change Set                     建立 Change Set               在完成轉譯後，選擇 Execute 以建立 AWS CloudFormation stack                     執行建立 CloudFormation Stack            在 Stack 建立完成後，若資源有正確建立，你應該可以預期會在信箱收到 SNS 訂閱確認信，請點擊確認，看到以下內容即成功訂閱：                     SNS 訂閱確認成功訊息            在你訂閱了 SNS Topic 之後，你便可以在 AWS CloudFormation Console 檢查對應建立出來的資源。如果你想要監控 Lambda Function 的執行狀態，點擊 Resources 可以開啟 Lambda Function(SNSPublisherFunction)：                     CloudFormation 部署的資源            現在，你可以在本機嘗試推送一個 Commit 至遠端的 AWS CodeCommit Repository   Step 1. 下載你的 (git clone) CodeCommit repository 至你的本機電腦。更多有關連接到 AWS CodeCommit 和驗證的資訊，請參考 AWS CodeCommit 使用手冊中的 Connect to an AWS CodeCommit Repository 內容幫助你設定。在這個範例中，展示了如何下載位於 Ohio 區域 (us-east-2) 名為 sample-repo 的 repository：   git clone ssh://git-codecommit.us-east-2.amazonaws.com/v1/repos/sample-repo   Step 2. 進入至該專案目錄並且見一粒一個純問自檔案   cd sample-repo/ echo 'This is a sample file' &gt; newfile   Step 3. 新增一個 Commit 並且紀錄這次的修改   git add newfile git commit -m 'Create initial file'   [輸出]   [master (root-commit) 810d192] Create initial file 1 file changed, 1 insertion(+) create mode 100644 newfile   Step 4. 推送到遠端的 CodeCommit Repository   git push -u origin master:master   [輸出]   Counting objects: 100% (3/3), done. Writing objects: 100% (3/3), 235 bytes | 235.00 KiB/s, done. … * [new branch]      master -&gt; master Branch 'master' set up to track remote branch 'master' from 'origin'.   在本機的 Commit 更改推送到遠端 CodeCommit Repository 後，將會觸發 CloudWatch event 並且偵測到這次的更新。你通常可以在訂閱的 Email 帳戶中預期看到以下的通知訊息：   Commit ID: &lt;Commit ID&gt; author: [YourName] (YourEmail@example.com) - &lt;Timestamp&gt; +0000 message: Create initial file  File: newfile Addition - Blob ID: &lt;Blob ID&gt;   總結   在這篇內容中，我展示了如何使用 AWS CloudFormation template 快速部署一個範例的解決方案，能夠幫助你的維運或是開發團隊追蹤任何有關 CodeCommit Repository 的更新。本篇示例的 CloudFormation template 及 Lambda Function 同時也在 AWS 官方的 GitHub 上被公開 - aws-codecommit-file-change-publisher。你可以依據你的需求參考這個範例程式幫助你自定義 Email 的內容 (例如加上 HTML 樣式)，亦或者是新增其他有用的訊息至你的 Email 訊息中。   若你對於這項實作感到興趣或是有其他建議，也歡迎在底下留言與我分享。當然，如果有任何關於開源專案的反饋，也歡迎開啟 GitHub issue 甚至是開啟 GitHub pull request 貢獻！  ","categories": [],
        "tags": ["aws","amazon web services","Lambda","Lambda Function","severless","CodeCommit","AWS CodeCommit","SNS","Amazon SNS","Git"],
        "url": "https://easoncao.com/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/using-aws-lambda-and-amazon-sns-to-get-file-change-notifications-from-aws-codecommit/architecture.png"
      },{
        "title": "我是如何在還沒畢業就錄取並進入到 Amazon 工作",
        "excerpt":"很多人好奇我是如何進到 Amazon 工作，並且在還沒畢業前就拿到 offer。今年 (2021) 是我加入 Amazon 這家公司的第三年 (把當 Intern 跟當兵的時間也算進去的話 XD)，所以一次把故事寫在這裡。                     我是如何在還沒畢業就錄取並進入到 Amazon 工作            人生第一份履歷   2016 年底，大三，我投遞了人生第一份履歷。其實在前面我做了一些研究，當時的心態是希望能夠跳脫校園舒適的環境，並至少在畢業前有一些實務的經驗 (感受職場的殘酷)，並且實際了解產業的運作。(結果畢業了更想回去學校唸書)   當時我應徵了下列的職缺：      Software Development Intern (The News Lens)   Cloud Support Engineer Intern (Amazon Web Services)   為什麼是這兩個，第一家原因，是因為當時也在媒體紛亂的時代，使得我對於獨立媒體產業很有興趣；並且創辦人 Joey Chung 當時也有來到學校分享 (不過那個講座超少人參加)。想要成為改變媒體亂象的念頭深植我心，加上更了解企業運作跟文化後，我才知道原來 The News Lens 是一家很有個性的新創公司，當時打死都不接受商業媒體贊助，竟然還能活得這麼好，因此一直深感在這樣的公司裡面工作應該能學習到很多。   點燃實際跨出找實習的契機，當時我在講座對 Joey Chung 問了一個問題 (具體內容我忘記了)，大意就是不太確定如何培養實務經驗跟具備企業所需要的能力，我獲得簡潔有力的回答：實際去 Internship。   他也說到：      Harvard 在你一開始入學時就將所有 tutions / fees 各項雜支計算給你，因此你必須很清楚你來學校的目的，並且努力善用你擁有的資源。     (現在想想真的是廢話，但當時真的是沒有那麼宏觀的格局，所以仍然十分感激這樣寶貴的建議)    加上學校對於畢業設立需要到業界實習的門檻 (真是優良的傳統)，這要求了每個學生在畢業前一定要去業界實習，並且透過老闆給的考核給予學分。雖然我當時找的時間點有點早 (同學們都還在煩惱中午吃什麼)，再加上大三又是課業最沈重的一年，雖然有點累，但想想仍其實利大於弊。因此待一切時機成熟，留意到有實習的職缺後，一個不猶豫有機會就投遞試試。   我現在努力回想只記得去 The News Lens 面試軟體開發實習生時，問了幾個問題：      過去用 PHP 做了哪些專案？   你解決過最難的技術問題是什麼，你如何解決？   我當時臨時只想到我在寫 PHP 專案時為了要設計選單式目錄，做了一些研究並想了怎麼規劃資料表，最後用遞迴的方式列舉出來覺得很酷。   總之最後，獲得感謝信一封，我很慶幸 The News Lens 當時沒錄取我，讓我連猶豫的機會都沒有。   差點錯過的機會   身為一名在資訊圈打混的人，多少都知道 Amazon Web Services (AWS) 就是一個聽起來就很炫砲的技術，會用 AWS 就像是站在技術之巔一樣。   當時，AWS 第一次在台灣開啟校園招聘計劃，我也有注意到 AWS 在學校發布對應的招聘訊息。我看了招聘訊息就真的是超級熱血沸騰，即使實習職缺的說明會地點在台大，我都覺得想去試試。   當時十分想去，但遺憾的是，時間點跟學校的課程衝突。而且，最重要的，招聘資訊上也沒有可以投履歷的窗口 (Email) ！！！。   避免你覺得我在唬爛，我還有留當時招聘的文宣 (可見當時有多氣憤，可…可惡)：                     AWS 第一次校園招聘與差點錯過的機會            結果就以為這麼沒了。   (是不是看完，沒有看到 Email？我還在想會不會藏第一題測驗問題，在圖片藏什麼解碼問題，還試著用純文字編輯器打開看看有沒有什麼有趣的地方。)   陰錯陽差的面試   結果好巧不巧，當時招聘的團隊裡，有畢業的 Eefy 學長，幾天後在 Facebook 放了相同的招聘訊息。於是我二話不說，找了機會搭上線，看能不能投遞履歷。學長也很熱心的幫我把履歷轉給 HR (真的十分感激！)。   約莫 1-2 週，我就收到一封信，裡面包涵了幾個線上測驗的問題。題目類型是情境題，給了一組 IAM 帳戶 (AWS 的帳戶) 要你登入進去 AWS Console 搭建環境。但說實話當時我連 AWS 的帳號都沒有，那次是我第一次使用 AWS 服務，更別提什麼 VPC。   所以我也是在期中考那週，花幾個小時，研究並把環境搭建起來，並且使用我人生第一個 Load Balancer (ELB)。不過由於過去就有管理過機器，使用過像是 Linode、虛擬主機，設定 Linux，所以對我來說學習的門檻並不會很高。還記得當時秉持著接觸新知的心情在研究文件上的資料，反覆檢查自己的設定符不符合題目設計的要求，搞得比自己的期中考還緊張。   再過了幾個星期，我就收到 HR 邀請安排面試的電話跟信件 (還是從北京發來的)！當時掛完電話收到 Email，就得知我即將迎來至少三關的面試。當時還很猶豫要怎麼穿面試服裝，最後還特地穿了個白襯衫加皮鞋。   因為實在是太興奮 (OMG 我要在世界前幾大的公司面試啊！)，提早了快 30 分鐘到。那時候 Amazon 剛在台灣進駐，連辦公室都沒有，所以那時還是在共享辦公室安排面試。我還記得我到指定地點，還以為自己被詐騙 (因為混著不同公司的人，沒有人知道 Amazon 的位置在哪)，直搗前台獲得引導才放下心中的忐忑。   三關的面試，其中包含了兩關的與 Technical Interviewer 的技術面試，以及一關 Hiring Manager 的面試。   技術面試問了很多 Linux 及網路相關的問題，主要目的都是測試對於這些知識的掌握程度。面試問題，說實話真的是很硬 (考官臉也很硬)。當時 Linux 的考官 (也是我現在的老闆)，剛見到面的氣場就完全鎮壓，面試從來不告訴你對還是不對，就是一直瘋狂猛問。劈頭開場就快把 Linux 運作原理挖透了 (我的知識也快被掏空)。   網路相關的問題，則問了很多基本網路概論，聊著聊著還分享了自己過去學習這些知識點的經歷、學習維運系統會做的哪些事情、自己當時系統因為用的套件有漏洞被 Hack 中間學到了哪些事情 (怎麼發現被 inject javascript、被放 DoS 的 PHP 程式還研究了一下樣本… etc)，當時大概就是說著說著眼裡都可以閃爍著光芒的那種程度。當時就只有一種想法，很慶幸能有機會與前輩交流，然後感受到在外商工作的人英文真的是很會，知道自己還有什麼樣的差距。   面試到後面，主管進來就說前面的 Interviewer 評價給得還不錯，有打算想先給我個口頭 offer。所以你要說我僥倖錄取，也許吧，就這麼剛好能多少回答面試官的問題，上至基本開發到 Linux、網路，下聊到電腦視覺。技術問題真的是被電歪，小緊張是會的，但是當時心情是很興奮且雀躍的，因為又感覺自己進步，並且又了解更多東西。   我還記得當時面試完我的腎上腺素飆高，打電話給我爸說我得到口頭 offer，結果還被我爸潑了冷水：「人家只是講講啦！啊你這樣會不會很忙，我是覺得你不用去實習啦。」真是幸虧我一直都很叛逆不怎麼聽話。   真的收到正式 offer 也差不多隔了快一個月，當時打開信看到一堆英文加上很多 statements (完全沒有中文)，覺得非常新鮮 (結果現在英文文件已經看到爛且習慣了，但第一次的感覺還記憶猶新)，而且面試過感覺每個人都很強，還是難掩說不出的興奮。   我的天！我真的獲得進入國際大公司工作的機會！   到正式入職當天 (我第一天報到還沒出門就超嗨)，Amazon 就馬上有了新的辦公室 (真的是決策很快)。我是第一批錄取的實習生，第一天到辦公室報到時，我以為會有一批實習生一起工作，並且未來的日子裡，將會跟電影情節一樣，有個很快樂美好的 Party 跟夥伴 (完全被 The Internship 誤導)。   結果就是，全部都是幻想 ….   後來才知道，當時邀請面試前，就先刷了一批人 (AWS 環境都搭不起來)，技術面試又再刷了一堆人。結果我進去之後，才發現原來當時我同事們竟然拿正職的考題來問我 (難怪一堆人被電歪)，只有我跟另一位台大的學長 William 倖存 (當時全台灣 Intern 就只有我們，這樣聽起來好像很厲害)。   於是領了電腦、螢幕，有了專屬的座位，就開始了我在 Amazon 實習生的歷程。      有時候就是一個機會，你無法預測現在做的決定，在未來是否仍是最好的選擇，但你永遠有權決定，是不是要把握這樣的機會。    轉成正職   我的前主管是非常厲害的一個人，並且用著很不一樣管理思維在看待實習生計畫。他認為實習生最大的任務，除了一起加入專案的開發幫助改善團隊的生產力外，其餘就是學習、甚至是在公司提供的資源下拿到 AWS Certificates。並且試著從團隊的目標中，在幫助實習生成長的過程裡，找到交集的可能。   我覺得在我實習的日子裡，最感激 Amazon 有著很成熟的師徒制度 (Mentor and Mentee)、一堆優秀的同事，並且有著一輩子學習不完的訓練資源幫助你成長，所以你總能找到人為你提供建議，並向他們學習。   我當時最喜歡的電影是 The intern 及 The Internship。一個是年過 80 的退休人士在新興網路公司工作的歷程，而另外一個則是兩位中年大叔轉職去 Google 當實習生的電影。這兩部電影著實給我很大的啟發，我有時會無限回放，想著哪些原則是可以套用在自己身上，並不斷優化。我真的很感謝這兩部電影的製作團隊，這麼好的作品，真的都成為我學習人生課題一部分的重要素材。   也因為這樣，當實習生的日子，我不怕丟臉，我的心態就是努力做、努力學習，像個海綿一樣。雖然大三是課業繁重的一年，但當時我在週一到週五排課，想辦法空出一個全天，以及週六週日其中一天時間到辦公室工作。由於那段時間真的是沒有休息日，其實還有點黑暗，但我常常自主在辦公室裡讀著文件、學習材料，常常一待就快 12 個小時，跟著前輩討論技術問題。每次覺得又獲得更多知識，這個過程是有趣的。   我在加入之後，其實也從不把自己當實習生看。我一直認為我也需要有能力跟前輩們一樣解決困難的問題，所以也主動找機會觀察團隊目前在進行的工作是什麼，幫助其他前輩們回答客戶問題、一起解決現實中客戶他們所遭遇的技術挑戰。也記得我以實習生的身份獲得客戶給的正面評價那種快樂 (估計客戶不知道是一個實習生回覆的)，當然也遭遇過很棘手的狀況、被其他團隊的同事洗臉，但都促使我有能力從這些過程學習更多。   在這短短一年內，我也推進自己通過AWS 五張主要的核心認證。當時在台灣其實並沒有那麼多人獲得，在別的部門裡，都知道有個不太正常的實習生竟然在一年內全部都拿到 (變相成為部門主管們 Push 績效的故事)。   實習就這麼來到快一年的時間，我的學分也在大四也早就修滿。所以在整個大四，就直接跟系上申請校外實習學分，讓我能夠專心的在 Amazon 工作。基於政策，Amazon 是不會有實習那麼久的情況。即使如此，我的前主管很努力說服很多團隊，讓我仍繼續待在崗位上，努力往下一個階段邁進 (因為我還沒有畢業)，實在是很感激！中間當然也在「要不要考研究所」、「出國留學」、「轉換機會」之間掙扎，但在綜觀考慮下，還是決定進到轉正式職位的過程，於是，又開啟另一波準備面試的流程。   Amazon 實習生轉成正職的階段，並不會因為已經過去錄取，就過過水讓你轉成正職。仍然需要經過一定的方式進行考核。所以在我的面試中，同樣又安排了三關：技術面試、HR、Hiring manager。   我的技術面試是西雅圖的同事幫我面的，又是另外一個境界的考題。考了很多 AWS 實務上、設計系統架構、執行部署的策略、細至解釋容器網路的行為等等 …..；HR 則了解人格特質、表明薪水談判的價碼跟期望待遇；Hiring Manager 則是我的前主管再一次的用更複雜的 behavior question 了解我的答案。   總體而言，並不會因為自己已經是實習生就比較輕鬆。但相對而言，因為經歷一年的大風大浪跟磨練，比起剛步入職場的菜味，相比下，能用更平常的心態面對這些問題。   一般剛畢業的學生，由於與業界所需求的能力仍有一段差距，所以能獲得的 offer 通常都是 Cloud Support Associate，也就是助理職位。然而，在我的面試官們綜觀討論結果後、包含過去在實習上的表現、有能力解決企業客戶遭遇的問題，前主管非常肯定我的績效、HR 很驚訝一個實習生竟然可以一年內全拿 5 個核心認證 (還與績效無關)、技術考官也覺得我的技術能力不輸一般在外面工作 3-5 年的工程師，所以最終決定以 Engineer (正式工程師) 給予我 offer，開啟我人生一個新篇章。   聽到這樣的好消息，真的是放下心中的大石頭，就準備畢業當天上工，直到接兵單中間再去服役。                     在 Seattle, US 參加 New Hire Training 換到的新 Badge。順帶一提，Amazon 的 Badge 有個有趣的小巧思可以幫助你識別這個人到底是不是老屁股            成長心態   在投遞這份工作之前，我並不認為自己很有可能會上，當時只是想著：「希望這是一個成長的機會，即使失敗也是一個很寶貴的經驗。」直到跨出那一步嘗試後，才發現其實自己並沒有想像的糟。   我回顧這些日子以來，我不敢說自己是最優秀的，但很努力是肯定的。在所有同學還不知道畢業後要幹什麼之前，我願意放棄放任時間流逝的機會，即使當時實習沒有學分，還是願意犧牲自己的假日，跨出那一步去學習我有興趣的知識。   很多人無法理解為什麼我可以錄取這麼大的一間公司，我只能說，其實這得多虧前面長期做了很多累積，在很多人不理解的環境下，仍堅持並持續學習。   我必須承認，真實世界 80% 的問題都不是學校會教的 (包含我自己面試遇到的問題也是)，學校只會告訴你課本上的答案，並且只是擔任領路人的角色，帶著你入門。我在學校學習的計算機概論、作業系統，考的也是一堆專業術語跟名詞 (e.g. 物件導向三大特性：encapsulation, polymorphism, inheritance / 作業系統怎麼處理 I/O 跟中斷 … )，而且我當時考題還是英文填空，拼錯就沒分 (管你是不是英文母語人士)。   這種評分機制下，我作業系統跟計算機概論成績還只是 B~C (大概只是剛好及格的程度)，你不見得能夠知道會這些東西有什麼用，但我很清楚背名詞術語跟其用途，只是用來拿分的工具。如果你問我 X 語言怎麼寫出封裝、設定繼承衍生物件，我可以直接給你或是找到對應程式碼的範例。亦或是告訴你我在 Linux 看到哪些指標，具體可以知道現在系統 I/O 處於瓶頸影響到應用程式效能。   很大一部分都是我在過去自己學習 Linux、操作實務上所累積的知識 (我從高中在 Linode 開啟了第一個虛擬機器並還在管理系統)。網路相關的知識也是過去經歷競賽，學習 CCNA 強化 Layer 1-3 的網路知識，並且在系統維運上也有對應的經驗 (雖然都是小規模)。我對於學習資訊技術本身就有濃厚的興趣，會在網路上搜尋開放式課程，更是把讀 O’Relly 系列的書視為聖經 (不得不說那個質量跟很多台灣出版商發佈的電腦書籍差太多了)。   在我實習面試過程的問題，大部分都是依照過去經驗回答，在當初實習生的面試前，花一大部分時間讀鳥哥複習 Linux 的知識，但是學校不會把鳥哥寫的書列為參考書單，講講實務上可能會遭遇的問題 (至少我當時是這樣，現在我相信很多老師正努力改革！)。而一直沿用在 1983 年出版第一版、已經出版 20 年以上的 Operating Systen Concepts (恐龍書) 投影片講到退休，所以同學們上完這些課，感想大概就是你聽過但不知道這個東西能做什麼。   這是台灣教育最有趣的一大特色，我有自信現有的教育制度，絕對能訓練出會考試的學生，但無法培養能夠解決未知問題的學生。所以我十分鼓勵你對有興趣的主題，透過實際遭遇的問題，找找線上資源充實你的求知慾。   現在很多很棒的線上學習資源，圖書館也有非常多的好書。我相信學習有很多途徑，你也能挖掘到適合你的渠道。   在這短短幾年，我見識到了很多優秀同事工作的方式，並且從中學習好的部分，學習深入研究及思考問題，幫助自己不斷跨出舒適圈。我一直感受到心態其實也是影響事情結果很大的一項因素。在大部分的情況下，很多人總覺得自己不可能錄取，於是就選擇放棄行動且學習跨出那一步 (定型心態)。但學習以成長心態看待事情，往往可以發掘很多成長的機會，並且努力思考及行動，讓自己更貼近自己理想中的自己。   當初畢業前我總是心心念念覺得自己應該成為一名軟體開發工程師，但在接觸這項工作內容後，我挖掘到了另一個面向發展的可能性。但當時很多人質疑我選擇這份職位，是否是一個正確的決定？   我的前主管告訴我，他過去也從一名研究生、決定工作、後覺再到英國全球前幾名的學校完成 Geography 博士學位 (中間再決定多修一年 Computer Science)、再到創業管理團隊，最後選擇到 Amazon 接下主管職位。每一個選擇總是有捨及有得，因此，你能做的，就是將目前可能的選擇列出來，然後選擇一個你比較滿意的，你的心中，自然會有答案。   總結   希望我的故事能帶給閱讀到這邊的你一些啟發。我始終相信人生是一段長而持續的累積，如果你正為了在某個人生的分岔點感到徬徨，請記得，如果五年、十年後往回來看，這不過是人生旅途的一部分，並且，是最有趣的部分。   往往我們總無法選擇最完美的結果，但我們能夠選擇當下最好的決定，並且努力思考及行動。正是因為這樣的過程，才學習如何強大、感受命運帶來的驚喜及餽贈、認知自己的使命，並明白自己要走的方向。   看更多系列文章      我在 Amazon 學到的 10 件事   我是如何在還沒畢業就錄取並進入到 Amazon 工作   Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)   我是如何在一年內通過 AWS 五大核心認證  ","categories": [],
        "tags": ["amazon","aws","amazon web services","work","Cloud Support","Cloud Support Engineer"],
        "url": "https://easoncao.com/how-am-I-get-into-amazon-before-graduate/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/how-am-I-get-into-amazon-before-graduate/amazon-macaron.jpg"
      },{
        "title": "我是如何在一年內通過 AWS 五大核心認證",
        "excerpt":"這是我在 2019 年與同事們分享的內容。當時我們團隊，只有少數人有去考 Solution Architect Associate。而我，是還在當實習生的時候，在短短一年內，通過 AWS 五張主要的核心認證 (All five)：      Certified Developer   Solution Architect Associate   SysOps administrator Associate   DevOps Engineer Professional   Solution Architect Professional   我在我同事的推薦下，當時還在內部辦了一個講座，分享自己的學習心得給大家，也讓很多同事陸陸續續獲得想要的認證。   順帶一提，我是 2017 年三月加入 AWS 才第一次擁有 AWS 帳號並開始學習。所以我想下這個標題分享我當時講了什麼，應該是蠻圖文相符的。   Certificate Path   我的第一個認證 (Solution Architect Associate) 是在 2017 年五月拿到的，在這之後我就連續安排了學習計畫及考試：                     我在一年內的 AWS 認證歷程            難易級別   當時 2019 年剛開始出現 Specialty (專家級) 類型的認證：                     2019 年的 AWS 認證種類            當然 AWS Certificate Team 也不是什麼事情都不做，這中間幾乎每 6 - 12 個月就誕生一個新種類。截至我寫這篇內容為止 (2021)，現階段又多了很多 Specialty (專家級) 的認證類型。                     2021 年的 AWS 認證種類            我個人第一張認證是 Solution Architect Associate，但再轉考 Certified Developer Associate 時，唯獨大概要學習怎麼算一下 DynamoDB 的 Capacity，因此覺得特別簡單 (因為都只是考 API)。   但如果依照難易度種類區別，我會把其難易度分為以下，這也是廣泛 AWS Certificate SME (認證專家) 認知的難易程度：                     AWS 認證難易度 (級別為個人意見僅供參考)            我的學習歷程   我的準備策略除了依循上述的難易級別循序考上去之外。以下分幾點列舉出我在準備及學習 AWS 歷程上，對我十分有幫助的幾項重點：   1. 建立正確的心態   我考取認證的主要心態，並不是「為了考取認證」、「為了獲得認證」這種想法而去選擇準備，而是：      我希望我能透過認證的方式，有系統性的學習 AWS 服務   我並不認為通過認證就代表真的非常有能力在 AWS 上建構系統，並且在 AWS 上所向無敵，但相對而言，擁有基本且足夠的知識，能夠幫助你知道如何解決當前所遭遇的問題，我僅將其視為一項過程   我認為考取認證是充實不同面向知識的一種方式，並且希望從學習過程中在面對問題時，汲取「如何 / How」相關的經驗及知識   對我來說，考取認證不是目的，而是學習的一部分。所以我並不認為一次的沒通過算什麼 (雖然當時要求自己可以的話就試著「一年全數通過」)。相反，我反而能更加了解自己不太熟悉的內容是什麼。      (不過我每次都盡可能在考試日前，甚至犧牲假日努力準備跟複習，所以至少都有及格)    而且這種考試根本與什麼定生死的學測不同，沒過還可以 Retake。   所以其實將其視為一種學習的循環，相較而言，準備起來更為輕鬆，更像是一種時時刻刻檢視自己學習的過程 (不過當時為了不想浪費錢，要跳向 Professional 的考試真的給他拖有點久)。   2. 釐清考試目標及大綱   因此，我會推薦 Blueprint (考試指南) 一定要閱讀，這能在你安排準備該認證時，有清楚的藍圖知道這個認證著重要學習的重點是什麼。你通常可以在認證頁面找到對應的位置下載：                     檢視 AWS 認證提供的指南            如果你仔細注意，你一定可以知道每個認證項目著重的重點是什麼，例如 DevOps Engineer Professional：                     AWS DevOps Engineer Professional 考試指南 (2021 年)            從裡面你可以確實注意到 CI/CD 相關的項目佔了不小的成分。如此一來，我就會知道需要努力學習 AWS 上能夠滿足 DevOps 相關的解決方案及 Stack，並且透過 Whitepapaer、re:Invent 等影音，知道如何實踐 CI/CD 等策略。例如：CloudFormation、Beanstalk、CodeSuite (CodeCommit, CodeBuild, CodePipeline) 等等。   但如果你看了 Solution Architect Professional，你會發現就幾乎是什麼都要沾一點邊：                     AWS Solution Architect Professional 考試指南 (2021 年)            所以我也認為 Solution Architect Professional 其考試真的有一些難度，當時的方法就是把產品頁面展開，然後看一看這些產品都是在做什麼。並且基於 Solution Architect Associate 的基礎再更進階有關高可用性等問題，大量透過實際案例做更深入的閱讀及學習理解。   Associate 類型的考試      閱讀 Whitepapaer 並且針對你有興趣學習的服務打開AWS 文件閱讀你感興趣的章節。   由於 AWS 服務非常多，有些教育單位會提供模擬試題，盡可能了解回答及錯誤的 AWS 服務，並且再回頭看看文件或是做做 Lab，幫助你加深印象。   Professional 類型的考試      沒有什麼訣竅能夠幫助你快速理解所有 AWS 服務。我推薦的方式是在 YouTube 上找找 re:Invent 系列的影片 (尤其是 Deep Dive 類型) 並且安排時間學習。re:Invent 系列的影片常常會包含許多實務應用範例及使用情境，甚至可以學習客戶的使用案例，這有助於幫助你了解一些複雜問題下該如何實踐最佳實務。   觀看 re:Invent 影片後推薦實際去玩一玩這些 AWS 服務，這有助於加深印象。   Professional 級別的考試因為題目又臭又長，每題都是情境題，因此更需要正確規劃作答時間。如果第一眼不是那麼確定，可以先選一個概略的答案，並且在答題系統上標記，等完成後再回來檢視。   3. 分散式學習   我的學習場域大部分都是在捷運上 (沒錯，就是捷運)：                     我的學習場域            在我的學習過程中，常常借助了大腦比較喜愛的學習模型 (這有機會再寫文章探討)。我會習慣將學習材料切割成多天分散學習，以幫助我在建構這些知識時，大腦有充分時間能夠組織及強健知識：                     分散式學習               有的人可能習慣在考前一天抱佛腳 (但往往考完就什麼都忘了)   有的人可能習慣在考前一週集中 (這通常也是考試導向，往往最終獲得的知識狀態也不見得很融會貫通)   或是分散幾天一小時 (有的人大腦需要一些時間喚醒才能進到之前學習的狀態)   學習並沒有最佳做法，只有最適合你的方式。但已經有許多研究顯示，分散學習確實有助於建立大腦記憶區塊。   在我的案例中，我大部分時間仍然是透過通勤時間 (20 - 30 分鐘) 聽著及看著學習材料幫助我了解不同項目，然後到辦公室繼續做其他事，假日有時間再額外看看或是做做 Lab。   我一直認為通常說的「沒時間」其實只是表示「我覺得這件事情不重要」。當你認知這件事情有需要去做時，你總能安排時間。      但當時在旁人眼裡，我就是一路盯著手機走出捷運站然後一路走到辦公室，外面的紛紛擾擾與我無關    總結   在這篇內容中，我分享了我自身在過去如何於一年內獲得五張 AWS 核心認證的經驗，並且與你分享一些實際上可以參考的應用方式。如果你正在準備 AWS 認證，希望這樣的內容對你有幫助。   如果你覺得這篇內容不錯，可以分享或是按個 Like / Clap。   看更多系列文章      我在 Amazon 學到的 10 件事   我是如何在還沒畢業就錄取並進入到 Amazon 工作   Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)   我是如何在一年內通過 AWS 五大核心認證  ","categories": [],
        "tags": ["amazon","aws","amazon web services","work"],
        "url": "https://easoncao.com/how-i-pass-aws-all-five-certificate-within-one-year/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/how-i-pass-aws-all-five-certificate-within-one-year/certificate-path.png"
      },{
        "title": "我在 Amazon 學到的 10 件事",
        "excerpt":"                  Every day is day 1            今年 (2021) 是我加入 Amazon 這家公司的第三年 (把當 Intern 跟當兵的時間也算進去的話 XD)。在加入這個組織這短短 3 年中，我明顯感受到自己的成長與變化，畢竟 Amazon 也是存活了 20 幾年的公司，在全世界雇用了上百萬名員工，並且仍不斷在成長。在組織運作執行上確實是有非常不得了的一套，許多原因使得我仍願意與企業組織一同成長，以下列舉幾項我至今覺得很寶貴並且感知受用的幾件體會：   1. 快速解決問題比起使用什麼技術來得重要 (完美是更迭出來的)   在過去，我完全就是一個有技術潔癖的人 (比如我是一個 Python 開發者我就一定要用 Python 的 tool stack 完成我所有的工作)，我很執著在解決問題前思考需要使用什麼樣的技術，目的可能是考慮了：擴展、比較嚴謹、之後可能比較不會出問題、想趁機會學習新東西、可以變成一項話題 (比如可以發表或是展現技術應用等) …… 等等。   總之不管什麼原因，可能在處理一個問題時會審慎思考自己用的技術 (比如：需要爬一些網頁資料、整合一些資訊變成另一種形式的操作模型 (比如輸出成報表、戳自己的 webhook 發通知)、設計應用跑運算)，或是基於完美主義覺得一定要所有事情一氣呵成。   拿爬網頁資料來說，我可能要：      構建 Python 應用程式   查一下需要的套件跟操作方法，比如 requests   跑 pip install 安裝需要的套件   測試 Python 應用程式是否能正確運作   Debug 再回去修改應用到爽   這還只是拿 Python 當範例，事實是可能來來回回都花了兩個小時，包含編譯打包做了一大堆工作，但是真正要解決的問題才有可能被解決。   但是在隨著這樣的處理模型下，我發現人們更在乎的是「解決問題」，而不是你使用了什麼樣的技術。客戶的需求跟問題只會每天地不斷地在變化，對於小問題，人們更注重於能否在有效的時間內解決問題。   所以現在，在工作上我可能傾向於使用 Linux 命令加上 Shell script 就可以完成我想做的大部分工作，可能只耗費我 30 分鐘做一個版本，只是步驟會被拆解成：      拿 curl 把 html 資料爬回來   grep 一下！可能在用個正規表達式過濾一下我想要的訊息   把結果貼到 Sublime / 文字編輯器在處理下篩選我要的內容   需要重複這個操作，Shell Script 寫個幾行就搞定   亦或者是我想要定期爬 facebook 上的資料，如果是用原生語言硬幹，我可能會：      構建 Python 應用程式   查一下需要的套件跟操作方法，比如 requests   跑 pip install 安裝需要的套件   模擬使用者登入並且處理 facebook 上的驗證操作   測試 Python 應用程式是否能正確運作   登入之後在爬爬感興趣的 HTML 資料進行字串處理   測試 Python 應用程式是否能正確運作   將有興趣的資料進行下一步處理 (比如觸發 Webhook)   Debug 再回去修改應用到爽   然而我只要在我的執行環境中使用 Greasemonky，並且跑一台 VM 開著瀏覽器，完全可以做到上述的工作：      安裝 Greasemonky   寫一寫 Javascript 把 DOM 跟感興趣的內容撈出來 (完全不用搞驗證)   Greasemonky 可以操作跨站存取 (CORS)，所以我可以把撈出來的資料吐到其他地方或是觸發 Webhook   在 Greasemonky 腳本裡面插一點檢查機制，發現資料撈不出來可以發個通知手動檢查下   雖然問題拆了幾個部分，並且不是那麼漂亮 (沒有一氣呵成的感覺)，但是，整體開發跟配置相對簡化非常多工作。   如果只是一些小規模或是立即性的需求，這種工作模型卻足以滿足大部分的工作。而且會發現，其實人們對於使用組合技 (特別是對於技術人員來說)，普遍接受程度並沒有自己想像中的低 (因為畢竟大家都只專注在解決問題)。   一旦真的出現規模需要或是下一步更進階的使用需求後，再來設計原生語言的套件也不是一件特別困難的事情。此外，我發現這種模型間接幫助你驗證整體的業務邏輯，再改用原生語言進行實作，其實在開發上像是有了一個規格，也相對「清楚要做什麼」，並且容易許多。   2. 這個世界變化的比你想像中的快 (… and, it’s always day 1)   Amazon 提供的雲端服務 (Amazon Web Services) 提供上百種不同的服務及解決方案，這其中都是上上下下數千個團隊跟不同部門合作下一起構建 (而且是不同跨時區下協同，幾乎整整 24 小時都有人在進行)。   常常過幾個星期就又有團隊迸出新的功能或是服務要上線、一覺醒來又有新的變革，亦或是在你還沒準備好了解這項變動之前，已經有客戶想知道這些釋出的功能的具體細節 (我從沒有跟上客戶的腳步過)。   在還沒加入這樣規模的組織前，我其實無法想像這個世界是多麽積極的在變化，尤其在台灣這樣的小島，每天媒體告訴你、周圍朋友在 Instagram / facebook 告知發生的事情，其實都大同小異，圈圈內流通的資訊更迭的速度其實並沒有那麼快，大家都很被動的感受自己接收的資訊。   但在跨時區分工合作的運作下，你必須習慣睡覺醒來一切又有變化的事實，並且是超級快速，每一天都像是第一天 (Day 1)。這迫使你會需要練習快速學習的能力以適應這個世界的更迭，不斷主動更新自己的知識跟技能。   3. 學習必須有效並深入核心而不是只停留於表面 (Dive Deep)   在 Amazon 提及的 14 條 Leadeship Principles，有一個我感知最深的項目就是 Dive Deep (追根究底)。   我在面試很多候選人後，我發現很多科技圈的人才都有一個普遍的慣性，就是對於新技術或是知識的理解都停留於 Surface Level (表面)，即使是自己熟悉的技術或能力也是。很多人往往對於接受一個知識或技術的理解都停留於表面就感到已經完全理解，但在我的工作中，往往是面對未知甚至是很多應用的 Bug，並且找到一個適當的解決方案。如果保持這樣的思考慣性，很容易導致在遇到問題時無法切入並解決核心問題。   以大家很常說自己會的 Kubernetes 舉例來說，通常問以下問題 95% 的候選人都回答不出來：      為什麼 Kubernetes 在同一個 Pod 中，不同的容器可以使用 localhost 彼此訪問？    你通常就是看書上說說知道 Pod 是什麼，並且會說是一個 Kubernetes 特有的元件。但往往就是這麼一個簡單的問題 (網路上也一堆解答)，讓我發現很多人對於接受一項知識的理解往往停留在：文件如此寫、會操作 kubectl 命令並且部署、看範例就是如此、Kubernetes 直接劃分一個可共用的網路環境 (然後就無限鬼打牆 … 例如：它就是一個抽象元件 等答案)。   但如果你真的試著想要進一步了解其底層運作原理，你會發現其實 Pod 並不是什麼特別的東西，網路的共用特性與 Docker 相同技術定義的 Network sandbox / ECS 定義的 Task 相似，這使得其可以使用 localhost，並且都基於 Linux Kernel 提供的功能執行類似虛擬化的操作。一旦你試著對知識刨根究柢，這在處理有關 Pod 網路相關的問題時，才會有更清楚的脈絡知道要往哪個層級剖析 (是容器？系統？還是外部網路？)。   當然我也不是一開始就如此，而是在經歷過各種亙古難題，並且每天都得想辦法解決未知問題的環境訓練下而養成。也因為如此，我更重視自己在接受知識時，必須以更深層次思考的習慣，並與現有理解嘗試融會貫通，並不斷反思自己現有的知識跟理解是否正確及深入。   這個 Dive Deep 也不是指要你鑽牛角尖，而是了解以更實際及應用的層次，逆推回去學習並引導自己往更核心的底層近一步理解。正是意識到這樣的能力培養在日常工作中的必要性，我仍不斷學習並強化自己的知識理解，進一步培養自己對於問題切入剖析核心的技巧，幫助我解決所遭遇未知的問題。   這樣的原則我感受到不同領域上如此應用，通常能引導自己能提出更深層次的解答、或是嘗試在挖掘更多具體我所需要的資訊，並且有效的行動。   4. 不必太執著於工作，但工作起來必須很執著   Amazon 有一句膾炙人口的標語：      Work hard, Have fun, Make history    看到這裡你覺得 Amazon 就是一家非常 Work hard 的血汗工廠 (加上一堆新聞都說 Amazon 很血汗)。但事實是 Amazon 並不希望自己的員工 Overtime 加班，並且盡可能優化不同團隊之間工作效率。   因此，在內部我們不斷的每天都在想著如何改善產品、改善服務、改善所有不太 OK 的事情，並且專注這些目標，努力在工作崗位上做出一些改變，而不是滿足於現狀，這是我們所謂的 Work hard。   在這裡，老闆會耳提面命希望你不要一直加班，並且要求你必須注重自己的私人時間及生活，否則很快就燃燒殆盡 (Burnout)，領導階層也會時時刻刻了解是不是有任何工作負載過於集中的問題 (有點類似避免單點故障的概念 / Single point failure)。   透過維運的角度盡可能幫忙你找到對應的 backup (換句話說，在身處周遭都是優秀的同事，其實你總能被取代，無需在自己的私人時間太執著於工作，並且需要相信你的隊友)。   當然，該認真工作時絕對是很可怕的。在工作上，除了會有各式的績效及 KPI 你會需要想辦法滿足 (其實並不是特別困難)。但更重要的是，因為身邊的同事都十分優秀，你必須很認真的思考自己的職涯規劃並設定目標，想辦法讓自己成長、努力工作改善現有的問題及自我，並把自己跳出舒適圈往更高的階段推，而非滿於現狀，否則很快面臨瓶頸。如果你是一個很樂於學習及成長的心態，在這個組織下工作是十分有趣的。(反之，你可能待不到幾個月就想走了 XD)   在這裡，大家不是搏感情比誰更努力工作加班，而是比誰能更有效率的完成事情。   5. 在必要的地方花錢不手軟，而不是無限度擴張福利      (我的解讀為：錢要花在刀口上)    在 Amazon 工作，你真的會覺得很「摳」，你會發現組織對於「免費午餐」、「員工旅遊」這類的事情錙銖必較。在過年過節也不會有所謂的三節獎金、上班提供按摩之類的服務 ….。當初 Package 跟你談好多少就是多少，而且不多也不少 (在 Leadeship Principles 有一條 Frugality，完全就是體現這項原則)。   但是相對的，若是對員工生產力、有助於客戶的事情，花錢絕對不手軟，例如：      需要擴充硬體設備進行開發   員工手機帳單可以報帳以提供在工作上穩定的網路服務   需要跨國出差，只要不是亂花錢，都願意全部包機票住宿及食宿給你飛過去 (我有幾次來回美洲幾星期，光一趟就報帳報掉快 7000 USD，折合約 10 - 20 幾萬台幣….)   並且在教育資源及訓練上投入大量的資源，十分專注幫助員工成長。目的都是為了提供給予客戶更好的服務及產品、解決方案。   有時候大家選擇工作可能優先都以公司提供什麼樣的福利及薪資為考量，這並沒有什麼問題，但最怕的是往往沒有意識到談來的薪水可能是一攤死水。然而，Amazon 招聘最為厲害的地方在於，其在吸引人才的招聘政策永遠不是用薪資及福利作為主要的手段 (當然薪水我相信在市場上也是很有競爭力的，但比起其他頂尖的外商我相信有更誇張的數字)，其更重視的是候選人願意用成長的角度加入這家公司，並且積極冒險及試驗，你會更感受到你跟公司的關係更像是合作夥伴，並且珍惜你手中握有的股票，這正是 Amazon 持續十幾年來一直在做的事情，致使你會願意參與這家公司的成長 (然後看著它市值一直一路往上，像是看著自己的孩子長大一樣)。   6. 做出初始版本跟寫好一頁的文件比起做好投影片重要  在 Amazon，我們很少做投影片，你會發現公司上上下下沒有人在專注弄絢麗的投影片樣板 (除非是給外部客戶看的，不然通常都單調並且樸實無華)。大家反而更專注把自己的 Document (文件) 寫好，如果你要說服你的老闆、提交升職、提出一個計畫或改進，在這裡做簡報並把一切描繪(吹噓)的多美好，是不太會有人理你的。   文件一般來說，通常會是 1 頁 (內容不超出一頁)，至多也不會超過 6 頁。在文件的內容上，都明確寫下了具體的 data point (需要有事實的資料陳述) 還有你的計畫、或是績效，因此，寫好一份文件比設計投影片更來得困難。因為 …. 你會發現做了一堆事結果可能在文件上只有一兩行字 ……   構建美好幻想不如實際做出一個初始版本，並且收集到相應的資料 (就算沒有初始資料也必須擁有相應的資料點幫助陳述你的計畫說服其他人)，並且讓他人閱讀你的文件後進行評論。在接受這樣的工作模式幾年後，我也覺得這種方式是一個很有效率的操作，由於你會需要累積足夠的 data points 置入，會更清楚知道自己的目標跟預期計劃是什麼，而不是埋著頭一直做，再讓老闆憑感覺同意你說的。   在我的團隊，大家分享內容後，比較常聽到的是「我把今天提到的內容、相關的資訊也整理成 wiki 跟文件 (或錄影，可以使用 1.5x 倍觀看)，有問題可以再讓我知道」而不是「簡報我已經分享在 …，有需要可以自己去下載」   7. 能不開會就不開會，要開也盡可能開得快   在 Amazon，員工每天都會匿名接受調查對於管理層的相應執行績效 (我們每天都會為管理層執行評分)，其中很常問的類似問題就是 The meeting I've is ... ? (類似可回答的答案: effective / non effective) 或像是 my workload is manageable?  這類的問題。   你可以注意到 Amazon 一直在嘗試減少不必要會議的數量，並且盡可能幫助員工有效地執行工作，包含我所在的團隊，通常只有 team meeting 為慣例會議，並且主管也是快速用幾分鐘講講近期的更新或變動就直接進 Round table (讓團隊成員看看有沒有問題需要提出來的)，一週平均會議時間都低於 2 小時，在其他地區有的團隊也是實行 Stand-up meeting (站著開會，所以開太久會站很酸)，可見大家真的很不愛會議。   即使 Manager 召集的會議，但若有其他重要事項在處理，如果沒什麼特別的事項，是有權拒絕參加的，並且由他人幫忙轉達 (而且通常大家都偏好能不開會就不開會，這點管理層或是上上下下的團隊大部分都很有共識)。通常會議的目的就是進行決議，或是傳遞必要訊息，工作上也會統計每週花在 meeting 上的時間，如果太多，你可能會需要審慎思考是不是真的有必要開這些會。   其他例子像是：所有 interviewer 面到後面投票決定不錄取一位面試者，HR 事先也已經透過意見調查收集大家的共識，會議就會直接取消，而無需裝忙開會。   8. 搞懂 Priority (優先順序) 跟重要性比起做一堆 KPI 來得重要      (如果不是很重要，能不做就不做，而不是一昧瞎忙)    在上萬人的組織中工作，你會發現每天要做的事情真的是很多，且每天收到的 Email 就像雪花般的飄進來。由於每個人都很忙，每個團隊都有自己的問題需要解決，有時候你做了一堆 KPI 跟數字，卻到頭來發現並不能長遠解決團隊所遭遇的問題，以及契合目前公司營運的發展目標。這體現了正確工作的重要性，因此每次在新的季度，領導階層一定會告訴大家 KPI 跟目標是什麼。   在每天被塞爆的工作中，你需要時時刻刻檢視自己正在做的事情是不是真的有必要，並且是完成重要的工作。在這裡達到 KPI 是應該的，並沒有什麼特別。   大家反而會不斷提出的問題是：「這真的是我們需要做的嗎？真的是重要的嗎？長遠來看對團隊是有幫助並能改善問題的嗎？」   因此，正確的配置任務的優先順序，並且有效平衡你的時間運用。正確滿足 KPI 需求之外，同時需要契合你的成長目標更顯重要；而不是一昧地瞎忙，或是出於人情幫忙做了一堆事情 (搞得自己表面很忙)，卻不符合團隊的營運目標。   9. 在很重要的事情十分快速，在不重要的事情無限拖延   Amazon 真的是道道地地的美國企業。你完全可以在這裡感受到外國人做事的效率 (在有些事情上真的是超 …. 級 …. 慢 ….)。像是我曾經為了在 AWS 官方部落格發佈一篇文章，整整花了半年 (其實也就 2 個季度、6 個月，我已經見識過擺至少 1-2 年的待辦事項，仍懷著感恩的心)。   像是在台灣你訂個包裹可能等個 3 天就快受不了了，但自從我在 Amazon 工作後，有些卡在其他團隊的事情，如同前面提到的，每個團隊都很忙。如果不是那麼緊急的事情，我在幾天至一個星期內得到回覆，真的是說不出的感激，並且覺得很快。在這樣的環境下，完全顛覆我過去的時間觀念，並且練就一身好耐性。   在這裡，大家會對重要的事情十分快速，並且立刻做出相應的行動 (例如發現有 Bug 需要 Roll back 以解決一個非常嚴重的影響)。但如果優先順序並不是那麼急迫，或是影響並不是那麼大，那你得預期可能會是一場持久戰，甚至很可能並不會被解決，並且需要抱持著不斷騷擾人的心理準備。如果想加快，你必須用 data Point (數據) 說服其他團隊為你做事，而不是只因為你想或是你職位比較大。   事實上，我認識很多國外的同事們並不愛加班、到晚餐時間一堆商店都關門，但這樣做出來他們的工作結果並不差。反過來真的會思考亞洲人拼命的文化到底是在拼什麼 (大陸的 996 工作制度真的有比較好？)，太執著於細節或是一些小事情，似乎並沒有帶來更大的效益，也許這也是值得你我學習的工作模式。   10. 永遠思考對客戶是否有幫助及符合長遠的目標，而不是只因短期利益選擇做事   在我的工作中，時常會在不同的解決方案中建議客戶選擇合適的項目。若是一般的商業利益考量，大部分會為了短期績效及利益，可能建議客戶使用不見得適合他們且昂貴的方案。當然，這確實有助於增加年度營收。然而，這並不符合 Amazon 追求對於客戶服務的目標。在這裡，我們會想著如何幫客戶省錢 (例如: Amazon EKS 折 50% 的定價、AWS Fargate 降價)，並且思考如何提供更好的服務及產品反饋給客戶，與客戶建立信賴關係。同時，為其帶來長遠的收益 (AWS 幾乎用了整整 10 年創新，並由虧轉盈)。即使可能短期效果並不是立即的反應，但這一直是 Amazon 做事的風格。   總結   最後讓我用 VP - James Hamilton (同時也是分散式系統的專家) 在 2016 年發表的文章進行總結，與你分享我在這幾年切身感受的體會：      As a member of the AWS engineering team, my first impressions are probably best summarized as fast. Decisions are made quickly. New ideas end up in code and available to customers at a speed that just makes the pace of enterprise IT look like continental drift. In a previous role, I remember (half) jokingly saying “we ship twice a decade whether customers need it or not.” Now new features are going out so frequently they are often hard to track.       作為AWS工程技術團隊的一員，新環境給我留下的第一印象就是一個“快”字-決策制定流程非常迅速，新思路能夠很快以代碼形式推出，並立即被交付至客戶手中 。這一切都讓傳統企業IT的響應速度看起來像是大陸板塊漂移。我記得自己曾經半開玩笑地回憶過往角色稱呼：“我們在十年中只發布了兩次客戶可能需要，也可能不需要 的更新。”現在新功能正以驚人的頻率推出，我們甚至很難追踪其推進節奏。       Another interesting aspect of AWS is how product or engineering debates are handled. These arguments come up frequently and are as actively debated at AWS as at any company. These decisions might even be argued with more fervor and conviction at AWS but its data that closes the debates and decisions are made remarkably quickly. At AWS instead of having a “strategy” and convincing customers that is what they really need, we deliver features we find useful ourselves and we invest quickly in services that customers adopt broadly. Good services become great services fast.       AWS的另一種有趣的特質在於對產品和工程技術相關的處理方式。各種爭議會不斷出現，而且AWS內部的辯論之聲要遠遠超過任何其他企業。這些決策的流程讓我可以選擇， AWS除了擁有卓越的數據處理能力外，也能夠快速中止逐步並存並採取出征性意見。在AWS中，我們不再自以為是地制定“戰略”並強行說服客戶認同其適用性，而又推出了適合自身業務環境 的方案，並通過面向服務的快速投入幫助更多客戶快速享受至由其帶來的便利。如此一來，優秀的服務能夠快速取代為卓越的服務。       (source: A Decade of Innovation / 中文: AWS的十年创新之路)    看更多系列文章      我在 Amazon 學到的 10 件事   我是如何在還沒畢業就錄取並進入到 Amazon 工作   Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)   我是如何在一年內通過 AWS 五大核心認證  ","categories": [],
        "tags": ["amazon","aws","amazon web services","work"],
        "url": "https://easoncao.com/ten-thing-I-learned-in-amazon/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/ten-thing-I-learned-in-amazon/day-1.png"
      },{
        "title": "Amazon 的 Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS) ",
        "excerpt":"AWS Cloud Support Engineer 主要面向的客戶受眾便是使用 AWS (Amazon Web Services) 服務的客戶，並為這些客戶提供訂閱制的支持計畫。(沒錯，也就是說，我們是付費的服務。)   由於這項服務屬於訂閱制，費用可以單月從 Developer Support Plan (29 USD) 至 Enterprise Support Plan (15,000 USD) 以上不等。因此，在我的工作裡，我每天會接觸各式各樣的規模的客戶。小至獨立、新創開發團隊，大至市值規模全球前 10 大的公司都有。      (有的客戶可能這輩子只開一台 EC2 Instance，但同時，有的客戶可能單一個 AWS Account、單一個 AWS Region，就擁有數百至數千個大規格的 EC2 Instance)                      我敬重的同事 Teddy Chen            面對的客戶                     AWS Support Plan 定價 - 來源: 比較 AWS Support 計劃            15,000 USD 光換算台幣就已經 40 幾萬，什麼服務都還沒有用就先燒一堆錢，聽起來好像傻子才去買 Enterprise Support。但很多時候，我們的客戶會十分願意花錢買我們的 Enterprise Support，你可以繼續往下看，就知道原因是什麼。   首先，AWS Support 針對不同級別的客戶提供了不同程度的響應時間：                     AWS Support Plan SLA - 來源: 比較 AWS Support 計劃            以 Premium Support 服務來說 (也就是 Cloud Support Engineer 的工作)，一旦購買 Support Plan，最大的優勢就是你可以開 Case 進來詢問我們團隊任何有關 AWS 相關的技術問題，並會有對應的專家提供解答。但這時候購買什麼 Support Plan 的差異就出現了，如果你是 Enterprise Support 的客戶，其回答的優先權絕對是很高的，即是只是詢問一般性指導問題 (e.g. A 服務怎麼使用、使用 B 有沒有什麼需要注意的細節)，服務 SLA 最久的保證時間是 24 小時內，你一定可以獲得回覆。此外，針對嚴重的問題，會有更快的響應速度。   若屬於最嚴重的案例 (業務關鍵系統當機，通常是遭遇 service outage)，在客戶開啟案例的同時，就會有資深的技術支持工程師在 15 分鐘內響應協助客戶解決。並且持續追蹤、提供相應的技術建議，以幫助快速定位問題，並盡快幫助客戶恢復系統業務的工作。   另外，若是簽訂 Enterprise Support，Enterprise Support Plan 的客戶會擁有專有的 Technical Account Manager，不斷地幫助客戶像 AWS 內部團隊反饋問題 (像是客戶的專屬窗口)、Solution Architect Team 也會協助客戶檢視系統架構設計的可行性。   同時還會有 Concierge 團隊幫助你怎麼優化每個月的帳單、提供怎麼訂閱 RI (Reserved Instance) 的優化建議，對於 500 人以上的大規模企業，借助 AWS 的資源，總體可以幫助客戶節省非常多的費用。   並且，這某種程度上幫助降低客戶端維運團隊的複雜性，專注在發展重要功能，並交付 AWS 的專業團隊及經驗快速幫助定位、協助解決很多超難又複雜的問題，並從中與我們合作互相學習成長。這正是許多客戶選擇購買 Enterprise Plan，以幫助他們加速業務成長的主要原因。   Cloud Support Engineer 的日常   Meet our Global Mandarin Premium Support Team at AWS:   (以下影片有點久遠，不過可以大略了解這份職位的特性)               (What’s it like to work at Amazon Web Services?)               在我撰寫這篇內容的同時，AWS 已經擁有超過 100 種不同的雲端服務及相關的解決方案產品。由於不同產品有其複雜性，因此，Cloud Support Engineer 的工作主要是協助客戶在使用這些服務時所遭遇的問題給予相關的技術建議及指導。   同時，在工作中也會需要協同不同團隊 (Development Team、Solution Architect Team、Account Team / Account Manager / Technical Account Manager) 亦或者是其他團隊，幫助解決客戶所遭遇的問題。由於 Cloud Support Engineer 最主要負責的技術是 AWS 相關的產品，每個工程師都會是相關服務的領域專家，因此，有的時候也可能是在協助 Amazon 內部其他團隊在使用 AWS 相關服務的問題。   在你第一天入職時，Amazon 就會有非常完善的技術訓練計畫，並且擁有累積多年的實務經驗，你還會有專屬的 Mentor。因此在前幾個月你會非常專注學習相關的知識，學習過去不同 Case 的處理方式，並在幾個月後完全的有能力處理不同規模客戶的案例。   這些案例問題涵蓋的範圍眾多，除了一般可能詢問怎麼設定、API 怎麼使用、客戶剛入門的級別 (我們稱之為很甜的案例)，大部分情況，我們處理的問題通常都不是 “Happy Case”，客戶會問的問題也都不是寫在文件裡面的。大部分時間我們都在挖掘真實世界的問題並且給予實質建議，比如：      “服務中斷”   “網站掛點, 救命!”   “資料庫連不上”   “好像有東西不太對勁並且無法正確運行”   “為什麼我的應用程式跑了一陣子就會自己 Crash?”   “為什麼我的生產環境 / 應用無法解析 DNS?”   “如何升級的時候不要有服務中斷?”   “為什麼我的生產環境 / Cluster遷移到 AWS 就不能工作?”   “救命, 我的應用程式 / 服務在遭遇大流量的時候會崩潰”   在我的工作上，得需要在短時間內學習新技術的技能，並且持續不斷的學習 (由於 AWS 團隊很多，產品迭代速度很快，往往我們都還不會之前客戶就已經在問了)。同時，很多情況客戶會混合不同服務一起使用，亦或者是大規模的集群有東西壞掉。因此，你必須要有能力剖析在複雜架構下，真正問題的核心點是什麼，並且有效的進行問題的定位。   同時，很多情況下，基於資料安全性及隱私權政策，往往我們都不會有權限存取客戶的資料、了解他實際運行的邏輯是什麼。這也意味著：我們沒有權限 SSH/登入 到客戶的環境裡面看設定、不知道客戶的 Code 是跑什麼、不確定客戶到底連接了哪些服務。所以你必須在這種情況下，還得知道如何有效的幫助客戶解決問題。   但我們的團隊在上述的情況下，還可以明確地告訴你要收集哪些東西，然後開啟以下對話 …：     哦！我從你收集回來的 kernel dump 確定是你的 Python 應用程式在 UTC 時間 XX 有問題，可能是有 memory leak，你要不要檢查一下   哦！你抓回來的封包明顯看得出來你的應用程式主動發 FIN 關閉連接耶，估計是超出應用程式可允許的等待時間。所以這不是 MySQL Server 關的，你要不要確認一下你語言 / Library 預設用的 timeout 時間?   哦！你的 Kubernetes Node 一直是 NotReady 是吧？從你收集回來的東西很明顯就是 kubelet 壞了啦！你說怎麼壞的？從我的分析可以注意到 Disk performance 不太 OK，你是不是跑了 I/O Bound 的應用啊？ …. 你說你不知道開發又寫了什麼埋坑讓你不能下班？好吧，所以你可以試試 …. etc   (我們通常不會跟客戶這樣說話，但大意大概就是如此)   很多人可能會以為這份工作就是一般的客服人員 / 低階接線生，就我在這個團隊幾年下來，我只能說這是大大的誤解，並且抱持偏見的角度在批判這份專業性的工作。   因此，要用其他方式描述我們的工作內容，我一直認為類似於「急診室的醫生」是滿貼切的說法。在客戶遭遇系統嚴重影響 (比如系統中斷、故障、service outage) 的情況。醫生也是秉持其專業需要在短時間內判斷下一步的動作是什麼，以盡快的緩解病患所遭遇的問題，並且適當的安排正確的處理優先順序 (所以為什麼在忙碌的急診室要排隊看發燒)。在我們的工作中，也會根據客戶所遭遇的問題嚴重等級正確區分 Priority，一直以來都視客務關鍵性業務系統當機為優先，以幫助降低客戶遭遇系統中斷而導致營收受損的影響。   在我們的工作中，提供了 24 小時 x 7 天 x 365 的工作模型，這意味著 Premium Support Team 在無時無刻都會有人在線上協助客戶所遭遇的問題。基於這種工作模型，我們的團隊屬於跨國的工作型態，在全世界各地都有相應的團隊執行交接。因此在台灣的下午及晚上，我們會將業務移轉至歐洲及美洲時區的同事執行跟進。   我的工作中會使用中文及英文兩種不同語言協助客戶的問題。由於用中文在 AWS 服務客戶仍然是一個很稀缺的技能 (在看這篇的你如果有訂閱 AWS Support Plan，請記得我們支持用中文開 Case，只需要在標題加上 [Mandarin] 即可！)，所以一大部分我的團隊會協助使用中文的客戶解決他們所遭遇的問題。   另一部分仍是協助全球世界各地使用英文為主的客戶、工程師亦或是 Amazon 內部的團隊。   特殊活動監控   遇到 Enterprise 客戶有大型活動或是線上業務特別熱鬧的時候 (例如：Black Friday、雙 11、搶票活動 …. 等等)，這時候就是我們會特別祈禱別發生鯊魚咬斷電纜之類的事件。有時候對於非預期的大規模流量導致任何非預期情況發生時，資深的 Cloud Support Engineer 就會進場幫忙排除可能的問題是哪些、釐清可能觸及到的系統限制、幫助客戶緩解問題，降低停機時間的影響。   Training &amp; SME   Cloud Support Engineer 擁有完善的訓練計畫，並且有豐富的資源及經驗使得你可以接受到非常深入的技術訓練 (強度頗高)。在我們的組織中，同時也有 Subject Matter Expert (SME) 的角色存在，很多 Cloud Support Engineer 都會很努力的爭取該項角色，其代表的是某些特定 AWS 服務的領域專家，並且分佈在全球世界各地 (但一個服務如果越新，全球不到 5 位 SME 是有可能的)。   不過你通常也不需要擔心沒有人可以為你提供相關的建議。這使我們很多情況我們會需要跟不同區域的外國同事交談，以討論深入的技術問題。   要成為 SME 其有一定的門檻，但總體而言，在成為 SME 前必須要累積一定數量及技術水平的 Show Case (實際的客戶案例) 進行至少由兩位現有專家的審查。通過之後會另行安排技術面試，時間可能是半夜或是早上 (通常是配合其他時區的專家)。面試的考官除了有現有專家外，還會有開發那個服務的資深開發工程師加入，全程英文對話來測驗你對於服務的了解及深度，最終再由考官們投票看看是否一致通過。這過程真的是有很多曲折離奇的故事能寫，但我相信 Amazon 對於工程師的技術標準要求仍是有一定的水準，所以絕對不是一個純接線生這麼簡單。   Cloud Support Engineer 如果成為某個領域的專家，通常也會想辦法貢獻自己的知識 (透過內部文件、製作訓練教材或是幫忙其他團隊審核公開發佈的內容)，並也有機會成為 Trainer，參與跨國訓練其他地區同事的機會。   奧客與第三方軟體   這一定每個行業都會有的，在我的工作中，常會收到負面情緒的客人通常都是購買 Developer Support Plan 的客戶，常常會挨著說每個月要充值 29 USD 太貴、問一般性問題都只能回 Email、能不能直接登入/SSH 到我的機器幫我設？   就我的觀察，這種客戶通常也是寧可花時間耗，而不是想解決問題。通常也不願意花力氣學習 AWS，並總希望有人幫忙幹到好。這種客戶典型會問的問題也像是：為什麼我的 EC2 關了還是會一直重開，你們是不是亂動我的帳戶！！！！   (結果進場看個 3 秒就注意到客戶完全不知道自己在幹嘛開了 EC2 Auto Scaling)。      小知識：AWS 非常重視 Secuirty &amp; Privacy，特別關注客戶存放在雲端上資料的機密性及安全性。AWS Support 沒有權限更改客戶的資料/SSH 進入到客戶的機器，我們光要看客戶的一些你都覺得沒什麼的配置 (比如這個帳戶在某個區域開了幾台 EC2)，都有嚴謹的稽核制度。    通常處理一個案例，我們都得想辦法引導並還得拜託客戶提供及收集一些資訊，以幫助我們確認下一步的動作或是進行分析。對於手裡握著核彈的嬰兒來說，還是建議找找 Partner 提供的 Support 計畫吧。   此外，還有些客戶會很喜歡詢問第三方軟體的一些問題 (比如 Terraform)。由於有些第三方軟體並不是我們能夠協助，也不是由 AWS 開發，這通常已經超出 AWS 所能協助的範圍。如果你跑來問 AWS Support，我們只能竭盡所能回答。然後你總是發現，即使我們可能看一眼知道怎麼修並且提供一些可能的建議，我們只能再無奈地請你關閉 Case，並請你去找 Partner (如果是 AWS 有合作的夥伴並且涵蓋在支持清單，我們會協助轉交至對應的團隊)。   但畢竟我們並不是該產品的專家，如果你遇到 Bug，我們真的無法負責，這是一種心有餘但力不足的哀傷。   技術挑戰   在有的情況，我們會需要跟 AWS 開發團隊合作，以改進產品上所遭遇的問題。在我的工作上，很多時候可能是客戶遭遇了服務功能不支持、或是產品有一些已知問題。如果 Cloud Support Engineer 想到能提供其他解決方法，我們會建議客戶採取相應的行動，例如我之前在網站上發佈的幾篇內容，都是我們客戶實際遭遇，並且提供一些可用案例的例子：      CoreDNS(kube-dns) resolution truncation issue on Kubernetes (Amazon EKS)   [AWS][EKS] Zero downtime deployment(RollingUpdate) when using ALB Ingress Controller on Amazon EKS   上述過程涵蓋的技術分析只是部分的案例，對於我們內部工程師在分析問題時，這通常是我們會需要從中挖掘的深度，並從中嘗試提供建議給客戶。但如果是要建議開發團隊修復的已知問題，同樣也會提供類似上述的分析報告，並且協同開發團隊在下一個版本中修復。   這種長期在挖掘我們家產品的 Bug，每次甩分析報告過去建議他們怎麼修 Bug 的模式，其實已經某種程度上形成良好的循環，讓我們贏得開發團隊的信任，跟開發達成良好的合作關係。(我甚至也認識很多很猛的同事是直接幫忙貢獻代碼的)   在我的團隊中能參與並能從中學習成長，事實是非常有成就感的一件事情。   因此，很多情況下，我們會需要學習用宏觀的角度分析並且切入問題核心，並且給予正確建議，而非只看單一問題亂槍打鳥，這是我認為這份工作最困難也最有趣的地方。   跟開發 (SDE) 差在哪   我們跟開發團隊相同共享存取服務原始程式碼的權限。所以有的時候，比較資深或是厲害的工程師，會協助開發找出應用程式的 Bug，並且指出要修正的細節。即使在我的團隊，仍有很多同事過去的背景從事開發、維運等不同角色。但在篩選標準上，技術的水平仍與 Amazon 開發團隊擁有著一致的標準，只是這個工作角色對於編程 (Coding / Programming) 的能力並不是必須。   但往往有時候擁有開發的經驗，在執行這份工作上，也能幫助你更加有效的定位問題的可能 (若屬於應用程式的問題的話)。與開發團隊最大的差異便是我們的工作不是在寫 code、做 feature，而是在解技術問題、每天都在 troubleshooting！(有的開發團隊工程師甚至不見得知道怎麼 troubleshooting，需要我們耐心引導協助客戶調查核心問題，這是也最有趣的部分)   並且工作型態固定，偏向 work-life balanced。   (但這並不代表你不能開發，很多 Cloud Support Engineer 還是會自己兼著做很多好用的工具，內部還是有很多專案能幫忙做的)。   總結   即使擔任雲端技術支持工程師約 1-2 年，我仍然對每天幫助人們應對棘手的技術挑戰感到興奮。我還是為了有機會每天學習這些實際案例而高興，並且能夠想辦法幫助不同產業客戶很多很偉大的業務達到成功 (大規模數據運算, 購物, 手持應用, Streaming, Travling, 加密貨幣交易 … 等)，確保客戶的服務運作在 AWS 上面保有高可用性和可靠性，以持續能運作 24 小時 x 365 天。最有趣的特別是面對一些未知問題最終發覺是個 Bug (不管是 K8s, Docker …)。   在 AWS / Amazon，在全球真的有很多優秀的人、超級酷的 Manager 還有超強的同事。在這裡工作就像是玩一場又一場遊戲，這個遊戲需要我們花很多力氣分享很多分析報告並且嘗試打爆每一個看起來不是那麼友善的問題，只為了提供更好的產品跟技術服務給到客戶。我很高興能夠參與超級多場類似這種遊戲，並且能跟很多優秀的工程師和開發團隊合作。   如果你正在疑惑 Cloud Support Engineer 是什麼樣的工作，希望這篇的內容能夠有助於你了解我們的工作日常。   若你對於這樣的工作環境及內容有所興趣並躍躍欲試，我們仍在持續招聘優秀的人才加入我們，你可以附上 CV 並透過我的 LinkedIn 與我聯繫。   看更多系列文章      我在 Amazon 學到的 10 件事   我是如何在還沒畢業就錄取並進入到 Amazon 工作   Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)   我是如何在一年內通過 AWS 五大核心認證  ","categories": [],
        "tags": ["amazon","aws","amazon web services","work","Cloud Support","Cloud Support Engineer"],
        "url": "https://easoncao.com/what-is-cloud-support-engineer-doing-in-amazon/",
        "teaser": "https://easoncao.com/assets/images/posts/2021/02/what-is-cloud-support-engineer-doing-in-amazon/amazon-bear-chen.png"
      }]
